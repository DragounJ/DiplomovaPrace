{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1678c2d1-3533-4c3c-bd59-c2fec846ce8f",
   "metadata": {},
   "source": [
    "# Notebook pro předbězný výpočet logitů\n",
    "\n",
    "V tomto notebooku se provádí výpočet logitů \"učitelským\" modelem dopředu. Důvod je jednoduchý, při učení formou distilace by se opakovaně logity počítaly, a tím by se učení zbytečně prodlužovalo. Logity tedy napočítáme dopředu a uložíme je k datasetu.\n",
    "Nejprve doinstalujeme knihovny, které nejsou v základu v imagy pro pytorch s GPU podporou na metacentu jupyter hub serveru v základu k dispozici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc597fe3-2d26-4174-bdf7-63b803352f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers[torch])\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers[torch])\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.5.0a0+e000cf0ad9.nv24.10)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, tokenizers, accelerate, transformers, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed accelerate-1.2.1 datasets-3.2.0 dill-0.3.8 huggingface-hub-0.27.0 multiprocess-0.70.16 tokenizers-0.21.0 transformers-4.47.1 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch] datasets ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c29cb6-d4c6-4107-8697-cfc7bfdc251f",
   "metadata": {},
   "source": [
    "Následuje import využívaných knihoven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440e43bc-677d-4fc6-90c4-4f9de936c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, DefaultDataCollator\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import DefaultDataCollator\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bad51-d97f-4bb8-bfab-1a88bcb4b0ad",
   "metadata": {},
   "source": [
    "Třídy pro načtení datasetů \"ručně\" chceme totiž pracovat s datech v jednotlivých souborech, ne s datasetem jako takovým. \n",
    "\n",
    "Předpokládá se již stažení datasetů. Případně je možné datasety stáhnout pomocí:\n",
    "\n",
    "torchvision.dataset.CIFAR10(root='./data/train', train=True, download=True, transform=transform)\n",
    "\n",
    "Samozřejmě je třeba úprava parametrů dle požadavků."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d2ad7b5-9d80-4e16-b3f2-c37a9d63ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10(Dataset):\n",
    "    def __init__(self, root, batch=None, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if self.train:\n",
    "            self.data_file = os.path.join(self.root, 'cifar-10-batches-py', f'data_batch_{batch}')\n",
    "        else:\n",
    "            self.data_file = os.path.join(self.root, 'cifar-10-batches-py', 'test_batch')\n",
    "\n",
    "        with open(self.data_file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "            self.data = dict[b'data']\n",
    "            self.labels = dict[b'labels']\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index].reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return  image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8233aef-eb72-4898-9965-354c86dc3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR100(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if self.train:\n",
    "            self.data_file = os.path.join(self.root, 'cifar-100-python', 'train')\n",
    "        else:\n",
    "            self.data_file = os.path.join(self.root, 'cifar-100-python', 'test')\n",
    "\n",
    "        with open(self.data_file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "            self.data = dict[b'data']\n",
    "            self.labels = dict[b'fine_labels']\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index].reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return  image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ad681-0ac0-4577-ba51-4839cc1a5742",
   "metadata": {},
   "source": [
    "Načtení datasetů a transformace do velikosti, na které byl učitelský, ale i studentský model naučen.\n",
    "\n",
    "Připraveno jak pro CIFAR10 i CIFAR100, stačí upravit komentáře. Stejné platí i pro dataloader, kde se train v případě CIFAR10 nevyužije (rozpad datasetu do několika souborů)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8db49fc4-f155-4f8f-aaaf-f67063e0654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "##Vybere se správný dataset\n",
    "\n",
    "#test = CustomCIFAR10(root='./data/10', train=False, transform=transform)\n",
    "test = CustomCIFAR100(root='./data/100', train=False, transform=transform)\n",
    "train = CustomCIFAR100(root='./data/100', train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1f5d8f2-adc7-46a1-97fd-b28d7fc54e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c23e7251-379b-48f6-a2bf-5e2cf1dfb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b02a4-61e8-4880-b456-44cc8c4e8a78",
   "metadata": {},
   "source": [
    "Nastevení a reset seedů pro replikovatelnost, zřejmě bude možné zredukovat ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09d1124f-6d38-4dce-b81c-bc9ad37a3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed2285c0-7918-4483-98c2-2e3dfa75b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ed8fd-da8d-49ff-ae77-688550738545",
   "metadata": {},
   "source": [
    "Definice učitelského modelu a jeho přepnutí do eval mode (nechceme trénovat a upravovat váhy, chceme pouze výstup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d3e4213-e298-4a5f-ab42-fd5d51620d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vybere se daný \"učitelský\" model pro výpočet logitů\n",
    "\n",
    "#model = AutoModelForImageClassification.from_pretrained(\n",
    "#    \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\",\n",
    "#    num_labels=10,\n",
    "#)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"Ahmed9275/Vit-Cifar100\",\n",
    "    num_labels=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee826e26-b78d-40d2-a2d5-afdfe06b5318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740686f-ff89-4594-b023-d394ce0ab0de",
   "metadata": {},
   "source": [
    "### Metody pro načtení a uložení upravených souborů\n",
    "\n",
    "Následně se vybere soubor, se kterým se pracuje. V případě CIFAR100 lze train část provést stejně jako test (veškerá data jsou v jednom souboru)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf3b9ed3-9aac-4597-8328-0f6e74ba08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def pickle_up(file, contents):\n",
    "    with open(file, 'wb') as fo:\n",
    "        pickle.dump(contents, fo, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6bbd28f-a5b9-4706-91fa-221006acea61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Vybere se daný soubor, se kterým se pracuje\n",
    "\n",
    "#testing = unpickle(\"data/test/cifar-10-batches-py/test_batch\")\n",
    "#testing = unpickle(\"data/100-logits/cifar-100-python/test\")\n",
    "testing = unpickle(\"data/100-logits/cifar-100-python/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05e58de1-b940-4685-b28b-cde497d9ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d4ebbc81-a60d-4206-8124-c93981dbafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3975ae6d884f60923eaba93e2b7d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Pro Cifar100 stačí pustit znovu pro train (není rozpadlý v souborech)\n",
    "\n",
    "for batch in tqdm(train_dataloader):\n",
    "#for batch in tqdm(test_dataloader):\n",
    "    pixel_values, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "    logits_arr.append(logits.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0533b6e2-8dfa-4a65-a69d-a87897b58e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_arr_flat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0bfcc27e-f03e-4eee-b260-786997e0459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor in logits_arr:\n",
    "    logits_arr_flat.extend(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "27a40aa0-3e22-445a-b0c0-7925d8247d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing[b\"logits\"] = logits_arr_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96a46025-7ef2-41c0-9a95-fa18aaf06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vybere se daný soubor, kam se uloží logity\n",
    "\n",
    "#pickle_up(\"data/10-logits/cifar-10-batches-py/test_batch\", testing) \n",
    "#pickle_up(\"data/100-logits/cifar-100-python/test\", testing) \n",
    "pickle_up(\"data/100-logits/cifar-100-python/train\", testing) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edb03b-113c-45df-84ad-d4dfa624daf0",
   "metadata": {},
   "source": [
    "## CIFAR10 \n",
    "Loop pro CIFAR10, jelikož má rozpadnutou training část datasetu do separátních souborů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "974c5e60-3b96-408b-87c2-eb58538f5078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77995532caf4ac295771c37e8ea97f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress for file 1:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae76d533dd9402bb625868de03d851a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress for file 2:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a20d4616524087b92aaef3230220f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress for file 3:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171fc81d16724a0e8e5343416f9f423c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress for file 4:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba07d9c4ec5482bbf281332f494384c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress for file 5:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pro Cifar10\n",
    "\n",
    "reset_seed(42)\n",
    "for index in range(1,6):\n",
    "    data = unpickle(f\"data/10/cifar-10-batches-py/data_batch_{index}\")\n",
    "    train = CustomCIFAR10(root='./data/10', batch=index, train=True, transform=transform)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=False)\n",
    "    logits_arr = []\n",
    "    for batch in tqdm(train_dataloader, desc = f\"Progress for file {index}\"): \n",
    "        pixel_values, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values)\n",
    "            logits = outputs.logits\n",
    "        logits_arr.append(logits.numpy())\n",
    "    logits_arr_flat = []\n",
    "    for tensor in logits_arr:\n",
    "        logits_arr_flat.extend(tensor)\n",
    "    data[b\"logits\"] = logits_arr_flat\n",
    "    pickle_up(f\"data/10-logits/cifar-10-batches-py/data_batch_{index}\",data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
