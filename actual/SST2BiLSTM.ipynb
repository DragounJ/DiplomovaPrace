{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0a0)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.5)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.5.0a0+e000cf0ad9.nv24.10)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch] huggingface_hub datasets evaluate torchvision kagglehub ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import base\n",
    "import kagglehub\n",
    "from datasets import load_from_disk\n",
    "from transformers import BasicTokenizer\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/kagglehub/datasets/thanakomsn/glove6b300dtxt/versions/1\n"
     ]
    }
   ],
   "source": [
    "my_glove = kagglehub.dataset_download(\"thanakomsn/glove6b300dtxt\")\n",
    "print(my_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = f\"{my_glove}/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_from_disk(\"./data/sst2/train-logits\")\n",
    "eval_data = load_from_disk(\"./data/sst2/eval-logits\")\n",
    "test_data = load_from_disk(\"./data/sst2/test-logits\")\n",
    "\n",
    "all_train_data = load_from_disk(\"./data/sst2/train-logits-augmented\")\n",
    "\n",
    "\n",
    "all_data = concatenate_datasets([load_from_disk(file) for file in [\"./data/sst2/train-logits\", \"./data/sst2/eval-logits\", \"./data/sst2/test-logits\", \"./data/sst2/train-logits-augmented\"]])\n",
    "tokenizer = BasicTokenizer(do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    if isinstance(dataset[\"sentence\"], str):\n",
    "        return list(tokenizer.tokenize(dataset[\"sentence\"]))\n",
    "    else:\n",
    "        raise ValueError(\"Input text is not string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    all_tokens = []\n",
    "    for data in dataset:\n",
    "        for token in data:\n",
    "            all_tokens.append(token)\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens = list(map(lambda e: tokenize(e), train_data))\n",
    "eval_data_tokens = list(map(lambda e: tokenize(e), eval_data))\n",
    "test_data_tokens = list(map(lambda e: tokenize(e), test_data))\n",
    "\n",
    "all_train_data_tokens = list(map(lambda e: tokenize(e), all_train_data))\n",
    "\n",
    "\n",
    "all_data_tokens = list(map(lambda e: tokenize(e), all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(all_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14621\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 14305 words (316)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocab) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "print(f\"Converted {hits} words ({misses})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd(data, max_length):\n",
    "    padding_length = max_length - len(data)\n",
    "    if padding_length > 0:\n",
    "        padding = [0 for _ in range(padding_length)]\n",
    "        data.extend(padding)\n",
    "    return data[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_index = list(map(lambda x: list(map(lambda y: word_index[y], x)),train_data_tokens))\n",
    "eval_data_index = list(map(lambda x: list(map(lambda y: word_index[y], x)),eval_data_tokens))\n",
    "test_data_index = list(map(lambda x: list(map(lambda y: word_index[y], x)),test_data_tokens))\n",
    "\n",
    "all_train_data_index = list(map(lambda x: list(map(lambda y: word_index[y], x)),all_train_data_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_data = list(map(lambda x: padd(x,300), train_data_index))\n",
    "eval_padded_data = list(map(lambda x: padd(x,300), eval_data_index))\n",
    "test_padded_data = list(map(lambda x: padd(x,300), test_data_index))\n",
    "\n",
    "all_train_padded_data = list(map(lambda x: padd(x,300), all_train_data_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.add_column(\"input_ids\", train_padded_data)\n",
    "eval_data = eval_data.add_column(\"input_ids\", eval_padded_data)\n",
    "test_data = test_data.add_column(\"input_ids\", test_padded_data)\n",
    "\n",
    "all_train_data = all_train_data.add_column(\"input_ids\", all_train_padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, fc_dim, output_dim, embedding_matrix):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, fc_dim)  \n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(fc_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        embedded = self.embedding(input_ids)  \n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        h_forward = h_n[-2, :, :]  # Last forward hidden state\n",
    "        h_backward = h_n[-1, :, :]  # Last backward hidden state\n",
    "        out_cat = torch.cat((h_forward, h_backward), dim=1)\n",
    "        fc1_out = F.relu(self.fc1(out_cat))\n",
    "        dropped = self.dropout(fc1_out)\n",
    "        logits = self.fc2(dropped)\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels = nn.functional.one_hot(labels, num_classes=self.fc2.out_features) \n",
    "            loss_fn = nn.CrossEntropyLoss() \n",
    "            loss = loss_fn(logits, labels.float())\n",
    "            return {\"loss\" : loss, \"logits\" : logits}\n",
    "        return {\"loss\" : None, \"logits\": logits}\n",
    "    \n",
    "model = BiLSTMClassifier(embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, fc_dim=400, hidden_dim=300, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(14623, 300)\n",
      "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=600, out_features=400, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=\"./results/bilstm-base\", logging_dir='./logs/bilstm-base', lr=.001,  epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=base.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4210/4210 01:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.331080</td>\n",
       "      <td>0.858723</td>\n",
       "      <td>0.859061</td>\n",
       "      <td>0.863918</td>\n",
       "      <td>0.858294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.266239</td>\n",
       "      <td>0.889161</td>\n",
       "      <td>0.887110</td>\n",
       "      <td>0.891044</td>\n",
       "      <td>0.888320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.221975</td>\n",
       "      <td>0.911656</td>\n",
       "      <td>0.909772</td>\n",
       "      <td>0.911955</td>\n",
       "      <td>0.910710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.226675</td>\n",
       "      <td>0.915293</td>\n",
       "      <td>0.917672</td>\n",
       "      <td>0.910864</td>\n",
       "      <td>0.913508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.211240</td>\n",
       "      <td>0.925835</td>\n",
       "      <td>0.923899</td>\n",
       "      <td>0.927066</td>\n",
       "      <td>0.925137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>0.218260</td>\n",
       "      <td>0.929696</td>\n",
       "      <td>0.928639</td>\n",
       "      <td>0.928890</td>\n",
       "      <td>0.928763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.237316</td>\n",
       "      <td>0.930364</td>\n",
       "      <td>0.928973</td>\n",
       "      <td>0.930133</td>\n",
       "      <td>0.929517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.269629</td>\n",
       "      <td>0.932071</td>\n",
       "      <td>0.931195</td>\n",
       "      <td>0.931089</td>\n",
       "      <td>0.931142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.293311</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.930517</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.931198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.312872</td>\n",
       "      <td>0.932368</td>\n",
       "      <td>0.930922</td>\n",
       "      <td>0.932313</td>\n",
       "      <td>0.931565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4210, training_loss=0.1488402756262845, metrics={'train_runtime': 97.858, 'train_samples_per_second': 5505.836, 'train_steps_per_second': 43.022, 'total_flos': 0.0, 'train_loss': 0.1488402756262845, 'epoch': 10.0})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(14623, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/7 00:00 < 00:00, 79.77 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7650835514068604,\n",
       " 'eval_accuracy': 0.8555045871559633,\n",
       " 'eval_precision': 0.8558324898785425,\n",
       " 'eval_recall': 0.8552033341752967,\n",
       " 'eval_f1': 0.8553554502369668,\n",
       " 'eval_runtime': 4.1204,\n",
       " 'eval_samples_per_second': 211.631,\n",
       " 'eval_steps_per_second': 1.699,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = BiLSTMClassifier(embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, fc_dim=400, hidden_dim=300, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=\"./results/bilstm-distill\", remove_unused_columns=False, logging_dir='./logs/bilstm-distill', lr=.001,  epochs=10, batch_size=128, lambda_param=.75, temp=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.ImageDistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=base.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4210/4210 01:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.220200</td>\n",
       "      <td>1.703732</td>\n",
       "      <td>0.866518</td>\n",
       "      <td>0.865024</td>\n",
       "      <td>0.869582</td>\n",
       "      <td>0.865781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.475800</td>\n",
       "      <td>1.298951</td>\n",
       "      <td>0.895694</td>\n",
       "      <td>0.894389</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.894241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.031000</td>\n",
       "      <td>1.020113</td>\n",
       "      <td>0.914625</td>\n",
       "      <td>0.912776</td>\n",
       "      <td>0.914930</td>\n",
       "      <td>0.913706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.834832</td>\n",
       "      <td>0.928731</td>\n",
       "      <td>0.927146</td>\n",
       "      <td>0.928843</td>\n",
       "      <td>0.927914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.753240</td>\n",
       "      <td>0.934521</td>\n",
       "      <td>0.933111</td>\n",
       "      <td>0.934469</td>\n",
       "      <td>0.933741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.711626</td>\n",
       "      <td>0.937268</td>\n",
       "      <td>0.936310</td>\n",
       "      <td>0.936565</td>\n",
       "      <td>0.936436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.657693</td>\n",
       "      <td>0.941203</td>\n",
       "      <td>0.939789</td>\n",
       "      <td>0.941380</td>\n",
       "      <td>0.940517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.631549</td>\n",
       "      <td>0.942762</td>\n",
       "      <td>0.941480</td>\n",
       "      <td>0.942743</td>\n",
       "      <td>0.942071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.629697</td>\n",
       "      <td>0.942910</td>\n",
       "      <td>0.941370</td>\n",
       "      <td>0.943415</td>\n",
       "      <td>0.942277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.612443</td>\n",
       "      <td>0.943356</td>\n",
       "      <td>0.942061</td>\n",
       "      <td>0.943380</td>\n",
       "      <td>0.942676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4210, training_loss=0.7408818333279209, metrics={'train_runtime': 102.5924, 'train_samples_per_second': 5251.753, 'train_steps_per_second': 41.036, 'total_flos': 0.0, 'train_loss': 0.7408818333279209, 'epoch': 10.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(14623, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7558796405792236,\n",
       " 'eval_accuracy': 0.8555045871559633,\n",
       " 'eval_precision': 0.8555213948787062,\n",
       " 'eval_recall': 0.855371726867054,\n",
       " 'eval_f1': 0.8554285353375861,\n",
       " 'eval_runtime': 3.0682,\n",
       " 'eval_samples_per_second': 284.206,\n",
       " 'eval_steps_per_second': 2.281,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMClassifier(embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, fc_dim=400, hidden_dim=300, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=\"./results/bilstm-base\", logging_dir='./logs/bilstm-base', lr=.001,  epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=base.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39290' max='39290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39290/39290 09:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.179852</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>0.930278</td>\n",
       "      <td>0.932296</td>\n",
       "      <td>0.931170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.171890</td>\n",
       "      <td>0.948255</td>\n",
       "      <td>0.947043</td>\n",
       "      <td>0.948294</td>\n",
       "      <td>0.947629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.189071</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>0.948179</td>\n",
       "      <td>0.948831</td>\n",
       "      <td>0.948495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.230110</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>0.951118</td>\n",
       "      <td>0.951055</td>\n",
       "      <td>0.951087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.290225</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>0.947913</td>\n",
       "      <td>0.949249</td>\n",
       "      <td>0.948536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.367420</td>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.951093</td>\n",
       "      <td>0.950919</td>\n",
       "      <td>0.951006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.412773</td>\n",
       "      <td>0.949220</td>\n",
       "      <td>0.948532</td>\n",
       "      <td>0.948532</td>\n",
       "      <td>0.948532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.491498</td>\n",
       "      <td>0.950408</td>\n",
       "      <td>0.949924</td>\n",
       "      <td>0.949510</td>\n",
       "      <td>0.949713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.538246</td>\n",
       "      <td>0.951225</td>\n",
       "      <td>0.950529</td>\n",
       "      <td>0.950607</td>\n",
       "      <td>0.950568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.596340</td>\n",
       "      <td>0.951299</td>\n",
       "      <td>0.950543</td>\n",
       "      <td>0.950761</td>\n",
       "      <td>0.950651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39290, training_loss=0.0610427748674532, metrics={'train_runtime': 562.4468, 'train_samples_per_second': 8940.4, 'train_steps_per_second': 69.855, 'total_flos': 0.0, 'train_loss': 0.0610427748674532, 'epoch': 10.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(14623, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5181599259376526,\n",
       " 'eval_accuracy': 0.8704128440366973,\n",
       " 'eval_precision': 0.8703937504931745,\n",
       " 'eval_recall': 0.8705165445819651,\n",
       " 'eval_f1': 0.8703990382781601,\n",
       " 'eval_runtime': 4.3202,\n",
       " 'eval_samples_per_second': 201.842,\n",
       " 'eval_steps_per_second': 1.62,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = BiLSTMClassifier(embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, fc_dim=400, hidden_dim=300, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=\"./results/bilstm-distill\", remove_unused_columns=False, logging_dir='./logs/bilstm-distill', lr=.001,  epochs=10, batch_size=128, lambda_param=.75, temp=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.ImageDistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=base.compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39290' max='39290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39290/39290 09:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.057900</td>\n",
       "      <td>0.608205</td>\n",
       "      <td>0.942465</td>\n",
       "      <td>0.941190</td>\n",
       "      <td>0.942424</td>\n",
       "      <td>0.941768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.419901</td>\n",
       "      <td>0.954566</td>\n",
       "      <td>0.953394</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>0.954020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.359612</td>\n",
       "      <td>0.960134</td>\n",
       "      <td>0.959187</td>\n",
       "      <td>0.960124</td>\n",
       "      <td>0.959635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.328496</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>0.960677</td>\n",
       "      <td>0.961646</td>\n",
       "      <td>0.961140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.319208</td>\n",
       "      <td>0.961470</td>\n",
       "      <td>0.960515</td>\n",
       "      <td>0.961513</td>\n",
       "      <td>0.960991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.296007</td>\n",
       "      <td>0.963103</td>\n",
       "      <td>0.962291</td>\n",
       "      <td>0.962995</td>\n",
       "      <td>0.962632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.292690</td>\n",
       "      <td>0.962806</td>\n",
       "      <td>0.961878</td>\n",
       "      <td>0.962850</td>\n",
       "      <td>0.962342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.281432</td>\n",
       "      <td>0.963697</td>\n",
       "      <td>0.962985</td>\n",
       "      <td>0.963475</td>\n",
       "      <td>0.963224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.273645</td>\n",
       "      <td>0.963400</td>\n",
       "      <td>0.962711</td>\n",
       "      <td>0.963139</td>\n",
       "      <td>0.962921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.272132</td>\n",
       "      <td>0.963846</td>\n",
       "      <td>0.963043</td>\n",
       "      <td>0.963747</td>\n",
       "      <td>0.963384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39290, training_loss=0.25565577871352246, metrics={'train_runtime': 597.9402, 'train_samples_per_second': 8409.704, 'train_steps_per_second': 65.709, 'total_flos': 0.0, 'train_loss': 0.25565577871352246, 'epoch': 10.0})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMClassifier(\n",
       "  (embedding): Embedding(14623, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8172466158866882,\n",
       " 'eval_accuracy': 0.8795871559633027,\n",
       " 'eval_precision': 0.8798068564383821,\n",
       " 'eval_recall': 0.879357160899217,\n",
       " 'eval_f1': 0.8794881008319744,\n",
       " 'eval_runtime': 2.9664,\n",
       " 'eval_samples_per_second': 293.962,\n",
       " 'eval_steps_per_second': 2.36,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
