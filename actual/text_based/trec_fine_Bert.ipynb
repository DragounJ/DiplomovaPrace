{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torchtext není k dispozici pro poslední verzi pytorch, budeme tedy využuívat něco jiného ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, BertForSequenceClassification, BertTokenizer, EarlyStoppingCallback\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import base\n",
    "import os \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"trec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_from_disk(f\"~/data/{DATASET}/train-logits_fine\")\n",
    "eval = load_from_disk(f\"~/data/{DATASET}/eval-logits_fine\")\n",
    "test = load_from_disk(f\"~/data/{DATASET}/test-logits_fine\")\n",
    "\n",
    "train_aug = load_from_disk(f\"~/data/{DATASET}/train-logits-augmented_fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the train dataset\")\n",
    "eval = eval.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the eval dataset\")\n",
    "test = test.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the test dataset\")\n",
    "\n",
    "train_aug = train_aug.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the augmented dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gpu = copy.deepcopy(train_data)\n",
    "train_data_gpu.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cuda\")\n",
    "gpu_data_loader = DataLoader(train_data_gpu, batch_size=1, shuffle=False)\n",
    "\n",
    "train_data_cpu = copy.deepcopy(train_data)\n",
    "train_data_cpu.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"], device=\"cpu\")\n",
    "cpu_data_loader = DataLoader(train_data_cpu, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base_fine\", logging_dir=f\"~/logs/{DATASET}/bert-base_fine\", lr=0.0012, weight_decay=.01, warmup_steps=4, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/700 01:18 < 00:26, 6.64 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.072300</td>\n",
       "      <td>2.295601</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.161854</td>\n",
       "      <td>0.165593</td>\n",
       "      <td>0.143170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.831300</td>\n",
       "      <td>1.538879</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.273265</td>\n",
       "      <td>0.284365</td>\n",
       "      <td>0.253168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>1.244950</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.345682</td>\n",
       "      <td>0.353773</td>\n",
       "      <td>0.332803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.800200</td>\n",
       "      <td>1.237962</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.379431</td>\n",
       "      <td>0.396922</td>\n",
       "      <td>0.364396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>1.113162</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.489907</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>0.454477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>1.110101</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.558650</td>\n",
       "      <td>0.506712</td>\n",
       "      <td>0.516080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>1.075222</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.566544</td>\n",
       "      <td>0.548188</td>\n",
       "      <td>0.540473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>1.189122</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.603306</td>\n",
       "      <td>0.578967</td>\n",
       "      <td>0.576549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>1.138134</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.623862</td>\n",
       "      <td>0.585633</td>\n",
       "      <td>0.588227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>1.139649</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.694329</td>\n",
       "      <td>0.649343</td>\n",
       "      <td>0.651218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>1.133888</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.727822</td>\n",
       "      <td>0.653316</td>\n",
       "      <td>0.667187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>1.150193</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.748921</td>\n",
       "      <td>0.682807</td>\n",
       "      <td>0.696778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.172859</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.757986</td>\n",
       "      <td>0.677741</td>\n",
       "      <td>0.692044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>1.170689</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.740795</td>\n",
       "      <td>0.667846</td>\n",
       "      <td>0.683592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>1.183414</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.762810</td>\n",
       "      <td>0.675612</td>\n",
       "      <td>0.696274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=525, training_loss=0.5915996124630882, metrics={'train_runtime': 79.107, 'train_samples_per_second': 1102.557, 'train_steps_per_second': 8.849, 'total_flos': 49425716214000.0, 'train_loss': 0.5915996124630882, 'epoch': 15.0})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0590753555297852,\n",
       " 'eval_accuracy': 0.796,\n",
       " 'eval_precision': 0.6639781101834673,\n",
       " 'eval_recall': 0.6490349427918719,\n",
       " 'eval_f1': 0.6378382138608072,\n",
       " 'eval_runtime': 3.4501,\n",
       " 'eval_samples_per_second': 144.924,\n",
       " 'eval_steps_per_second': 1.159,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-base_fine.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill_fine\", logging_dir=f\"~/logs/{DATASET}/bert-distill_fine\", remove_unused_columns=False, lr=0.0015, weight_decay=.003, warmup_steps=4, batch_size=128, epochs=20, temp=6, lambda_param=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/700 01:26 < 00:21, 6.48 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.141800</td>\n",
       "      <td>1.637039</td>\n",
       "      <td>0.518790</td>\n",
       "      <td>0.153312</td>\n",
       "      <td>0.173378</td>\n",
       "      <td>0.142616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.307400</td>\n",
       "      <td>1.139175</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.246957</td>\n",
       "      <td>0.261602</td>\n",
       "      <td>0.236382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.898400</td>\n",
       "      <td>0.960543</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.288119</td>\n",
       "      <td>0.320861</td>\n",
       "      <td>0.298602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.886360</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.383868</td>\n",
       "      <td>0.380070</td>\n",
       "      <td>0.367709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.833152</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.490537</td>\n",
       "      <td>0.451600</td>\n",
       "      <td>0.449750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.811514</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.537126</td>\n",
       "      <td>0.479521</td>\n",
       "      <td>0.490648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.806628</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.553484</td>\n",
       "      <td>0.524728</td>\n",
       "      <td>0.524161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.808993</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.601631</td>\n",
       "      <td>0.569304</td>\n",
       "      <td>0.569710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.803415</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.621739</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.573217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.818579</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.644498</td>\n",
       "      <td>0.615526</td>\n",
       "      <td>0.611536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.795746</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.611486</td>\n",
       "      <td>0.619591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.820520</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.728816</td>\n",
       "      <td>0.639680</td>\n",
       "      <td>0.664378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.802748</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.738073</td>\n",
       "      <td>0.672046</td>\n",
       "      <td>0.689426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.797151</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.716164</td>\n",
       "      <td>0.655016</td>\n",
       "      <td>0.668725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.804529</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.745928</td>\n",
       "      <td>0.670536</td>\n",
       "      <td>0.688871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>0.805691</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.727436</td>\n",
       "      <td>0.666809</td>\n",
       "      <td>0.681071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=560, training_loss=0.4668802525315966, metrics={'train_runtime': 86.4269, 'train_samples_per_second': 1009.177, 'train_steps_per_second': 8.099, 'total_flos': 52720763961600.0, 'train_loss': 0.4668802525315966, 'epoch': 16.0})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8118124008178711,\n",
       " 'eval_accuracy': 0.766,\n",
       " 'eval_precision': 0.6861004212477909,\n",
       " 'eval_recall': 0.6483048743655664,\n",
       " 'eval_f1': 0.6356455735630214,\n",
       " 'eval_runtime': 3.3471,\n",
       " 'eval_samples_per_second': 149.384,\n",
       " 'eval_steps_per_second': 1.195,\n",
       " 'epoch': 16.0}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-distil_fine.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base-aug_fine\", logging_dir=f\"~/logs/{DATASET}/bert-base-aug_fine\", lr=0.00018, warmup_steps=25,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6336' max='10560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6336/10560 03:42 < 02:28, 28.51 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.727300</td>\n",
       "      <td>1.114826</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.448947</td>\n",
       "      <td>0.448363</td>\n",
       "      <td>0.437642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.992170</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.606985</td>\n",
       "      <td>0.551132</td>\n",
       "      <td>0.554259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>1.029273</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.642513</td>\n",
       "      <td>0.599345</td>\n",
       "      <td>0.608961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>1.056720</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.777675</td>\n",
       "      <td>0.707703</td>\n",
       "      <td>0.726482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>1.124146</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.748868</td>\n",
       "      <td>0.688627</td>\n",
       "      <td>0.703290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>1.161935</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.778954</td>\n",
       "      <td>0.710943</td>\n",
       "      <td>0.727378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>1.246127</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.710310</td>\n",
       "      <td>0.722345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>1.226771</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.774692</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.726748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.295860</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.803265</td>\n",
       "      <td>0.706617</td>\n",
       "      <td>0.733698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.372799</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.776010</td>\n",
       "      <td>0.709725</td>\n",
       "      <td>0.721296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.395776</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.766694</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.709738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.408027</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.775851</td>\n",
       "      <td>0.721736</td>\n",
       "      <td>0.731221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6336, training_loss=0.21836013053402756, metrics={'train_runtime': 222.4221, 'train_samples_per_second': 6076.373, 'train_steps_per_second': 47.477, 'total_flos': 612702077299200.0, 'train_loss': 0.21836013053402756, 'epoch': 12.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2343593835830688,\n",
       " 'eval_accuracy': 0.784,\n",
       " 'eval_precision': 0.6889687628677962,\n",
       " 'eval_recall': 0.6815071194775028,\n",
       " 'eval_f1': 0.6608612287748105,\n",
       " 'eval_runtime': 3.7143,\n",
       " 'eval_samples_per_second': 134.615,\n",
       " 'eval_steps_per_second': 1.077,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-base-aug_fine.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill-aug_fine\", logging_dir=f\"~/logs/{DATASET}/bert-distill-aug_fine\", remove_unused_columns=False, lr=0.00047, weight_decay=.007, warmup_steps=15, epochs=20, temp=4, lambda_param=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10560' max='10560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10560/10560 06:03, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.471725</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.549351</td>\n",
       "      <td>0.514425</td>\n",
       "      <td>0.513555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.459124</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.618439</td>\n",
       "      <td>0.601509</td>\n",
       "      <td>0.598734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.475293</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.719573</td>\n",
       "      <td>0.626858</td>\n",
       "      <td>0.651858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.463248</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.736075</td>\n",
       "      <td>0.654855</td>\n",
       "      <td>0.680492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.748872</td>\n",
       "      <td>0.664825</td>\n",
       "      <td>0.691250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.470117</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.748254</td>\n",
       "      <td>0.668805</td>\n",
       "      <td>0.693948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.481648</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.760266</td>\n",
       "      <td>0.672599</td>\n",
       "      <td>0.700890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.460094</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.749415</td>\n",
       "      <td>0.693793</td>\n",
       "      <td>0.709451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.471035</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.785497</td>\n",
       "      <td>0.692480</td>\n",
       "      <td>0.720783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.774716</td>\n",
       "      <td>0.700257</td>\n",
       "      <td>0.722220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.468206</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.806756</td>\n",
       "      <td>0.701938</td>\n",
       "      <td>0.736571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.473176</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.784572</td>\n",
       "      <td>0.712942</td>\n",
       "      <td>0.735165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.469910</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.816619</td>\n",
       "      <td>0.723964</td>\n",
       "      <td>0.753467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.459850</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.809138</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.747842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.474780</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.805927</td>\n",
       "      <td>0.716751</td>\n",
       "      <td>0.746659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.463562</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.818665</td>\n",
       "      <td>0.723182</td>\n",
       "      <td>0.755315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.456453</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.809004</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.756542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.461476</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.812994</td>\n",
       "      <td>0.730668</td>\n",
       "      <td>0.756406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.461775</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.807621</td>\n",
       "      <td>0.715582</td>\n",
       "      <td>0.745914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.454767</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.807463</td>\n",
       "      <td>0.717112</td>\n",
       "      <td>0.747049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10560, training_loss=0.08340635299682617, metrics={'train_runtime': 363.5742, 'train_samples_per_second': 3717.315, 'train_steps_per_second': 29.045, 'total_flos': 1021170128832000.0, 'train_loss': 0.08340635299682617, 'epoch': 20.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41090407967567444,\n",
       " 'eval_accuracy': 0.806,\n",
       " 'eval_precision': 0.6752235121669131,\n",
       " 'eval_recall': 0.6893776846211949,\n",
       " 'eval_f1': 0.6622485946860568,\n",
       " 'eval_runtime': 3.5522,\n",
       " 'eval_samples_per_second': 140.757,\n",
       " 'eval_steps_per_second': 1.126,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-distil-aug_fine.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.count_parameters(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_benchmark = base.BenchMarkRunner(student_model, cpu_data_loader, \"cpu\", 1000)\n",
    "print(cpu_benchmark.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_benchmark = base.BenchMarkRunner(student_model, gpu_data_loader, \"cuda\", 1000)\n",
    "print(gpu_benchmark.run_benchmark())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
