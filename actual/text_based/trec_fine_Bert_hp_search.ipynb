{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e7a26f-aa1f-4645-b3b3-4643ca8be2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_from_disk\n",
    "import optuna\n",
    "import torch\n",
    "import math\n",
    "import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b9ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c9d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"trec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14bfbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_from_disk(f\"~/data/{DATASET}/train-logits_fine\")\n",
    "eval_data = load_from_disk(f\"~/data/{DATASET}/eval-logits_fine\")\n",
    "test_data = load_from_disk(f\"~/data/{DATASET}/test-logits_fine\")\n",
    "\n",
    "all_train_data = load_from_disk(f\"~/data/{DATASET}/train-logits-augmented_fine\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bd0688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the train dataset\")\n",
    "eval = eval_data.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the eval dataset\")\n",
    "test = test_data.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the test dataset\")\n",
    "\n",
    "train_aug = all_train_data.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the augmented dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e61bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f8400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nápočet epoch na steps\n",
    "data_length = len(train_data)\n",
    "min_r = math.ceil(data_length/batch_size)*5\n",
    "max_r = math.ceil(data_length/batch_size)*num_epochs\n",
    "warm_up = math.ceil(data_length/batch_size/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3574e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    params =  {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 1e-2, step=1e-3),\n",
    "        \"adam_beta1\" : trial.suggest_float(\"adam_beta1\", 0.9, 0.99, step=0.01),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, warm_up)\n",
    "    }   \n",
    "    print(f\"Trial {trial.number} with params: {params}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174fff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.HyperbandPruner(min_resource=min_r, max_resource=max_r, reduction_factor=2, bootstrap_count=2)\n",
    "sampler = optuna.samplers.TPESampler(seed=42, multivariate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ba99151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bert():\n",
    "    return BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a906d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b4c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base_fine_hp-search\", logging_dir=f\"~/logs/{DATASET}/bert-base_fine_hp-search\", epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc752b97-d843-4919-a0fc-066d192e037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    model_init = lambda: get_Bert(),\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97855619-93d5-4fc2-93d2-fee24c61b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:38:21,587] A new study created in memory with name: Test-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 with params: {'learning_rate': 1.0253509690168497e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:50 < 01:40, 6.95 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.867300</td>\n",
       "      <td>3.826696</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.006274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.808300</td>\n",
       "      <td>3.777743</td>\n",
       "      <td>0.157654</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.009450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.769800</td>\n",
       "      <td>3.734266</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>0.011374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.728500</td>\n",
       "      <td>3.698701</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.011023</td>\n",
       "      <td>0.022636</td>\n",
       "      <td>0.010120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.704000</td>\n",
       "      <td>3.666467</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.010992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.666800</td>\n",
       "      <td>3.635868</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.637900</td>\n",
       "      <td>3.607637</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.614800</td>\n",
       "      <td>3.580955</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.588400</td>\n",
       "      <td>3.555389</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.566300</td>\n",
       "      <td>3.530838</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:39:12,558] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 with params: {'learning_rate': 2.636875533972305e-06, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:10, 6.73 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.887400</td>\n",
       "      <td>3.868106</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.864700</td>\n",
       "      <td>3.852914</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.856100</td>\n",
       "      <td>3.839477</td>\n",
       "      <td>0.027498</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.003981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.840800</td>\n",
       "      <td>3.826836</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.035579</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.833500</td>\n",
       "      <td>3.814686</td>\n",
       "      <td>0.074244</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.029648</td>\n",
       "      <td>0.008795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:39:39,287] Trial 1 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 with params: {'learning_rate': 4.191711516695204e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:28 < 02:22, 6.12 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.809100</td>\n",
       "      <td>3.696986</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.009983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.642600</td>\n",
       "      <td>3.557530</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.520400</td>\n",
       "      <td>3.427715</td>\n",
       "      <td>0.189734</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.012656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.393000</td>\n",
       "      <td>3.304562</td>\n",
       "      <td>0.317140</td>\n",
       "      <td>0.073325</td>\n",
       "      <td>0.061969</td>\n",
       "      <td>0.054421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.296400</td>\n",
       "      <td>3.193207</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.070009</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>0.065953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:40:08,612] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 with params: {'learning_rate': 0.0001764971584817573, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:59 < 02:00, 5.80 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.604100</td>\n",
       "      <td>3.324278</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.148900</td>\n",
       "      <td>2.925156</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.070810</td>\n",
       "      <td>0.089797</td>\n",
       "      <td>0.065725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.788600</td>\n",
       "      <td>2.575201</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.103001</td>\n",
       "      <td>0.112565</td>\n",
       "      <td>0.083142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.449800</td>\n",
       "      <td>2.284197</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.207764</td>\n",
       "      <td>0.174382</td>\n",
       "      <td>0.158651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.191500</td>\n",
       "      <td>2.049921</td>\n",
       "      <td>0.598533</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.217085</td>\n",
       "      <td>0.202227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.927100</td>\n",
       "      <td>1.856178</td>\n",
       "      <td>0.650779</td>\n",
       "      <td>0.275616</td>\n",
       "      <td>0.264156</td>\n",
       "      <td>0.249957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.713900</td>\n",
       "      <td>1.705789</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.337832</td>\n",
       "      <td>0.288408</td>\n",
       "      <td>0.276678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.558800</td>\n",
       "      <td>1.589029</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.366951</td>\n",
       "      <td>0.332002</td>\n",
       "      <td>0.318553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.399900</td>\n",
       "      <td>1.489767</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.344747</td>\n",
       "      <td>0.336943</td>\n",
       "      <td>0.319753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.264000</td>\n",
       "      <td>1.410418</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.358795</td>\n",
       "      <td>0.352657</td>\n",
       "      <td>0.335164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:41:09,905] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 with params: {'learning_rate': 6.624310605949985e-06, 'weight_decay': 0.005, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:15, 6.47 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.875700</td>\n",
       "      <td>3.844629</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.022320</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.832900</td>\n",
       "      <td>3.811327</td>\n",
       "      <td>0.074244</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.028811</td>\n",
       "      <td>0.007069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.807500</td>\n",
       "      <td>3.779107</td>\n",
       "      <td>0.153987</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>0.009386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.775700</td>\n",
       "      <td>3.749092</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.011068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.755000</td>\n",
       "      <td>3.722877</td>\n",
       "      <td>0.190651</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.024370</td>\n",
       "      <td>0.012025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:41:37,718] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 with params: {'learning_rate': 4.480975918214949e-05, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:49 < 01:40, 6.97 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.797100</td>\n",
       "      <td>3.682824</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.014353</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.627000</td>\n",
       "      <td>3.539651</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.500700</td>\n",
       "      <td>3.405065</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.024860</td>\n",
       "      <td>0.014219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.368600</td>\n",
       "      <td>3.278097</td>\n",
       "      <td>0.333639</td>\n",
       "      <td>0.070008</td>\n",
       "      <td>0.066459</td>\n",
       "      <td>0.057701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.269000</td>\n",
       "      <td>3.164247</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>0.084201</td>\n",
       "      <td>0.066301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.149600</td>\n",
       "      <td>3.061096</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.095822</td>\n",
       "      <td>0.089549</td>\n",
       "      <td>0.069288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.044800</td>\n",
       "      <td>2.964983</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.092273</td>\n",
       "      <td>0.094191</td>\n",
       "      <td>0.073619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.962300</td>\n",
       "      <td>2.877033</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.087469</td>\n",
       "      <td>0.105601</td>\n",
       "      <td>0.083240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.872800</td>\n",
       "      <td>2.796171</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.084480</td>\n",
       "      <td>0.107805</td>\n",
       "      <td>0.081959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.797400</td>\n",
       "      <td>2.722031</td>\n",
       "      <td>0.464711</td>\n",
       "      <td>0.104504</td>\n",
       "      <td>0.119127</td>\n",
       "      <td>0.094073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:42:28,646] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 with params: {'learning_rate': 1.7018418817029176e-05, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:14, 6.51 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.852300</td>\n",
       "      <td>3.793196</td>\n",
       "      <td>0.123740</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.008628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.763800</td>\n",
       "      <td>3.716041</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>0.010897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.704500</td>\n",
       "      <td>3.655700</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.644600</td>\n",
       "      <td>3.601826</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.603300</td>\n",
       "      <td>3.545932</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:42:56,365] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 with params: {'learning_rate': 3.971084710792477e-05, 'weight_decay': 0.0, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:43, 6.78 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.801900</td>\n",
       "      <td>3.699815</td>\n",
       "      <td>0.191567</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.651400</td>\n",
       "      <td>3.577038</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.545000</td>\n",
       "      <td>3.464563</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.432500</td>\n",
       "      <td>3.359885</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.073712</td>\n",
       "      <td>0.032764</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.349700</td>\n",
       "      <td>3.257524</td>\n",
       "      <td>0.329973</td>\n",
       "      <td>0.071322</td>\n",
       "      <td>0.065530</td>\n",
       "      <td>0.057387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.245100</td>\n",
       "      <td>3.165879</td>\n",
       "      <td>0.384968</td>\n",
       "      <td>0.080572</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>0.065432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.152800</td>\n",
       "      <td>3.081800</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.075603</td>\n",
       "      <td>0.087804</td>\n",
       "      <td>0.066010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.079700</td>\n",
       "      <td>3.002302</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.073261</td>\n",
       "      <td>0.090896</td>\n",
       "      <td>0.067756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.000300</td>\n",
       "      <td>2.928501</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.072187</td>\n",
       "      <td>0.095750</td>\n",
       "      <td>0.074489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.932000</td>\n",
       "      <td>2.860335</td>\n",
       "      <td>0.437214</td>\n",
       "      <td>0.087341</td>\n",
       "      <td>0.102646</td>\n",
       "      <td>0.078603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:43:48,729] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 with params: {'learning_rate': 1.4982086432155468e-06, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:50 < 01:41, 6.91 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.890400</td>\n",
       "      <td>3.874964</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.874500</td>\n",
       "      <td>3.865734</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.871200</td>\n",
       "      <td>3.857420</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.860400</td>\n",
       "      <td>3.849826</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.002666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.857000</td>\n",
       "      <td>3.842901</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>0.003047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.845800</td>\n",
       "      <td>3.836313</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.025005</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.004821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.839100</td>\n",
       "      <td>3.830123</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>0.030279</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.006044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.834600</td>\n",
       "      <td>3.824315</td>\n",
       "      <td>0.047663</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.026643</td>\n",
       "      <td>0.007005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.829400</td>\n",
       "      <td>3.818778</td>\n",
       "      <td>0.054995</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.026986</td>\n",
       "      <td>0.006512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.823700</td>\n",
       "      <td>3.813504</td>\n",
       "      <td>0.073327</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.007053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:44:40,043] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 with params: {'learning_rate': 6.639623079859462e-06, 'weight_decay': 0.001, 'adam_beta1': 0.96, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:07, 6.84 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.876200</td>\n",
       "      <td>3.845111</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.022320</td>\n",
       "      <td>0.002962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.833300</td>\n",
       "      <td>3.811925</td>\n",
       "      <td>0.073327</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.007044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.808000</td>\n",
       "      <td>3.780201</td>\n",
       "      <td>0.150321</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.008642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.776700</td>\n",
       "      <td>3.750859</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.011081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.756400</td>\n",
       "      <td>3.724987</td>\n",
       "      <td>0.189734</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:45:06,538] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 with params: {'learning_rate': 0.0003327590120039615, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:55, 6.36 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.437200</td>\n",
       "      <td>3.010705</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.057087</td>\n",
       "      <td>0.084450</td>\n",
       "      <td>0.063515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.739400</td>\n",
       "      <td>2.418249</td>\n",
       "      <td>0.494042</td>\n",
       "      <td>0.169154</td>\n",
       "      <td>0.136597</td>\n",
       "      <td>0.114692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.200900</td>\n",
       "      <td>1.966449</td>\n",
       "      <td>0.600367</td>\n",
       "      <td>0.267899</td>\n",
       "      <td>0.237507</td>\n",
       "      <td>0.226177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.770400</td>\n",
       "      <td>1.675867</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.338317</td>\n",
       "      <td>0.309582</td>\n",
       "      <td>0.292793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.459800</td>\n",
       "      <td>1.470566</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.340215</td>\n",
       "      <td>0.352231</td>\n",
       "      <td>0.329620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.208500</td>\n",
       "      <td>1.319039</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.386151</td>\n",
       "      <td>0.365073</td>\n",
       "      <td>0.349412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>1.240175</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.376775</td>\n",
       "      <td>0.361396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>1.186004</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.426146</td>\n",
       "      <td>0.423945</td>\n",
       "      <td>0.405376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>1.132536</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.469212</td>\n",
       "      <td>0.431798</td>\n",
       "      <td>0.426584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>1.094384</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.480821</td>\n",
       "      <td>0.439009</td>\n",
       "      <td>0.443816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.564200</td>\n",
       "      <td>1.057392</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.516607</td>\n",
       "      <td>0.474786</td>\n",
       "      <td>0.479896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>1.031431</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.500508</td>\n",
       "      <td>0.487610</td>\n",
       "      <td>0.486105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>1.020995</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.515484</td>\n",
       "      <td>0.496154</td>\n",
       "      <td>0.495795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.985759</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.500462</td>\n",
       "      <td>0.490720</td>\n",
       "      <td>0.489867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.997384</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.555825</td>\n",
       "      <td>0.510683</td>\n",
       "      <td>0.512461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.984386</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>0.522218</td>\n",
       "      <td>0.523267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.533798</td>\n",
       "      <td>0.520692</td>\n",
       "      <td>0.516643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.982209</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.564680</td>\n",
       "      <td>0.542369</td>\n",
       "      <td>0.542513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.982519</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.598374</td>\n",
       "      <td>0.553515</td>\n",
       "      <td>0.561280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.590856</td>\n",
       "      <td>0.542393</td>\n",
       "      <td>0.555306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:46:57,615] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 with params: {'learning_rate': 0.0003522178034287917, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:52 < 00:56, 6.22 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.418900</td>\n",
       "      <td>2.975949</td>\n",
       "      <td>0.402383</td>\n",
       "      <td>0.055383</td>\n",
       "      <td>0.086363</td>\n",
       "      <td>0.064005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.695600</td>\n",
       "      <td>2.367807</td>\n",
       "      <td>0.505958</td>\n",
       "      <td>0.176992</td>\n",
       "      <td>0.144579</td>\n",
       "      <td>0.126175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.142800</td>\n",
       "      <td>1.911507</td>\n",
       "      <td>0.626031</td>\n",
       "      <td>0.299060</td>\n",
       "      <td>0.255965</td>\n",
       "      <td>0.245454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.707800</td>\n",
       "      <td>1.624111</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.329687</td>\n",
       "      <td>0.314608</td>\n",
       "      <td>0.297772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.396200</td>\n",
       "      <td>1.432057</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.350946</td>\n",
       "      <td>0.358072</td>\n",
       "      <td>0.335169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.151900</td>\n",
       "      <td>1.282835</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.382850</td>\n",
       "      <td>0.360638</td>\n",
       "      <td>0.344969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>1.211352</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.415620</td>\n",
       "      <td>0.391923</td>\n",
       "      <td>0.378423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.814400</td>\n",
       "      <td>1.161197</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.454024</td>\n",
       "      <td>0.432154</td>\n",
       "      <td>0.419232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>1.111417</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.470745</td>\n",
       "      <td>0.444910</td>\n",
       "      <td>0.442304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>1.072799</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.485935</td>\n",
       "      <td>0.444087</td>\n",
       "      <td>0.449272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.516100</td>\n",
       "      <td>1.045344</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.515594</td>\n",
       "      <td>0.480452</td>\n",
       "      <td>0.485864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>1.021360</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.513139</td>\n",
       "      <td>0.487243</td>\n",
       "      <td>0.489199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>1.013807</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.498399</td>\n",
       "      <td>0.497673</td>\n",
       "      <td>0.491189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.979624</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.521987</td>\n",
       "      <td>0.496941</td>\n",
       "      <td>0.499647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.990493</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.555334</td>\n",
       "      <td>0.524021</td>\n",
       "      <td>0.525959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.980535</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.550328</td>\n",
       "      <td>0.523476</td>\n",
       "      <td>0.523802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.982515</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.584271</td>\n",
       "      <td>0.543549</td>\n",
       "      <td>0.546003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.980266</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.595910</td>\n",
       "      <td>0.556691</td>\n",
       "      <td>0.561917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.985244</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.598532</td>\n",
       "      <td>0.549161</td>\n",
       "      <td>0.558979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.985191</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.593195</td>\n",
       "      <td>0.539757</td>\n",
       "      <td>0.552567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:48:51,142] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 with params: {'learning_rate': 0.0001253755316943676, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.66 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.667000</td>\n",
       "      <td>3.443404</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.307200</td>\n",
       "      <td>3.120686</td>\n",
       "      <td>0.384968</td>\n",
       "      <td>0.060333</td>\n",
       "      <td>0.081479</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.022500</td>\n",
       "      <td>2.843782</td>\n",
       "      <td>0.433547</td>\n",
       "      <td>0.068327</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>0.075838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.747100</td>\n",
       "      <td>2.596740</td>\n",
       "      <td>0.462878</td>\n",
       "      <td>0.098269</td>\n",
       "      <td>0.117706</td>\n",
       "      <td>0.091629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.534200</td>\n",
       "      <td>2.376079</td>\n",
       "      <td>0.522456</td>\n",
       "      <td>0.167433</td>\n",
       "      <td>0.159024</td>\n",
       "      <td>0.140469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.303900</td>\n",
       "      <td>2.198604</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.227967</td>\n",
       "      <td>0.195540</td>\n",
       "      <td>0.180611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.116300</td>\n",
       "      <td>2.052144</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.244274</td>\n",
       "      <td>0.214195</td>\n",
       "      <td>0.202294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.972400</td>\n",
       "      <td>1.926363</td>\n",
       "      <td>0.649863</td>\n",
       "      <td>0.309583</td>\n",
       "      <td>0.271876</td>\n",
       "      <td>0.261446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.819800</td>\n",
       "      <td>1.811438</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.285827</td>\n",
       "      <td>0.277576</td>\n",
       "      <td>0.264113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.687000</td>\n",
       "      <td>1.716246</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.361282</td>\n",
       "      <td>0.314251</td>\n",
       "      <td>0.304716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.589000</td>\n",
       "      <td>1.632952</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.330561</td>\n",
       "      <td>0.322713</td>\n",
       "      <td>0.309456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.473400</td>\n",
       "      <td>1.570725</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.361616</td>\n",
       "      <td>0.332798</td>\n",
       "      <td>0.319206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.403500</td>\n",
       "      <td>1.503610</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.368305</td>\n",
       "      <td>0.340634</td>\n",
       "      <td>0.324779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.315300</td>\n",
       "      <td>1.459456</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.383662</td>\n",
       "      <td>0.374821</td>\n",
       "      <td>0.358373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.252300</td>\n",
       "      <td>1.419798</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.379685</td>\n",
       "      <td>0.380375</td>\n",
       "      <td>0.361310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.193700</td>\n",
       "      <td>1.390785</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.355561</td>\n",
       "      <td>0.376908</td>\n",
       "      <td>0.353645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.126000</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.355407</td>\n",
       "      <td>0.388218</td>\n",
       "      <td>0.361449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.083800</td>\n",
       "      <td>1.329953</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.367025</td>\n",
       "      <td>0.382035</td>\n",
       "      <td>0.361402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.065400</td>\n",
       "      <td>1.314917</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.374852</td>\n",
       "      <td>0.400910</td>\n",
       "      <td>0.378364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.018100</td>\n",
       "      <td>1.292598</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.404436</td>\n",
       "      <td>0.397842</td>\n",
       "      <td>0.378198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:50:37,096] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 with params: {'learning_rate': 0.0004449518806372288, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.381700</td>\n",
       "      <td>2.872418</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.068809</td>\n",
       "      <td>0.096303</td>\n",
       "      <td>0.074193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.551800</td>\n",
       "      <td>2.203707</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.216713</td>\n",
       "      <td>0.186537</td>\n",
       "      <td>0.174408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.930400</td>\n",
       "      <td>1.716975</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.299072</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>0.267099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.468600</td>\n",
       "      <td>1.445356</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.326728</td>\n",
       "      <td>0.322378</td>\n",
       "      <td>0.304091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.153100</td>\n",
       "      <td>1.296354</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.360052</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.351780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>1.172018</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.432753</td>\n",
       "      <td>0.388149</td>\n",
       "      <td>0.379087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>1.129712</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.489186</td>\n",
       "      <td>0.440263</td>\n",
       "      <td>0.437420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>1.088870</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.473226</td>\n",
       "      <td>0.456779</td>\n",
       "      <td>0.449153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>1.049461</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.498873</td>\n",
       "      <td>0.472802</td>\n",
       "      <td>0.473080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>1.032657</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.478292</td>\n",
       "      <td>0.452320</td>\n",
       "      <td>0.453588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>1.011636</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.518075</td>\n",
       "      <td>0.495670</td>\n",
       "      <td>0.492803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.995452</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.530850</td>\n",
       "      <td>0.510353</td>\n",
       "      <td>0.511994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>1.007139</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.551990</td>\n",
       "      <td>0.534207</td>\n",
       "      <td>0.532628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.977074</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.586878</td>\n",
       "      <td>0.538166</td>\n",
       "      <td>0.547512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.988278</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.632632</td>\n",
       "      <td>0.563183</td>\n",
       "      <td>0.576144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>1.010080</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.666249</td>\n",
       "      <td>0.580107</td>\n",
       "      <td>0.598427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.990222</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.661414</td>\n",
       "      <td>0.597584</td>\n",
       "      <td>0.606907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.993023</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.652597</td>\n",
       "      <td>0.589343</td>\n",
       "      <td>0.598401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>1.014954</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.661395</td>\n",
       "      <td>0.593183</td>\n",
       "      <td>0.608035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>1.023863</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.677671</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.622018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.021025</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.701501</td>\n",
       "      <td>0.621956</td>\n",
       "      <td>0.645056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>1.008450</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.738247</td>\n",
       "      <td>0.665551</td>\n",
       "      <td>0.684647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.024484</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.743321</td>\n",
       "      <td>0.662436</td>\n",
       "      <td>0.683464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>1.022841</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.746194</td>\n",
       "      <td>0.672595</td>\n",
       "      <td>0.692779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>1.024273</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.744035</td>\n",
       "      <td>0.666089</td>\n",
       "      <td>0.687792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>1.032613</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.766371</td>\n",
       "      <td>0.674667</td>\n",
       "      <td>0.700846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>1.034733</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.767957</td>\n",
       "      <td>0.678055</td>\n",
       "      <td>0.703805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>1.034798</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.771872</td>\n",
       "      <td>0.678604</td>\n",
       "      <td>0.705688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>1.031936</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.766528</td>\n",
       "      <td>0.678743</td>\n",
       "      <td>0.703836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>1.033590</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.766927</td>\n",
       "      <td>0.677959</td>\n",
       "      <td>0.703408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:53:22,300] Trial 13 finished with value: 0.7034076415908637 and parameters: {'learning_rate': 0.0004449518806372288, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 1}. Best is trial 13 with value: 0.7034076415908637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 with params: {'learning_rate': 0.0002223123214912636, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.39 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.592200</td>\n",
       "      <td>3.266318</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.008028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>2.801454</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>0.072819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.637200</td>\n",
       "      <td>2.405925</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.129852</td>\n",
       "      <td>0.138585</td>\n",
       "      <td>0.115885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.257200</td>\n",
       "      <td>2.093859</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.183873</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.179959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.964600</td>\n",
       "      <td>1.846426</td>\n",
       "      <td>0.647113</td>\n",
       "      <td>0.302119</td>\n",
       "      <td>0.259143</td>\n",
       "      <td>0.248374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.680300</td>\n",
       "      <td>1.643362</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.364683</td>\n",
       "      <td>0.316996</td>\n",
       "      <td>0.309421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.451500</td>\n",
       "      <td>1.503017</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.340973</td>\n",
       "      <td>0.327517</td>\n",
       "      <td>0.315294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.291900</td>\n",
       "      <td>1.402086</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.362094</td>\n",
       "      <td>0.367392</td>\n",
       "      <td>0.345679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.138800</td>\n",
       "      <td>1.325498</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.358743</td>\n",
       "      <td>0.373929</td>\n",
       "      <td>0.356387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.007000</td>\n",
       "      <td>1.268316</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.425207</td>\n",
       "      <td>0.402843</td>\n",
       "      <td>0.394821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.925300</td>\n",
       "      <td>1.224210</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.443026</td>\n",
       "      <td>0.409183</td>\n",
       "      <td>0.403702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>1.192827</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.419440</td>\n",
       "      <td>0.411782</td>\n",
       "      <td>0.402492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.767900</td>\n",
       "      <td>1.156741</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.473522</td>\n",
       "      <td>0.430778</td>\n",
       "      <td>0.428333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>1.122885</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.490157</td>\n",
       "      <td>0.461632</td>\n",
       "      <td>0.461874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>1.113275</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.471820</td>\n",
       "      <td>0.468843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>1.104638</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>0.461986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.544600</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>0.477926</td>\n",
       "      <td>0.473854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>1.074575</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.491802</td>\n",
       "      <td>0.483632</td>\n",
       "      <td>0.479693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>1.065789</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.496650</td>\n",
       "      <td>0.485662</td>\n",
       "      <td>0.482056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>1.047550</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.508107</td>\n",
       "      <td>0.488885</td>\n",
       "      <td>0.487403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:55:12,564] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 with params: {'learning_rate': 0.0004839141884869, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>2.822894</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.066254</td>\n",
       "      <td>0.099744</td>\n",
       "      <td>0.075367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.486900</td>\n",
       "      <td>2.133440</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.198798</td>\n",
       "      <td>0.190561</td>\n",
       "      <td>0.174529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.843500</td>\n",
       "      <td>1.641774</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.286341</td>\n",
       "      <td>0.296340</td>\n",
       "      <td>0.275298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.380400</td>\n",
       "      <td>1.382836</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.335211</td>\n",
       "      <td>0.330472</td>\n",
       "      <td>0.314049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.069800</td>\n",
       "      <td>1.256555</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.374673</td>\n",
       "      <td>0.375889</td>\n",
       "      <td>0.351370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>1.133198</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.453194</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>0.401240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>1.113636</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.469173</td>\n",
       "      <td>0.453050</td>\n",
       "      <td>0.446436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>1.070914</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>1.031723</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.484484</td>\n",
       "      <td>0.481521</td>\n",
       "      <td>0.474505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>1.010424</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>0.474066</td>\n",
       "      <td>0.470531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>1.012033</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.552155</td>\n",
       "      <td>0.509534</td>\n",
       "      <td>0.513126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>1.004005</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.547547</td>\n",
       "      <td>0.521934</td>\n",
       "      <td>0.523105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>1.010312</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.580306</td>\n",
       "      <td>0.546370</td>\n",
       "      <td>0.542863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.998863</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.620525</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.574037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.998691</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.627071</td>\n",
       "      <td>0.569658</td>\n",
       "      <td>0.580815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>1.023221</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.663205</td>\n",
       "      <td>0.589448</td>\n",
       "      <td>0.604111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>1.004907</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.689532</td>\n",
       "      <td>0.619118</td>\n",
       "      <td>0.631750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>1.005760</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.688921</td>\n",
       "      <td>0.605709</td>\n",
       "      <td>0.624930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>1.032907</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.683707</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.631339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>1.041786</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.702149</td>\n",
       "      <td>0.628660</td>\n",
       "      <td>0.646049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:57:01,059] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 with params: {'learning_rate': 0.00019200962492670843, 'weight_decay': 0.005, 'adam_beta1': 0.92, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:50 < 00:55, 6.34 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.613600</td>\n",
       "      <td>3.316540</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.130300</td>\n",
       "      <td>2.893987</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>0.091925</td>\n",
       "      <td>0.069330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.748300</td>\n",
       "      <td>2.525989</td>\n",
       "      <td>0.461962</td>\n",
       "      <td>0.125173</td>\n",
       "      <td>0.118583</td>\n",
       "      <td>0.090627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.392700</td>\n",
       "      <td>2.224229</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.203924</td>\n",
       "      <td>0.182503</td>\n",
       "      <td>0.165845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.119900</td>\n",
       "      <td>1.983265</td>\n",
       "      <td>0.607699</td>\n",
       "      <td>0.293279</td>\n",
       "      <td>0.227151</td>\n",
       "      <td>0.215968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.846700</td>\n",
       "      <td>1.783638</td>\n",
       "      <td>0.668194</td>\n",
       "      <td>0.303654</td>\n",
       "      <td>0.278497</td>\n",
       "      <td>0.265349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.626800</td>\n",
       "      <td>1.635730</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.341924</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.291776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.468300</td>\n",
       "      <td>1.520844</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.354929</td>\n",
       "      <td>0.341462</td>\n",
       "      <td>0.322491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.310100</td>\n",
       "      <td>1.429763</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.353042</td>\n",
       "      <td>0.350833</td>\n",
       "      <td>0.335567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.173800</td>\n",
       "      <td>1.359206</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.369160</td>\n",
       "      <td>0.374041</td>\n",
       "      <td>0.359385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.087500</td>\n",
       "      <td>1.306958</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.407855</td>\n",
       "      <td>0.393408</td>\n",
       "      <td>0.381369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.979000</td>\n",
       "      <td>1.268797</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.382581</td>\n",
       "      <td>0.397834</td>\n",
       "      <td>0.379728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.926500</td>\n",
       "      <td>1.218422</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.397726</td>\n",
       "      <td>0.407976</td>\n",
       "      <td>0.389583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>1.186885</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.437946</td>\n",
       "      <td>0.422937</td>\n",
       "      <td>0.415688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>1.175293</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.463159</td>\n",
       "      <td>0.437140</td>\n",
       "      <td>0.431364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>1.162124</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.471956</td>\n",
       "      <td>0.439139</td>\n",
       "      <td>0.437636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>1.145777</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.472343</td>\n",
       "      <td>0.446375</td>\n",
       "      <td>0.440581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>1.116973</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.472914</td>\n",
       "      <td>0.448985</td>\n",
       "      <td>0.449850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.619100</td>\n",
       "      <td>1.111629</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.488418</td>\n",
       "      <td>0.467939</td>\n",
       "      <td>0.465295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>1.094785</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.499805</td>\n",
       "      <td>0.467266</td>\n",
       "      <td>0.465138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 09:58:52,253] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 with params: {'learning_rate': 0.0003518674002568535, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:52 < 00:56, 6.23 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.463000</td>\n",
       "      <td>3.053541</td>\n",
       "      <td>0.353804</td>\n",
       "      <td>0.066554</td>\n",
       "      <td>0.071733</td>\n",
       "      <td>0.056794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.784800</td>\n",
       "      <td>2.478107</td>\n",
       "      <td>0.454629</td>\n",
       "      <td>0.115564</td>\n",
       "      <td>0.116799</td>\n",
       "      <td>0.089047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.260700</td>\n",
       "      <td>2.035408</td>\n",
       "      <td>0.564620</td>\n",
       "      <td>0.215085</td>\n",
       "      <td>0.191624</td>\n",
       "      <td>0.174802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.821100</td>\n",
       "      <td>1.728583</td>\n",
       "      <td>0.665445</td>\n",
       "      <td>0.309581</td>\n",
       "      <td>0.288001</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.502700</td>\n",
       "      <td>1.501738</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.366820</td>\n",
       "      <td>0.350635</td>\n",
       "      <td>0.330457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.225400</td>\n",
       "      <td>1.327790</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.360910</td>\n",
       "      <td>0.357714</td>\n",
       "      <td>0.340822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.002100</td>\n",
       "      <td>1.242130</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.376558</td>\n",
       "      <td>0.392067</td>\n",
       "      <td>0.372448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>1.190875</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.428164</td>\n",
       "      <td>0.423477</td>\n",
       "      <td>0.407370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.728700</td>\n",
       "      <td>1.138039</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.466428</td>\n",
       "      <td>0.438964</td>\n",
       "      <td>0.433579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>1.118648</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.477567</td>\n",
       "      <td>0.450450</td>\n",
       "      <td>0.448462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>1.073484</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.508831</td>\n",
       "      <td>0.491049</td>\n",
       "      <td>0.489879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>1.045704</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.526147</td>\n",
       "      <td>0.488489</td>\n",
       "      <td>0.492032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>1.043701</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.499277</td>\n",
       "      <td>0.504602</td>\n",
       "      <td>0.493977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>1.009460</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.489656</td>\n",
       "      <td>0.490564</td>\n",
       "      <td>0.483286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>1.010793</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.513354</td>\n",
       "      <td>0.508977</td>\n",
       "      <td>0.501847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.997239</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.529718</td>\n",
       "      <td>0.515920</td>\n",
       "      <td>0.510879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.992586</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.562052</td>\n",
       "      <td>0.532215</td>\n",
       "      <td>0.533961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.589621</td>\n",
       "      <td>0.557535</td>\n",
       "      <td>0.558861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>1.009201</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.583382</td>\n",
       "      <td>0.555669</td>\n",
       "      <td>0.553015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>1.000252</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.596467</td>\n",
       "      <td>0.547359</td>\n",
       "      <td>0.555945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:00:45,379] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 with params: {'learning_rate': 0.00044774926371395345, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.389100</td>\n",
       "      <td>2.875515</td>\n",
       "      <td>0.416132</td>\n",
       "      <td>0.072124</td>\n",
       "      <td>0.094344</td>\n",
       "      <td>0.072598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.552500</td>\n",
       "      <td>2.201509</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.209289</td>\n",
       "      <td>0.185004</td>\n",
       "      <td>0.170074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.931700</td>\n",
       "      <td>1.724058</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.322797</td>\n",
       "      <td>0.288172</td>\n",
       "      <td>0.272777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.472900</td>\n",
       "      <td>1.451723</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.315040</td>\n",
       "      <td>0.314717</td>\n",
       "      <td>0.297280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.161000</td>\n",
       "      <td>1.319734</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.377535</td>\n",
       "      <td>0.385675</td>\n",
       "      <td>0.360683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.926300</td>\n",
       "      <td>1.199748</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.422343</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>0.396751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.751400</td>\n",
       "      <td>1.141133</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.466819</td>\n",
       "      <td>0.434935</td>\n",
       "      <td>0.431540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>1.099949</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.463691</td>\n",
       "      <td>0.461363</td>\n",
       "      <td>0.448306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>1.042960</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.504160</td>\n",
       "      <td>0.476670</td>\n",
       "      <td>0.476533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>1.031526</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.482453</td>\n",
       "      <td>0.470611</td>\n",
       "      <td>0.468023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>1.023100</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.495480</td>\n",
       "      <td>0.495410</td>\n",
       "      <td>0.487886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>1.014060</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.556831</td>\n",
       "      <td>0.524216</td>\n",
       "      <td>0.525698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>1.013893</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.594543</td>\n",
       "      <td>0.554810</td>\n",
       "      <td>0.559217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.998191</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.612103</td>\n",
       "      <td>0.550556</td>\n",
       "      <td>0.563864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>1.017427</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.601084</td>\n",
       "      <td>0.551494</td>\n",
       "      <td>0.558952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>1.012576</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.593079</td>\n",
       "      <td>0.554219</td>\n",
       "      <td>0.557813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>1.016660</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.638182</td>\n",
       "      <td>0.593622</td>\n",
       "      <td>0.601473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>1.003320</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.656390</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>0.600308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>1.043707</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.677359</td>\n",
       "      <td>0.606845</td>\n",
       "      <td>0.619368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>1.051840</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.679321</td>\n",
       "      <td>0.604380</td>\n",
       "      <td>0.622330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>1.048445</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.701171</td>\n",
       "      <td>0.626266</td>\n",
       "      <td>0.643487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>1.041322</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.696633</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.647152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>1.055132</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.702478</td>\n",
       "      <td>0.629374</td>\n",
       "      <td>0.645715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>1.059412</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.733519</td>\n",
       "      <td>0.645407</td>\n",
       "      <td>0.667463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>1.058881</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.724084</td>\n",
       "      <td>0.639971</td>\n",
       "      <td>0.659666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>1.059180</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.704701</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.648068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>1.061628</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.697598</td>\n",
       "      <td>0.629860</td>\n",
       "      <td>0.645098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>1.063048</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.704802</td>\n",
       "      <td>0.631407</td>\n",
       "      <td>0.648512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>1.060099</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>0.652074</td>\n",
       "      <td>0.672828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>1.061327</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.740061</td>\n",
       "      <td>0.651021</td>\n",
       "      <td>0.672105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:03:26,266] Trial 18 finished with value: 0.6721048077147119 and parameters: {'learning_rate': 0.00044774926371395345, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 2}. Best is trial 13 with value: 0.7034076415908637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 with params: {'learning_rate': 0.00025421502789618744, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:48 < 00:54, 6.44 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.547400</td>\n",
       "      <td>3.184589</td>\n",
       "      <td>0.292392</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.950600</td>\n",
       "      <td>2.662975</td>\n",
       "      <td>0.449129</td>\n",
       "      <td>0.082464</td>\n",
       "      <td>0.110887</td>\n",
       "      <td>0.079648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.478200</td>\n",
       "      <td>2.236907</td>\n",
       "      <td>0.541705</td>\n",
       "      <td>0.195753</td>\n",
       "      <td>0.173261</td>\n",
       "      <td>0.157039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.073400</td>\n",
       "      <td>1.922567</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.310003</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.255542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.764700</td>\n",
       "      <td>1.687317</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.335174</td>\n",
       "      <td>0.317789</td>\n",
       "      <td>0.304163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.489300</td>\n",
       "      <td>1.497694</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.354108</td>\n",
       "      <td>0.341318</td>\n",
       "      <td>0.326613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.269900</td>\n",
       "      <td>1.383525</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.348619</td>\n",
       "      <td>0.352036</td>\n",
       "      <td>0.337568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.119800</td>\n",
       "      <td>1.315917</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.402964</td>\n",
       "      <td>0.405178</td>\n",
       "      <td>0.381929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>1.239282</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.424810</td>\n",
       "      <td>0.407654</td>\n",
       "      <td>0.396401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.865300</td>\n",
       "      <td>1.193924</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.406938</td>\n",
       "      <td>0.401546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.790600</td>\n",
       "      <td>1.159753</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.447952</td>\n",
       "      <td>0.432173</td>\n",
       "      <td>0.422115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>1.133847</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.455590</td>\n",
       "      <td>0.440936</td>\n",
       "      <td>0.436229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.637700</td>\n",
       "      <td>1.100534</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.474877</td>\n",
       "      <td>0.452149</td>\n",
       "      <td>0.448101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>1.071263</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.483577</td>\n",
       "      <td>0.479880</td>\n",
       "      <td>0.475124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>1.058578</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.501546</td>\n",
       "      <td>0.480289</td>\n",
       "      <td>0.480055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>1.053374</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.485447</td>\n",
       "      <td>0.489244</td>\n",
       "      <td>0.481363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>1.040680</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.480995</td>\n",
       "      <td>0.491122</td>\n",
       "      <td>0.480067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>1.028167</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.491372</td>\n",
       "      <td>0.493725</td>\n",
       "      <td>0.486642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>1.029376</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.489436</td>\n",
       "      <td>0.497257</td>\n",
       "      <td>0.487510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>1.010150</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.497801</td>\n",
       "      <td>0.495337</td>\n",
       "      <td>0.490522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:05:15,647] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 with params: {'learning_rate': 1.6173144067582056e-06, 'weight_decay': 0.008, 'adam_beta1': 0.97, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:44, 6.67 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.889400</td>\n",
       "      <td>3.873489</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.872700</td>\n",
       "      <td>3.863680</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.869000</td>\n",
       "      <td>3.854964</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.857700</td>\n",
       "      <td>3.846903</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.854100</td>\n",
       "      <td>3.839314</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.004425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.842400</td>\n",
       "      <td>3.832316</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>0.028239</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>0.006349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.835200</td>\n",
       "      <td>3.825656</td>\n",
       "      <td>0.051329</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>0.027227</td>\n",
       "      <td>0.007711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.830300</td>\n",
       "      <td>3.819247</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.029732</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>0.007819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.824600</td>\n",
       "      <td>3.813191</td>\n",
       "      <td>0.078827</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.029680</td>\n",
       "      <td>0.007978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.818600</td>\n",
       "      <td>3.807405</td>\n",
       "      <td>0.092576</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>0.008861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:06:08,959] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 with params: {'learning_rate': 0.00019320445253174156, 'weight_decay': 0.003, 'adam_beta1': 0.99, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.65 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.626100</td>\n",
       "      <td>3.359946</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.207200</td>\n",
       "      <td>3.029256</td>\n",
       "      <td>0.360220</td>\n",
       "      <td>0.060941</td>\n",
       "      <td>0.074539</td>\n",
       "      <td>0.056072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.922800</td>\n",
       "      <td>2.761611</td>\n",
       "      <td>0.427131</td>\n",
       "      <td>0.064416</td>\n",
       "      <td>0.098081</td>\n",
       "      <td>0.072022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.652800</td>\n",
       "      <td>2.519797</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.094420</td>\n",
       "      <td>0.118262</td>\n",
       "      <td>0.091130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.452900</td>\n",
       "      <td>2.320155</td>\n",
       "      <td>0.483960</td>\n",
       "      <td>0.131432</td>\n",
       "      <td>0.141750</td>\n",
       "      <td>0.120534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.224600</td>\n",
       "      <td>2.138783</td>\n",
       "      <td>0.503208</td>\n",
       "      <td>0.177459</td>\n",
       "      <td>0.153831</td>\n",
       "      <td>0.134565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.022700</td>\n",
       "      <td>1.982812</td>\n",
       "      <td>0.578368</td>\n",
       "      <td>0.203854</td>\n",
       "      <td>0.201065</td>\n",
       "      <td>0.180162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.865800</td>\n",
       "      <td>1.851292</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.232196</td>\n",
       "      <td>0.231009</td>\n",
       "      <td>0.203597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.719997</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.223226</td>\n",
       "      <td>0.232581</td>\n",
       "      <td>0.205222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.550300</td>\n",
       "      <td>1.625941</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.295175</td>\n",
       "      <td>0.300929</td>\n",
       "      <td>0.269486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.441400</td>\n",
       "      <td>1.523103</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.291471</td>\n",
       "      <td>0.290760</td>\n",
       "      <td>0.268021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.311600</td>\n",
       "      <td>1.457297</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.377215</td>\n",
       "      <td>0.348499</td>\n",
       "      <td>0.328005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.233200</td>\n",
       "      <td>1.397483</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.385879</td>\n",
       "      <td>0.344582</td>\n",
       "      <td>0.322443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.132400</td>\n",
       "      <td>1.350615</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.365885</td>\n",
       "      <td>0.363687</td>\n",
       "      <td>0.340988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>1.309937</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.375642</td>\n",
       "      <td>0.386076</td>\n",
       "      <td>0.363522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>1.285137</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.404555</td>\n",
       "      <td>0.400666</td>\n",
       "      <td>0.378839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>1.252823</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.393758</td>\n",
       "      <td>0.401371</td>\n",
       "      <td>0.379258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>1.226650</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.418441</td>\n",
       "      <td>0.407806</td>\n",
       "      <td>0.391711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>1.209535</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.412960</td>\n",
       "      <td>0.398112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>1.204456</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.438088</td>\n",
       "      <td>0.428653</td>\n",
       "      <td>0.416867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:07:54,929] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 with params: {'learning_rate': 0.0001668393524829016, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:54, 6.14 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.637400</td>\n",
       "      <td>3.361547</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>2.961573</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.093877</td>\n",
       "      <td>0.088962</td>\n",
       "      <td>0.066628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.829500</td>\n",
       "      <td>2.614855</td>\n",
       "      <td>0.458295</td>\n",
       "      <td>0.103058</td>\n",
       "      <td>0.115369</td>\n",
       "      <td>0.088315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.497400</td>\n",
       "      <td>2.327456</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>0.187387</td>\n",
       "      <td>0.166025</td>\n",
       "      <td>0.149057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.241700</td>\n",
       "      <td>2.092039</td>\n",
       "      <td>0.593034</td>\n",
       "      <td>0.277076</td>\n",
       "      <td>0.214242</td>\n",
       "      <td>0.200573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.979400</td>\n",
       "      <td>1.899177</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.284641</td>\n",
       "      <td>0.262266</td>\n",
       "      <td>0.250887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.768900</td>\n",
       "      <td>1.746739</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.365632</td>\n",
       "      <td>0.290854</td>\n",
       "      <td>0.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.613100</td>\n",
       "      <td>1.628473</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.371371</td>\n",
       "      <td>0.334254</td>\n",
       "      <td>0.322368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.453500</td>\n",
       "      <td>1.522167</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.362209</td>\n",
       "      <td>0.339215</td>\n",
       "      <td>0.324705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.316700</td>\n",
       "      <td>1.442157</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.368010</td>\n",
       "      <td>0.346639</td>\n",
       "      <td>0.331300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:08:52,936] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 with params: {'learning_rate': 0.0004951185712772382, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.354400</td>\n",
       "      <td>2.809672</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>0.066495</td>\n",
       "      <td>0.098964</td>\n",
       "      <td>0.075264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.464600</td>\n",
       "      <td>2.107485</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.230323</td>\n",
       "      <td>0.201904</td>\n",
       "      <td>0.187669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.823100</td>\n",
       "      <td>1.631352</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.291192</td>\n",
       "      <td>0.302115</td>\n",
       "      <td>0.282497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.364500</td>\n",
       "      <td>1.378104</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.337756</td>\n",
       "      <td>0.342950</td>\n",
       "      <td>0.325717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.056600</td>\n",
       "      <td>1.256115</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.411225</td>\n",
       "      <td>0.404124</td>\n",
       "      <td>0.384990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>1.138978</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.460960</td>\n",
       "      <td>0.420699</td>\n",
       "      <td>0.417828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>1.106931</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.469086</td>\n",
       "      <td>0.448686</td>\n",
       "      <td>0.443968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>1.084435</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.502588</td>\n",
       "      <td>0.467873</td>\n",
       "      <td>0.466165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>1.021252</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.523088</td>\n",
       "      <td>0.493830</td>\n",
       "      <td>0.495122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>1.020649</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.535814</td>\n",
       "      <td>0.487131</td>\n",
       "      <td>0.493944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>1.036923</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.584370</td>\n",
       "      <td>0.530496</td>\n",
       "      <td>0.538954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>1.013863</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.571905</td>\n",
       "      <td>0.537043</td>\n",
       "      <td>0.542388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>1.009947</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.603158</td>\n",
       "      <td>0.566382</td>\n",
       "      <td>0.567385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>1.013725</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.641722</td>\n",
       "      <td>0.581162</td>\n",
       "      <td>0.595415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>1.020854</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.655736</td>\n",
       "      <td>0.596269</td>\n",
       "      <td>0.605344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>1.038879</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.667813</td>\n",
       "      <td>0.603808</td>\n",
       "      <td>0.617595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>1.030053</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.710664</td>\n",
       "      <td>0.632453</td>\n",
       "      <td>0.649744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>1.025131</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.721969</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>0.658039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>1.068755</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.697537</td>\n",
       "      <td>0.635083</td>\n",
       "      <td>0.648630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>1.082973</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.706361</td>\n",
       "      <td>0.628133</td>\n",
       "      <td>0.647613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>1.075542</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.712134</td>\n",
       "      <td>0.642393</td>\n",
       "      <td>0.660010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.071653</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.723133</td>\n",
       "      <td>0.653802</td>\n",
       "      <td>0.669133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>1.099225</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.705396</td>\n",
       "      <td>0.632809</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>1.096329</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.732542</td>\n",
       "      <td>0.648283</td>\n",
       "      <td>0.670739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>1.090705</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.722139</td>\n",
       "      <td>0.645442</td>\n",
       "      <td>0.664316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>1.088991</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.725680</td>\n",
       "      <td>0.657876</td>\n",
       "      <td>0.674169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>1.093625</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.725516</td>\n",
       "      <td>0.657428</td>\n",
       "      <td>0.674083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>1.095453</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.729908</td>\n",
       "      <td>0.652549</td>\n",
       "      <td>0.671724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>1.091535</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.731064</td>\n",
       "      <td>0.661916</td>\n",
       "      <td>0.679213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>1.093170</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.731064</td>\n",
       "      <td>0.661916</td>\n",
       "      <td>0.679213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:11:39,645] Trial 23 finished with value: 0.6792130293173753 and parameters: {'learning_rate': 0.0004951185712772382, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 2}. Best is trial 13 with value: 0.7034076415908637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 with params: {'learning_rate': 0.0001946670988041245, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.66 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.610800</td>\n",
       "      <td>3.311188</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.123200</td>\n",
       "      <td>2.885005</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.070305</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.070682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.737700</td>\n",
       "      <td>2.514435</td>\n",
       "      <td>0.465628</td>\n",
       "      <td>0.126781</td>\n",
       "      <td>0.121102</td>\n",
       "      <td>0.094455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.379800</td>\n",
       "      <td>2.211711</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.205188</td>\n",
       "      <td>0.184043</td>\n",
       "      <td>0.167424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.105300</td>\n",
       "      <td>1.970304</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.288733</td>\n",
       "      <td>0.230130</td>\n",
       "      <td>0.218857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.831100</td>\n",
       "      <td>1.770309</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.303681</td>\n",
       "      <td>0.278140</td>\n",
       "      <td>0.265187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.610400</td>\n",
       "      <td>1.623082</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.309569</td>\n",
       "      <td>0.303415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.452100</td>\n",
       "      <td>1.509766</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.354663</td>\n",
       "      <td>0.341462</td>\n",
       "      <td>0.322299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.294500</td>\n",
       "      <td>1.420034</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.354434</td>\n",
       "      <td>0.352262</td>\n",
       "      <td>0.337342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.158400</td>\n",
       "      <td>1.350623</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.373045</td>\n",
       "      <td>0.380803</td>\n",
       "      <td>0.365117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.072700</td>\n",
       "      <td>1.299341</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.406674</td>\n",
       "      <td>0.397067</td>\n",
       "      <td>0.383570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>1.262005</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.382033</td>\n",
       "      <td>0.397834</td>\n",
       "      <td>0.379460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.912300</td>\n",
       "      <td>1.212151</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.396214</td>\n",
       "      <td>0.405931</td>\n",
       "      <td>0.388546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.825100</td>\n",
       "      <td>1.181346</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.437521</td>\n",
       "      <td>0.424080</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>1.169371</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.470860</td>\n",
       "      <td>0.437140</td>\n",
       "      <td>0.432838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.722300</td>\n",
       "      <td>1.156699</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.473778</td>\n",
       "      <td>0.446847</td>\n",
       "      <td>0.444046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>1.140799</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.476732</td>\n",
       "      <td>0.452739</td>\n",
       "      <td>0.447159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>1.112444</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.464616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>1.106922</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.486469</td>\n",
       "      <td>0.467927</td>\n",
       "      <td>0.464526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>1.089226</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.499549</td>\n",
       "      <td>0.470793</td>\n",
       "      <td>0.470631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:13:25,752] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 with params: {'learning_rate': 0.0004873379869051783, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.385200</td>\n",
       "      <td>2.861707</td>\n",
       "      <td>0.416132</td>\n",
       "      <td>0.070786</td>\n",
       "      <td>0.096518</td>\n",
       "      <td>0.074874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.520600</td>\n",
       "      <td>2.163730</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.215830</td>\n",
       "      <td>0.183967</td>\n",
       "      <td>0.171783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.880900</td>\n",
       "      <td>1.668993</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.288436</td>\n",
       "      <td>0.291551</td>\n",
       "      <td>0.270628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.412800</td>\n",
       "      <td>1.405193</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.323523</td>\n",
       "      <td>0.322690</td>\n",
       "      <td>0.302926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.090400</td>\n",
       "      <td>1.267122</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.402925</td>\n",
       "      <td>0.395584</td>\n",
       "      <td>0.371112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.468544</td>\n",
       "      <td>0.412684</td>\n",
       "      <td>0.411409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>1.106336</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.480257</td>\n",
       "      <td>0.473015</td>\n",
       "      <td>0.465655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>1.072843</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.486380</td>\n",
       "      <td>0.471693</td>\n",
       "      <td>0.465823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>1.034702</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.477737</td>\n",
       "      <td>0.478260</td>\n",
       "      <td>0.471279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>1.028865</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.461672</td>\n",
       "      <td>0.473162</td>\n",
       "      <td>0.463816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>1.016986</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.550832</td>\n",
       "      <td>0.517409</td>\n",
       "      <td>0.517479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>1.005821</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.561631</td>\n",
       "      <td>0.534711</td>\n",
       "      <td>0.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>1.015509</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.614630</td>\n",
       "      <td>0.567253</td>\n",
       "      <td>0.571967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.180600</td>\n",
       "      <td>1.010924</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.622625</td>\n",
       "      <td>0.569178</td>\n",
       "      <td>0.576471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>1.005185</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.648696</td>\n",
       "      <td>0.584165</td>\n",
       "      <td>0.598777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>1.003300</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.652248</td>\n",
       "      <td>0.589787</td>\n",
       "      <td>0.600369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>1.005496</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.679809</td>\n",
       "      <td>0.607108</td>\n",
       "      <td>0.622631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>1.003805</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.666059</td>\n",
       "      <td>0.593003</td>\n",
       "      <td>0.608630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>1.048760</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.718499</td>\n",
       "      <td>0.622921</td>\n",
       "      <td>0.644459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>1.033730</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.695965</td>\n",
       "      <td>0.631927</td>\n",
       "      <td>0.645472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>1.039040</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.740876</td>\n",
       "      <td>0.650355</td>\n",
       "      <td>0.671983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>1.044375</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.647286</td>\n",
       "      <td>0.662582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>1.049545</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.746265</td>\n",
       "      <td>0.659183</td>\n",
       "      <td>0.679277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>1.053044</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.774175</td>\n",
       "      <td>0.672236</td>\n",
       "      <td>0.699420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>1.052846</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.770718</td>\n",
       "      <td>0.672378</td>\n",
       "      <td>0.697714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>1.051641</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.764581</td>\n",
       "      <td>0.682325</td>\n",
       "      <td>0.703370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>1.055568</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.776721</td>\n",
       "      <td>0.685245</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>1.054699</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.775651</td>\n",
       "      <td>0.682974</td>\n",
       "      <td>0.706467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.054423</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.782405</td>\n",
       "      <td>0.696077</td>\n",
       "      <td>0.716673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>1.055839</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.780130</td>\n",
       "      <td>0.693051</td>\n",
       "      <td>0.713771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:16:06,055] Trial 25 finished with value: 0.713771496767995 and parameters: {'learning_rate': 0.0004873379869051783, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 3}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 with params: {'learning_rate': 0.00036485975145723236, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:42, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.480700</td>\n",
       "      <td>3.035590</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.063729</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.057650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.736100</td>\n",
       "      <td>2.397801</td>\n",
       "      <td>0.503208</td>\n",
       "      <td>0.152157</td>\n",
       "      <td>0.149534</td>\n",
       "      <td>0.129990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.159500</td>\n",
       "      <td>1.913382</td>\n",
       "      <td>0.616865</td>\n",
       "      <td>0.307896</td>\n",
       "      <td>0.246833</td>\n",
       "      <td>0.234856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.706100</td>\n",
       "      <td>1.624993</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.329430</td>\n",
       "      <td>0.316704</td>\n",
       "      <td>0.299107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.377900</td>\n",
       "      <td>1.424653</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.357703</td>\n",
       "      <td>0.363074</td>\n",
       "      <td>0.339401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.120500</td>\n",
       "      <td>1.270166</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.401348</td>\n",
       "      <td>0.383789</td>\n",
       "      <td>0.367852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>1.198814</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.432743</td>\n",
       "      <td>0.406419</td>\n",
       "      <td>0.395643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>1.147538</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.463775</td>\n",
       "      <td>0.449789</td>\n",
       "      <td>0.441745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>1.098641</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.501285</td>\n",
       "      <td>0.473334</td>\n",
       "      <td>0.474552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>1.082698</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>0.468603</td>\n",
       "      <td>0.462072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>1.039735</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.516120</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.491947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>1.031196</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.520737</td>\n",
       "      <td>0.488066</td>\n",
       "      <td>0.491056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>1.016140</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.500862</td>\n",
       "      <td>0.491921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.989711</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.516072</td>\n",
       "      <td>0.508177</td>\n",
       "      <td>0.505022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.998107</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.560430</td>\n",
       "      <td>0.524217</td>\n",
       "      <td>0.525047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.988141</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.594327</td>\n",
       "      <td>0.559300</td>\n",
       "      <td>0.563414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>0.988711</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.591529</td>\n",
       "      <td>0.561673</td>\n",
       "      <td>0.563679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.992231</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.591361</td>\n",
       "      <td>0.569973</td>\n",
       "      <td>0.569760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>1.005153</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.573543</td>\n",
       "      <td>0.578256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>1.003705</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.636688</td>\n",
       "      <td>0.567011</td>\n",
       "      <td>0.583214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>1.003960</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.632354</td>\n",
       "      <td>0.582403</td>\n",
       "      <td>0.593335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.998366</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.660301</td>\n",
       "      <td>0.597269</td>\n",
       "      <td>0.609608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>1.005063</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.662114</td>\n",
       "      <td>0.604361</td>\n",
       "      <td>0.615995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>1.006251</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.663732</td>\n",
       "      <td>0.601888</td>\n",
       "      <td>0.612416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>1.012508</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.671029</td>\n",
       "      <td>0.604063</td>\n",
       "      <td>0.618367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>1.015445</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.610665</td>\n",
       "      <td>0.622118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>1.015318</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.697668</td>\n",
       "      <td>0.630388</td>\n",
       "      <td>0.644820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>1.010710</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.708758</td>\n",
       "      <td>0.631624</td>\n",
       "      <td>0.649267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>1.011574</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.710179</td>\n",
       "      <td>0.642667</td>\n",
       "      <td>0.658214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>1.012887</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.709560</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.651926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:18:50,650] Trial 26 finished with value: 0.651926342895605 and parameters: {'learning_rate': 0.00036485975145723236, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 4}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 with params: {'learning_rate': 0.00048712164237687974, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.389900</td>\n",
       "      <td>2.875697</td>\n",
       "      <td>0.408799</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.093054</td>\n",
       "      <td>0.072795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.539600</td>\n",
       "      <td>2.182945</td>\n",
       "      <td>0.534372</td>\n",
       "      <td>0.194605</td>\n",
       "      <td>0.179273</td>\n",
       "      <td>0.159774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.903100</td>\n",
       "      <td>1.687192</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.279213</td>\n",
       "      <td>0.287394</td>\n",
       "      <td>0.267503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.433100</td>\n",
       "      <td>1.431242</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.300685</td>\n",
       "      <td>0.317188</td>\n",
       "      <td>0.290889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.108700</td>\n",
       "      <td>1.285738</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.394336</td>\n",
       "      <td>0.383973</td>\n",
       "      <td>0.358910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.873100</td>\n",
       "      <td>1.160569</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.442751</td>\n",
       "      <td>0.407728</td>\n",
       "      <td>0.397502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>1.109850</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.488289</td>\n",
       "      <td>0.457808</td>\n",
       "      <td>0.452436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.578300</td>\n",
       "      <td>1.074225</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.479737</td>\n",
       "      <td>0.464107</td>\n",
       "      <td>0.458520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>1.031456</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.485178</td>\n",
       "      <td>0.479871</td>\n",
       "      <td>0.473529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>1.030198</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.482655</td>\n",
       "      <td>0.475304</td>\n",
       "      <td>0.469942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>1.007673</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.544523</td>\n",
       "      <td>0.515179</td>\n",
       "      <td>0.514753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>1.002400</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.551163</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>0.519652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>1.017296</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.617911</td>\n",
       "      <td>0.571869</td>\n",
       "      <td>0.578629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.998068</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.618905</td>\n",
       "      <td>0.570539</td>\n",
       "      <td>0.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>1.015372</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.628250</td>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.587001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.643735</td>\n",
       "      <td>0.591744</td>\n",
       "      <td>0.599537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>1.008933</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.666975</td>\n",
       "      <td>0.607314</td>\n",
       "      <td>0.618916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>1.006174</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.689646</td>\n",
       "      <td>0.618809</td>\n",
       "      <td>0.633253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>1.041072</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.708753</td>\n",
       "      <td>0.617977</td>\n",
       "      <td>0.638680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>1.032425</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.695083</td>\n",
       "      <td>0.636725</td>\n",
       "      <td>0.648150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>1.031897</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.718401</td>\n",
       "      <td>0.640937</td>\n",
       "      <td>0.659788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>1.042364</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.773733</td>\n",
       "      <td>0.672874</td>\n",
       "      <td>0.697442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>1.044765</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.756112</td>\n",
       "      <td>0.669248</td>\n",
       "      <td>0.690145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.046918</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.782593</td>\n",
       "      <td>0.678511</td>\n",
       "      <td>0.705526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>1.053798</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783327</td>\n",
       "      <td>0.678118</td>\n",
       "      <td>0.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>1.056213</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.779644</td>\n",
       "      <td>0.675674</td>\n",
       "      <td>0.701081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>1.062051</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.781156</td>\n",
       "      <td>0.676970</td>\n",
       "      <td>0.702603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>1.062176</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.781568</td>\n",
       "      <td>0.673747</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>1.062227</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.788205</td>\n",
       "      <td>0.687494</td>\n",
       "      <td>0.712493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>1.063827</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.783429</td>\n",
       "      <td>0.688711</td>\n",
       "      <td>0.711449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:21:34,222] Trial 27 finished with value: 0.711448611893853 and parameters: {'learning_rate': 0.00048712164237687974, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 3}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 with params: {'learning_rate': 0.00043402962575343795, 'weight_decay': 0.006, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.451000</td>\n",
       "      <td>3.001331</td>\n",
       "      <td>0.360220</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>0.074836</td>\n",
       "      <td>0.057278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.698400</td>\n",
       "      <td>2.385731</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.143524</td>\n",
       "      <td>0.132316</td>\n",
       "      <td>0.107745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.133700</td>\n",
       "      <td>1.907023</td>\n",
       "      <td>0.600367</td>\n",
       "      <td>0.241314</td>\n",
       "      <td>0.237691</td>\n",
       "      <td>0.215368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.663400</td>\n",
       "      <td>1.593915</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.306495</td>\n",
       "      <td>0.315368</td>\n",
       "      <td>0.289206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.319300</td>\n",
       "      <td>1.387321</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.342811</td>\n",
       "      <td>0.358938</td>\n",
       "      <td>0.332760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.049400</td>\n",
       "      <td>1.236084</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.450464</td>\n",
       "      <td>0.418355</td>\n",
       "      <td>0.407533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>1.171244</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.418844</td>\n",
       "      <td>0.407558</td>\n",
       "      <td>0.394263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>1.119355</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.485049</td>\n",
       "      <td>0.462433</td>\n",
       "      <td>0.459105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>1.078010</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.519514</td>\n",
       "      <td>0.490734</td>\n",
       "      <td>0.492011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>1.089035</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.487492</td>\n",
       "      <td>0.481850</td>\n",
       "      <td>0.472383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>1.031377</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.517078</td>\n",
       "      <td>0.497044</td>\n",
       "      <td>0.498268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>1.024843</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.547715</td>\n",
       "      <td>0.511148</td>\n",
       "      <td>0.514508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>1.022635</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.571445</td>\n",
       "      <td>0.542161</td>\n",
       "      <td>0.544557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>1.001263</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.598440</td>\n",
       "      <td>0.555247</td>\n",
       "      <td>0.561434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>1.014330</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.592783</td>\n",
       "      <td>0.561117</td>\n",
       "      <td>0.566079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>1.005040</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.591616</td>\n",
       "      <td>0.566977</td>\n",
       "      <td>0.568415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>1.015645</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.593216</td>\n",
       "      <td>0.572558</td>\n",
       "      <td>0.572183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>1.004956</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.594487</td>\n",
       "      <td>0.576784</td>\n",
       "      <td>0.575982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>1.029001</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.663805</td>\n",
       "      <td>0.608486</td>\n",
       "      <td>0.618942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>1.023027</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.651993</td>\n",
       "      <td>0.604365</td>\n",
       "      <td>0.611731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>1.032685</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.672642</td>\n",
       "      <td>0.616919</td>\n",
       "      <td>0.627036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>1.037584</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.682079</td>\n",
       "      <td>0.624393</td>\n",
       "      <td>0.636501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>1.044897</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.720384</td>\n",
       "      <td>0.663176</td>\n",
       "      <td>0.675328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>1.043921</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.703865</td>\n",
       "      <td>0.647586</td>\n",
       "      <td>0.659314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>1.048420</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.729023</td>\n",
       "      <td>0.664860</td>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>1.049950</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.732373</td>\n",
       "      <td>0.673316</td>\n",
       "      <td>0.687291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>1.051391</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.727284</td>\n",
       "      <td>0.669697</td>\n",
       "      <td>0.683733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>1.047637</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.710909</td>\n",
       "      <td>0.650843</td>\n",
       "      <td>0.665186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.046249</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.721442</td>\n",
       "      <td>0.663674</td>\n",
       "      <td>0.677017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>1.047017</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.722615</td>\n",
       "      <td>0.663674</td>\n",
       "      <td>0.677724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:24:14,713] Trial 28 finished with value: 0.677724449478389 and parameters: {'learning_rate': 0.00043402962575343795, 'weight_decay': 0.006, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 with params: {'learning_rate': 0.0002755879459606728, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:46, 6.59 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.548300</td>\n",
       "      <td>3.190046</td>\n",
       "      <td>0.224565</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>0.025379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>2.685336</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.080616</td>\n",
       "      <td>0.107454</td>\n",
       "      <td>0.077147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.500900</td>\n",
       "      <td>2.269268</td>\n",
       "      <td>0.516957</td>\n",
       "      <td>0.144051</td>\n",
       "      <td>0.152518</td>\n",
       "      <td>0.131393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.090900</td>\n",
       "      <td>1.944497</td>\n",
       "      <td>0.599450</td>\n",
       "      <td>0.259428</td>\n",
       "      <td>0.223165</td>\n",
       "      <td>0.205830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.773500</td>\n",
       "      <td>1.690490</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.336968</td>\n",
       "      <td>0.300691</td>\n",
       "      <td>0.286147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.480400</td>\n",
       "      <td>1.490884</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.340555</td>\n",
       "      <td>0.337443</td>\n",
       "      <td>0.323538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.374671</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.343896</td>\n",
       "      <td>0.337263</td>\n",
       "      <td>0.323111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.086800</td>\n",
       "      <td>1.296757</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.408984</td>\n",
       "      <td>0.405945</td>\n",
       "      <td>0.386111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.943700</td>\n",
       "      <td>1.231935</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.404205</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.387070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>1.193969</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.490599</td>\n",
       "      <td>0.433264</td>\n",
       "      <td>0.434065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:25:08,501] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 with params: {'learning_rate': 0.00027086882126714745, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:44, 6.67 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.554600</td>\n",
       "      <td>3.185080</td>\n",
       "      <td>0.282310</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>0.050771</td>\n",
       "      <td>0.041793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.940200</td>\n",
       "      <td>2.642440</td>\n",
       "      <td>0.444546</td>\n",
       "      <td>0.088409</td>\n",
       "      <td>0.110123</td>\n",
       "      <td>0.079615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.449900</td>\n",
       "      <td>2.204448</td>\n",
       "      <td>0.547204</td>\n",
       "      <td>0.220815</td>\n",
       "      <td>0.178450</td>\n",
       "      <td>0.164385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.032300</td>\n",
       "      <td>1.884424</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.293246</td>\n",
       "      <td>0.271097</td>\n",
       "      <td>0.257291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.711600</td>\n",
       "      <td>1.639735</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.343792</td>\n",
       "      <td>0.334846</td>\n",
       "      <td>0.320584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.429200</td>\n",
       "      <td>1.451692</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.350990</td>\n",
       "      <td>0.346397</td>\n",
       "      <td>0.331531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.206000</td>\n",
       "      <td>1.345203</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.345813</td>\n",
       "      <td>0.349176</td>\n",
       "      <td>0.333529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.056000</td>\n",
       "      <td>1.288666</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.411122</td>\n",
       "      <td>0.387876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>1.214523</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.431672</td>\n",
       "      <td>0.419365</td>\n",
       "      <td>0.406861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>1.169700</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.436788</td>\n",
       "      <td>0.411731</td>\n",
       "      <td>0.404291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:26:01,814] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 with params: {'learning_rate': 0.00023665444713746742, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.68 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.564700</td>\n",
       "      <td>3.218992</td>\n",
       "      <td>0.256645</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.042880</td>\n",
       "      <td>0.035435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>2.720794</td>\n",
       "      <td>0.437214</td>\n",
       "      <td>0.061411</td>\n",
       "      <td>0.105498</td>\n",
       "      <td>0.075671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.546000</td>\n",
       "      <td>2.309584</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.165705</td>\n",
       "      <td>0.152593</td>\n",
       "      <td>0.134927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.156000</td>\n",
       "      <td>1.997207</td>\n",
       "      <td>0.609533</td>\n",
       "      <td>0.289805</td>\n",
       "      <td>0.234191</td>\n",
       "      <td>0.221816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.853900</td>\n",
       "      <td>1.754989</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.341923</td>\n",
       "      <td>0.307213</td>\n",
       "      <td>0.296206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.574600</td>\n",
       "      <td>1.562187</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.356442</td>\n",
       "      <td>0.331172</td>\n",
       "      <td>0.319693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.352100</td>\n",
       "      <td>1.435514</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.321935</td>\n",
       "      <td>0.327241</td>\n",
       "      <td>0.310434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.198600</td>\n",
       "      <td>1.356616</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.393458</td>\n",
       "      <td>0.384405</td>\n",
       "      <td>0.361924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>1.276526</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.376631</td>\n",
       "      <td>0.377866</td>\n",
       "      <td>0.362502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>1.227765</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.439124</td>\n",
       "      <td>0.408161</td>\n",
       "      <td>0.402805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>1.191052</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.432420</td>\n",
       "      <td>0.412324</td>\n",
       "      <td>0.402808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>1.167234</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.425610</td>\n",
       "      <td>0.423124</td>\n",
       "      <td>0.413405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>1.128244</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.468678</td>\n",
       "      <td>0.431619</td>\n",
       "      <td>0.427645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.635700</td>\n",
       "      <td>1.102523</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.468315</td>\n",
       "      <td>0.462275</td>\n",
       "      <td>0.455301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>1.084643</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.508306</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.485039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>1.082489</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.487683</td>\n",
       "      <td>0.474092</td>\n",
       "      <td>0.469011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>1.063902</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.481347</td>\n",
       "      <td>0.488319</td>\n",
       "      <td>0.479531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>1.049830</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.488953</td>\n",
       "      <td>0.489368</td>\n",
       "      <td>0.482386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>1.049246</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.490668</td>\n",
       "      <td>0.497489</td>\n",
       "      <td>0.488238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>1.027860</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.496678</td>\n",
       "      <td>0.495337</td>\n",
       "      <td>0.489653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:27:47,524] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 with params: {'learning_rate': 0.0003692254257662043, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:40, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.462800</td>\n",
       "      <td>3.006428</td>\n",
       "      <td>0.381302</td>\n",
       "      <td>0.060096</td>\n",
       "      <td>0.081008</td>\n",
       "      <td>0.061017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.704300</td>\n",
       "      <td>2.364909</td>\n",
       "      <td>0.513291</td>\n",
       "      <td>0.176790</td>\n",
       "      <td>0.154715</td>\n",
       "      <td>0.138435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.123200</td>\n",
       "      <td>1.882494</td>\n",
       "      <td>0.641613</td>\n",
       "      <td>0.318542</td>\n",
       "      <td>0.266771</td>\n",
       "      <td>0.257772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.669300</td>\n",
       "      <td>1.591036</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.308896</td>\n",
       "      <td>0.312208</td>\n",
       "      <td>0.294653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.347700</td>\n",
       "      <td>1.412908</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.348895</td>\n",
       "      <td>0.360266</td>\n",
       "      <td>0.335635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.099200</td>\n",
       "      <td>1.259933</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.404490</td>\n",
       "      <td>0.376749</td>\n",
       "      <td>0.362838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>1.195505</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.432458</td>\n",
       "      <td>0.410583</td>\n",
       "      <td>0.396424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.772600</td>\n",
       "      <td>1.145831</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>0.444078</td>\n",
       "      <td>0.435122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>1.084206</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.502980</td>\n",
       "      <td>0.474033</td>\n",
       "      <td>0.473657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.545400</td>\n",
       "      <td>1.070478</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.473553</td>\n",
       "      <td>0.467874</td>\n",
       "      <td>0.461314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>1.037021</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.507838</td>\n",
       "      <td>0.492466</td>\n",
       "      <td>0.489372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>1.021931</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.519005</td>\n",
       "      <td>0.490409</td>\n",
       "      <td>0.491894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>1.010270</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.498243</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.489637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.304800</td>\n",
       "      <td>0.980490</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.491883</td>\n",
       "      <td>0.491402</td>\n",
       "      <td>0.485850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.988293</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.557560</td>\n",
       "      <td>0.517154</td>\n",
       "      <td>0.521551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.983250</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.599536</td>\n",
       "      <td>0.549130</td>\n",
       "      <td>0.555675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.980497</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.607145</td>\n",
       "      <td>0.567466</td>\n",
       "      <td>0.570773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.600106</td>\n",
       "      <td>0.565058</td>\n",
       "      <td>0.569713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.995352</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.647693</td>\n",
       "      <td>0.583645</td>\n",
       "      <td>0.597042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.995209</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.641033</td>\n",
       "      <td>0.571232</td>\n",
       "      <td>0.588755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>1.004008</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.654077</td>\n",
       "      <td>0.589162</td>\n",
       "      <td>0.607086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.996924</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.660766</td>\n",
       "      <td>0.597369</td>\n",
       "      <td>0.610741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>1.004362</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.667516</td>\n",
       "      <td>0.602008</td>\n",
       "      <td>0.616699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>1.003291</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.644868</td>\n",
       "      <td>0.594775</td>\n",
       "      <td>0.602179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>1.007481</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.650967</td>\n",
       "      <td>0.598963</td>\n",
       "      <td>0.608866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>1.011145</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.669553</td>\n",
       "      <td>0.610292</td>\n",
       "      <td>0.622132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>1.011550</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.702952</td>\n",
       "      <td>0.634585</td>\n",
       "      <td>0.648939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>1.009127</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.710742</td>\n",
       "      <td>0.631286</td>\n",
       "      <td>0.650778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>1.010255</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.705641</td>\n",
       "      <td>0.635675</td>\n",
       "      <td>0.652448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>1.011435</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.706094</td>\n",
       "      <td>0.635675</td>\n",
       "      <td>0.652703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:30:29,236] Trial 32 finished with value: 0.6527034994036484 and parameters: {'learning_rate': 0.0003692254257662043, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 3}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 with params: {'learning_rate': 1.2161047690501487e-06, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:54, 6.46 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.891200</td>\n",
       "      <td>3.876736</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.877000</td>\n",
       "      <td>3.869148</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.875300</td>\n",
       "      <td>3.862123</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.865600</td>\n",
       "      <td>3.855793</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.863300</td>\n",
       "      <td>3.849910</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.002648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.853300</td>\n",
       "      <td>3.844429</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.022527</td>\n",
       "      <td>0.003167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.847400</td>\n",
       "      <td>3.839283</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.023603</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.844000</td>\n",
       "      <td>3.834450</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>0.005380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.839700</td>\n",
       "      <td>3.829863</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.030243</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.006082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.834900</td>\n",
       "      <td>3.825515</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.030504</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.006682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.831300</td>\n",
       "      <td>3.821437</td>\n",
       "      <td>0.055912</td>\n",
       "      <td>0.030510</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.007945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.827900</td>\n",
       "      <td>3.817525</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.006587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.824700</td>\n",
       "      <td>3.813920</td>\n",
       "      <td>0.073327</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.007027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.817900</td>\n",
       "      <td>3.810445</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.030613</td>\n",
       "      <td>0.008350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.817600</td>\n",
       "      <td>3.807165</td>\n",
       "      <td>0.090742</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.031028</td>\n",
       "      <td>0.008094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.811400</td>\n",
       "      <td>3.804097</td>\n",
       "      <td>0.098992</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.031960</td>\n",
       "      <td>0.008276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.811700</td>\n",
       "      <td>3.801236</td>\n",
       "      <td>0.107241</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.032893</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.810700</td>\n",
       "      <td>3.798545</td>\n",
       "      <td>0.112741</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>0.008856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.808200</td>\n",
       "      <td>3.796094</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.008853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.810400</td>\n",
       "      <td>3.793846</td>\n",
       "      <td>0.120073</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.034514</td>\n",
       "      <td>0.008755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:32:18,248] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34 with params: {'learning_rate': 3.6875829250628446e-05, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:50 < 01:42, 6.86 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>3.717633</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>0.020232</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.013269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.674000</td>\n",
       "      <td>3.610057</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.582800</td>\n",
       "      <td>3.516364</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.486700</td>\n",
       "      <td>3.431469</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.420700</td>\n",
       "      <td>3.346731</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.334500</td>\n",
       "      <td>3.271289</td>\n",
       "      <td>0.228231</td>\n",
       "      <td>0.075042</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.028247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.258100</td>\n",
       "      <td>3.203288</td>\n",
       "      <td>0.319890</td>\n",
       "      <td>0.069716</td>\n",
       "      <td>0.062808</td>\n",
       "      <td>0.053385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.199300</td>\n",
       "      <td>3.138176</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>0.077249</td>\n",
       "      <td>0.062111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.135900</td>\n",
       "      <td>3.078069</td>\n",
       "      <td>0.389551</td>\n",
       "      <td>0.059374</td>\n",
       "      <td>0.082475</td>\n",
       "      <td>0.064210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.082700</td>\n",
       "      <td>3.022057</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.055802</td>\n",
       "      <td>0.083573</td>\n",
       "      <td>0.063255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:33:10,117] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 with params: {'learning_rate': 0.00028825028916564503, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:40, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.535000</td>\n",
       "      <td>3.161306</td>\n",
       "      <td>0.288726</td>\n",
       "      <td>0.072758</td>\n",
       "      <td>0.052720</td>\n",
       "      <td>0.043350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.917400</td>\n",
       "      <td>2.625308</td>\n",
       "      <td>0.444546</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.110123</td>\n",
       "      <td>0.078588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.431600</td>\n",
       "      <td>2.197621</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>0.180434</td>\n",
       "      <td>0.165777</td>\n",
       "      <td>0.145873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.016700</td>\n",
       "      <td>1.881277</td>\n",
       "      <td>0.612282</td>\n",
       "      <td>0.267668</td>\n",
       "      <td>0.237511</td>\n",
       "      <td>0.224720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.698200</td>\n",
       "      <td>1.630718</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.327383</td>\n",
       "      <td>0.311242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.406700</td>\n",
       "      <td>1.437995</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.350835</td>\n",
       "      <td>0.350178</td>\n",
       "      <td>0.334491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>1.333214</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.342199</td>\n",
       "      <td>0.345976</td>\n",
       "      <td>0.330475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.020400</td>\n",
       "      <td>1.260923</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.400939</td>\n",
       "      <td>0.406162</td>\n",
       "      <td>0.384668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>1.203830</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.416582</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.399949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.762800</td>\n",
       "      <td>1.172130</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.496358</td>\n",
       "      <td>0.438631</td>\n",
       "      <td>0.443034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>1.124296</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.464524</td>\n",
       "      <td>0.465386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>1.098902</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.505891</td>\n",
       "      <td>0.472548</td>\n",
       "      <td>0.473043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>1.078499</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.486309</td>\n",
       "      <td>0.486110</td>\n",
       "      <td>0.479110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>1.042298</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>0.498944</td>\n",
       "      <td>0.499002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>1.045440</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.488595</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.480334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>1.029833</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.484404</td>\n",
       "      <td>0.490504</td>\n",
       "      <td>0.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>1.015447</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.508915</td>\n",
       "      <td>0.493940</td>\n",
       "      <td>0.487695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>1.015048</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.504223</td>\n",
       "      <td>0.503298</td>\n",
       "      <td>0.496836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>1.015462</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.530063</td>\n",
       "      <td>0.510909</td>\n",
       "      <td>0.506617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>1.001774</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>0.510234</td>\n",
       "      <td>0.512061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>1.004238</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.550762</td>\n",
       "      <td>0.524735</td>\n",
       "      <td>0.528432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>1.008389</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.573106</td>\n",
       "      <td>0.543032</td>\n",
       "      <td>0.546046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>1.001307</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.577028</td>\n",
       "      <td>0.547165</td>\n",
       "      <td>0.550119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.999380</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.578202</td>\n",
       "      <td>0.543170</td>\n",
       "      <td>0.545991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>1.008610</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.595005</td>\n",
       "      <td>0.558172</td>\n",
       "      <td>0.563802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>1.006547</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.589055</td>\n",
       "      <td>0.557672</td>\n",
       "      <td>0.558659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>1.011793</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.597858</td>\n",
       "      <td>0.559284</td>\n",
       "      <td>0.567144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>1.008976</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.596372</td>\n",
       "      <td>0.561756</td>\n",
       "      <td>0.568255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>1.009016</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.594692</td>\n",
       "      <td>0.561130</td>\n",
       "      <td>0.567694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>1.008758</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.592314</td>\n",
       "      <td>0.559966</td>\n",
       "      <td>0.565624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:35:52,395] Trial 35 finished with value: 0.5656240822367657 and parameters: {'learning_rate': 0.00028825028916564503, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 3}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 with params: {'learning_rate': 0.0004455699071177728, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.390800</td>\n",
       "      <td>2.878551</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>0.071787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.556600</td>\n",
       "      <td>2.206064</td>\n",
       "      <td>0.549038</td>\n",
       "      <td>0.210915</td>\n",
       "      <td>0.184550</td>\n",
       "      <td>0.170415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.936800</td>\n",
       "      <td>1.728372</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.324055</td>\n",
       "      <td>0.288275</td>\n",
       "      <td>0.272936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.478100</td>\n",
       "      <td>1.455531</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.315265</td>\n",
       "      <td>0.314717</td>\n",
       "      <td>0.297387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.165900</td>\n",
       "      <td>1.322307</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.386549</td>\n",
       "      <td>0.384342</td>\n",
       "      <td>0.359881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>1.201065</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.433711</td>\n",
       "      <td>0.409824</td>\n",
       "      <td>0.396876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.755400</td>\n",
       "      <td>1.141269</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.466652</td>\n",
       "      <td>0.435173</td>\n",
       "      <td>0.431408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>1.101549</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.464723</td>\n",
       "      <td>0.466265</td>\n",
       "      <td>0.451055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>1.043104</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.508115</td>\n",
       "      <td>0.480296</td>\n",
       "      <td>0.479539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>1.031891</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.483167</td>\n",
       "      <td>0.466513</td>\n",
       "      <td>0.465721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>1.020051</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.493145</td>\n",
       "      <td>0.486185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>1.013078</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.550560</td>\n",
       "      <td>0.522834</td>\n",
       "      <td>0.522935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>1.012462</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.590835</td>\n",
       "      <td>0.547982</td>\n",
       "      <td>0.553164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.998118</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.611568</td>\n",
       "      <td>0.550206</td>\n",
       "      <td>0.563393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>1.016730</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.601542</td>\n",
       "      <td>0.549508</td>\n",
       "      <td>0.557303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>1.010763</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>0.553692</td>\n",
       "      <td>0.553667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>1.014506</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.617257</td>\n",
       "      <td>0.576531</td>\n",
       "      <td>0.581640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>1.004476</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.633350</td>\n",
       "      <td>0.577622</td>\n",
       "      <td>0.588840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>1.049803</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.678293</td>\n",
       "      <td>0.603806</td>\n",
       "      <td>0.618313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>1.042533</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.677159</td>\n",
       "      <td>0.602943</td>\n",
       "      <td>0.620477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.043769</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.700008</td>\n",
       "      <td>0.625022</td>\n",
       "      <td>0.642888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>1.036518</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.698605</td>\n",
       "      <td>0.633955</td>\n",
       "      <td>0.646093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>1.051181</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.700843</td>\n",
       "      <td>0.625822</td>\n",
       "      <td>0.643311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>1.055148</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.712143</td>\n",
       "      <td>0.638707</td>\n",
       "      <td>0.656525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>1.055074</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.722938</td>\n",
       "      <td>0.638492</td>\n",
       "      <td>0.658069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>1.054497</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.701997</td>\n",
       "      <td>0.629455</td>\n",
       "      <td>0.647291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>1.056041</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.696524</td>\n",
       "      <td>0.627955</td>\n",
       "      <td>0.643554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>1.057641</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.703585</td>\n",
       "      <td>0.631407</td>\n",
       "      <td>0.648183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>1.056073</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.742788</td>\n",
       "      <td>0.653021</td>\n",
       "      <td>0.674707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>1.057323</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.742471</td>\n",
       "      <td>0.652069</td>\n",
       "      <td>0.674092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:38:40,977] Trial 36 finished with value: 0.6740922743074372 and parameters: {'learning_rate': 0.0004455699071177728, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 with params: {'learning_rate': 0.0004242582047930815, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.57 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.431800</td>\n",
       "      <td>2.954758</td>\n",
       "      <td>0.373052</td>\n",
       "      <td>0.078597</td>\n",
       "      <td>0.080304</td>\n",
       "      <td>0.059518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.643000</td>\n",
       "      <td>2.299810</td>\n",
       "      <td>0.522456</td>\n",
       "      <td>0.176465</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>0.146096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.042900</td>\n",
       "      <td>1.806176</td>\n",
       "      <td>0.641613</td>\n",
       "      <td>0.281055</td>\n",
       "      <td>0.263426</td>\n",
       "      <td>0.246867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.574700</td>\n",
       "      <td>1.528529</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.313108</td>\n",
       "      <td>0.325986</td>\n",
       "      <td>0.301925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.242200</td>\n",
       "      <td>1.368002</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.362768</td>\n",
       "      <td>0.336324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>1.210244</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.453239</td>\n",
       "      <td>0.410007</td>\n",
       "      <td>0.404067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>1.147501</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.481636</td>\n",
       "      <td>0.453899</td>\n",
       "      <td>0.444928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>1.099383</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.484251</td>\n",
       "      <td>0.466965</td>\n",
       "      <td>0.460502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>1.067924</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.471616</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.462725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>1.056194</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.455922</td>\n",
       "      <td>0.469015</td>\n",
       "      <td>0.457141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>1.022475</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.505547</td>\n",
       "      <td>0.504038</td>\n",
       "      <td>0.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>1.009575</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.536114</td>\n",
       "      <td>0.497898</td>\n",
       "      <td>0.503428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>1.006097</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.595438</td>\n",
       "      <td>0.538942</td>\n",
       "      <td>0.544281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.992963</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.597555</td>\n",
       "      <td>0.536235</td>\n",
       "      <td>0.548687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>1.010703</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.600559</td>\n",
       "      <td>0.558827</td>\n",
       "      <td>0.564020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.996761</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.621073</td>\n",
       "      <td>0.574663</td>\n",
       "      <td>0.580857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.989552</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.628476</td>\n",
       "      <td>0.582914</td>\n",
       "      <td>0.590758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.989070</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.619455</td>\n",
       "      <td>0.583874</td>\n",
       "      <td>0.586727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>1.007591</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.658633</td>\n",
       "      <td>0.596498</td>\n",
       "      <td>0.607979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>1.004343</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>0.613233</td>\n",
       "      <td>0.633818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:40:28,271] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38 with params: {'learning_rate': 1.1626268513139648e-06, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.51 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.891100</td>\n",
       "      <td>3.876778</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.877300</td>\n",
       "      <td>3.869459</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.875900</td>\n",
       "      <td>3.862891</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.866500</td>\n",
       "      <td>3.856837</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.864600</td>\n",
       "      <td>3.851101</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.855000</td>\n",
       "      <td>3.845855</td>\n",
       "      <td>0.023831</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.849100</td>\n",
       "      <td>3.840852</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.845900</td>\n",
       "      <td>3.836077</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.005353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.841700</td>\n",
       "      <td>3.831641</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.027906</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>0.006626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.837100</td>\n",
       "      <td>3.827292</td>\n",
       "      <td>0.051329</td>\n",
       "      <td>0.036069</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>0.008371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.833500</td>\n",
       "      <td>3.823215</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.008715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.830200</td>\n",
       "      <td>3.819430</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.008856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.826800</td>\n",
       "      <td>3.815794</td>\n",
       "      <td>0.069661</td>\n",
       "      <td>0.030289</td>\n",
       "      <td>0.029481</td>\n",
       "      <td>0.008923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.820300</td>\n",
       "      <td>3.812373</td>\n",
       "      <td>0.086159</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>0.030995</td>\n",
       "      <td>0.009348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.819800</td>\n",
       "      <td>3.809111</td>\n",
       "      <td>0.089826</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.031275</td>\n",
       "      <td>0.008879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.813800</td>\n",
       "      <td>3.806077</td>\n",
       "      <td>0.098075</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.032208</td>\n",
       "      <td>0.009035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.814200</td>\n",
       "      <td>3.803282</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.009421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.813200</td>\n",
       "      <td>3.800611</td>\n",
       "      <td>0.107241</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>0.033063</td>\n",
       "      <td>0.008804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.810700</td>\n",
       "      <td>3.798150</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>0.009526</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>0.008803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.812900</td>\n",
       "      <td>3.795914</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.008851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:42:16,540] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39 with params: {'learning_rate': 2.744905812550553e-06, 'weight_decay': 0.006, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:15, 6.46 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.887100</td>\n",
       "      <td>3.867419</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.863700</td>\n",
       "      <td>3.851833</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.854600</td>\n",
       "      <td>3.837835</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.004423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.839000</td>\n",
       "      <td>3.824835</td>\n",
       "      <td>0.053162</td>\n",
       "      <td>0.035998</td>\n",
       "      <td>0.027786</td>\n",
       "      <td>0.008530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.831400</td>\n",
       "      <td>3.812328</td>\n",
       "      <td>0.082493</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.008714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:42:44,539] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 with params: {'learning_rate': 8.598520700165698e-06, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:50, 6.35 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.871900</td>\n",
       "      <td>3.835581</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.024503</td>\n",
       "      <td>0.005260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>3.793098</td>\n",
       "      <td>0.119157</td>\n",
       "      <td>0.009668</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>0.008785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.786300</td>\n",
       "      <td>3.752241</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.009991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.747600</td>\n",
       "      <td>3.717832</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.035121</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>3.687949</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.023732</td>\n",
       "      <td>0.011482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.690100</td>\n",
       "      <td>3.660423</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.664000</td>\n",
       "      <td>3.634884</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.643400</td>\n",
       "      <td>3.611047</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.619800</td>\n",
       "      <td>3.588417</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.599900</td>\n",
       "      <td>3.567006</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:43:40,916] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 with params: {'learning_rate': 0.000270092496664244, 'weight_decay': 0.006, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.37 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.565300</td>\n",
       "      <td>3.214044</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>0.007406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.996700</td>\n",
       "      <td>2.733759</td>\n",
       "      <td>0.440880</td>\n",
       "      <td>0.061075</td>\n",
       "      <td>0.106052</td>\n",
       "      <td>0.075880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.554500</td>\n",
       "      <td>2.324817</td>\n",
       "      <td>0.497709</td>\n",
       "      <td>0.162196</td>\n",
       "      <td>0.146949</td>\n",
       "      <td>0.124265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.147000</td>\n",
       "      <td>1.991963</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.285229</td>\n",
       "      <td>0.223418</td>\n",
       "      <td>0.207220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.827400</td>\n",
       "      <td>1.733214</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.321450</td>\n",
       "      <td>0.296882</td>\n",
       "      <td>0.282840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.533600</td>\n",
       "      <td>1.534032</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.318003</td>\n",
       "      <td>0.328068</td>\n",
       "      <td>0.308938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.294200</td>\n",
       "      <td>1.406061</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.365208</td>\n",
       "      <td>0.345588</td>\n",
       "      <td>0.330761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.322978</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.401060</td>\n",
       "      <td>0.401448</td>\n",
       "      <td>0.381774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>1.247690</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.413072</td>\n",
       "      <td>0.400195</td>\n",
       "      <td>0.385790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>1.207762</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.450265</td>\n",
       "      <td>0.432089</td>\n",
       "      <td>0.424327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>1.168682</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.486279</td>\n",
       "      <td>0.442209</td>\n",
       "      <td>0.442173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>1.130959</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.490979</td>\n",
       "      <td>0.453442</td>\n",
       "      <td>0.456965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>1.117843</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>0.480136</td>\n",
       "      <td>0.475089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>1.075759</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.483897</td>\n",
       "      <td>0.470969</td>\n",
       "      <td>0.467544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>1.067748</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.497174</td>\n",
       "      <td>0.485088</td>\n",
       "      <td>0.481716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>1.063204</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.472303</td>\n",
       "      <td>0.481904</td>\n",
       "      <td>0.472526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>1.043055</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.488461</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.483780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>1.039346</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.477366</td>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.480568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>1.038484</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.489481</td>\n",
       "      <td>0.488599</td>\n",
       "      <td>0.483849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>1.026333</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.488543</td>\n",
       "      <td>0.491307</td>\n",
       "      <td>0.484069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:45:31,604] Trial 41 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 with params: {'learning_rate': 0.00026846937217687056, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.97, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:51 < 00:55, 6.27 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.572000</td>\n",
       "      <td>3.233528</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.033400</td>\n",
       "      <td>2.799177</td>\n",
       "      <td>0.423465</td>\n",
       "      <td>0.063873</td>\n",
       "      <td>0.097923</td>\n",
       "      <td>0.072537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.634800</td>\n",
       "      <td>2.418626</td>\n",
       "      <td>0.479377</td>\n",
       "      <td>0.138312</td>\n",
       "      <td>0.138124</td>\n",
       "      <td>0.113598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.250700</td>\n",
       "      <td>2.100898</td>\n",
       "      <td>0.571036</td>\n",
       "      <td>0.211709</td>\n",
       "      <td>0.200204</td>\n",
       "      <td>0.178503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.942900</td>\n",
       "      <td>1.834127</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>0.290084</td>\n",
       "      <td>0.246393</td>\n",
       "      <td>0.225341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.647600</td>\n",
       "      <td>1.627400</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.334435</td>\n",
       "      <td>0.304472</td>\n",
       "      <td>0.292816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.402000</td>\n",
       "      <td>1.483749</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.372979</td>\n",
       "      <td>0.327732</td>\n",
       "      <td>0.312980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.223200</td>\n",
       "      <td>1.374950</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.388163</td>\n",
       "      <td>0.368156</td>\n",
       "      <td>0.349959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.306632</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.418329</td>\n",
       "      <td>0.407713</td>\n",
       "      <td>0.385673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>1.258739</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.403635</td>\n",
       "      <td>0.388351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>1.232011</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.443844</td>\n",
       "      <td>0.424014</td>\n",
       "      <td>0.410047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>1.185900</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.469076</td>\n",
       "      <td>0.433546</td>\n",
       "      <td>0.426875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>1.152087</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.493530</td>\n",
       "      <td>0.464545</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>1.113055</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.458799</td>\n",
       "      <td>0.464121</td>\n",
       "      <td>0.455307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.573400</td>\n",
       "      <td>1.109272</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.464769</td>\n",
       "      <td>0.461596</td>\n",
       "      <td>0.454823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>1.106024</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.463660</td>\n",
       "      <td>0.466499</td>\n",
       "      <td>0.458180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.473300</td>\n",
       "      <td>1.089306</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.451869</td>\n",
       "      <td>0.473445</td>\n",
       "      <td>0.457491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>1.063736</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.475745</td>\n",
       "      <td>0.474316</td>\n",
       "      <td>0.470234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.410800</td>\n",
       "      <td>1.071819</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.482904</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.479042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>1.072659</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.488144</td>\n",
       "      <td>0.490327</td>\n",
       "      <td>0.483097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:47:23,913] Trial 42 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 with params: {'learning_rate': 0.00012302824092650286, 'weight_decay': 0.005, 'adam_beta1': 0.97, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.50 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.707500</td>\n",
       "      <td>3.505467</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.381100</td>\n",
       "      <td>3.221679</td>\n",
       "      <td>0.253896</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>0.042242</td>\n",
       "      <td>0.035472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.133100</td>\n",
       "      <td>2.982766</td>\n",
       "      <td>0.395050</td>\n",
       "      <td>0.055152</td>\n",
       "      <td>0.084405</td>\n",
       "      <td>0.062279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.894700</td>\n",
       "      <td>2.762021</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.064514</td>\n",
       "      <td>0.102611</td>\n",
       "      <td>0.076198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.708800</td>\n",
       "      <td>2.560732</td>\n",
       "      <td>0.460128</td>\n",
       "      <td>0.104242</td>\n",
       "      <td>0.118392</td>\n",
       "      <td>0.088547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.492900</td>\n",
       "      <td>2.380018</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.163917</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>0.126869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.306600</td>\n",
       "      <td>2.225447</td>\n",
       "      <td>0.547204</td>\n",
       "      <td>0.197027</td>\n",
       "      <td>0.186542</td>\n",
       "      <td>0.171587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.161600</td>\n",
       "      <td>2.094768</td>\n",
       "      <td>0.582035</td>\n",
       "      <td>0.245911</td>\n",
       "      <td>0.218821</td>\n",
       "      <td>0.206099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.004700</td>\n",
       "      <td>1.966284</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.287361</td>\n",
       "      <td>0.246473</td>\n",
       "      <td>0.231195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.863300</td>\n",
       "      <td>1.860889</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.324754</td>\n",
       "      <td>0.275698</td>\n",
       "      <td>0.265741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:48:18,541] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 with params: {'learning_rate': 0.0003092173892725669, 'weight_decay': 0.004, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:42 < 00:51, 6.78 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.533800</td>\n",
       "      <td>3.157379</td>\n",
       "      <td>0.243813</td>\n",
       "      <td>0.036014</td>\n",
       "      <td>0.038673</td>\n",
       "      <td>0.029922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.917200</td>\n",
       "      <td>2.633150</td>\n",
       "      <td>0.446379</td>\n",
       "      <td>0.064466</td>\n",
       "      <td>0.109139</td>\n",
       "      <td>0.077243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.440200</td>\n",
       "      <td>2.208831</td>\n",
       "      <td>0.525206</td>\n",
       "      <td>0.160527</td>\n",
       "      <td>0.159821</td>\n",
       "      <td>0.138363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.013500</td>\n",
       "      <td>1.871538</td>\n",
       "      <td>0.623281</td>\n",
       "      <td>0.315534</td>\n",
       "      <td>0.256655</td>\n",
       "      <td>0.244970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.678100</td>\n",
       "      <td>1.615660</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.337499</td>\n",
       "      <td>0.324622</td>\n",
       "      <td>0.305431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.380600</td>\n",
       "      <td>1.425485</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.361841</td>\n",
       "      <td>0.355482</td>\n",
       "      <td>0.335627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.144400</td>\n",
       "      <td>1.320124</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.375064</td>\n",
       "      <td>0.358796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>1.253955</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.416426</td>\n",
       "      <td>0.415359</td>\n",
       "      <td>0.396880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>1.187218</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.444059</td>\n",
       "      <td>0.423121</td>\n",
       "      <td>0.416298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>1.166965</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.504934</td>\n",
       "      <td>0.465564</td>\n",
       "      <td>0.464268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>1.121678</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.487458</td>\n",
       "      <td>0.466240</td>\n",
       "      <td>0.462007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.564600</td>\n",
       "      <td>1.090024</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.480553</td>\n",
       "      <td>0.472091</td>\n",
       "      <td>0.467287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.084908</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.478556</td>\n",
       "      <td>0.488734</td>\n",
       "      <td>0.476740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>1.044743</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.490679</td>\n",
       "      <td>0.483245</td>\n",
       "      <td>0.476687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>1.043005</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.498492</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>0.485135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>1.032083</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.483359</td>\n",
       "      <td>0.492495</td>\n",
       "      <td>0.482709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.330800</td>\n",
       "      <td>1.018084</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.528174</td>\n",
       "      <td>0.512193</td>\n",
       "      <td>0.509174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>1.019126</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.511198</td>\n",
       "      <td>0.505752</td>\n",
       "      <td>0.501607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>1.025103</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.562469</td>\n",
       "      <td>0.522323</td>\n",
       "      <td>0.522554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>1.016606</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.574563</td>\n",
       "      <td>0.532564</td>\n",
       "      <td>0.540022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:50:02,494] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 with params: {'learning_rate': 0.0003090706082000009, 'weight_decay': 0.0, 'adam_beta1': 0.91, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:48 < 00:54, 6.43 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.489400</td>\n",
       "      <td>3.088002</td>\n",
       "      <td>0.347388</td>\n",
       "      <td>0.068093</td>\n",
       "      <td>0.070863</td>\n",
       "      <td>0.057688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.829200</td>\n",
       "      <td>2.519150</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>0.104213</td>\n",
       "      <td>0.114052</td>\n",
       "      <td>0.084344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.311000</td>\n",
       "      <td>2.071147</td>\n",
       "      <td>0.571952</td>\n",
       "      <td>0.255636</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.185815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.880700</td>\n",
       "      <td>1.768602</td>\n",
       "      <td>0.668194</td>\n",
       "      <td>0.341544</td>\n",
       "      <td>0.292178</td>\n",
       "      <td>0.276907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.562900</td>\n",
       "      <td>1.542407</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.351093</td>\n",
       "      <td>0.352726</td>\n",
       "      <td>0.332423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.292300</td>\n",
       "      <td>1.370013</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.362786</td>\n",
       "      <td>0.343873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.080500</td>\n",
       "      <td>1.279246</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.362930</td>\n",
       "      <td>0.341672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>1.234309</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.411174</td>\n",
       "      <td>0.415890</td>\n",
       "      <td>0.393618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>1.168012</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.458538</td>\n",
       "      <td>0.429501</td>\n",
       "      <td>0.421130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.698700</td>\n",
       "      <td>1.125639</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.470863</td>\n",
       "      <td>0.425773</td>\n",
       "      <td>0.424154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>1.093374</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.482129</td>\n",
       "      <td>0.461973</td>\n",
       "      <td>0.462952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>1.074254</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.519421</td>\n",
       "      <td>0.485096</td>\n",
       "      <td>0.488044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>1.054063</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.480494</td>\n",
       "      <td>0.476228</td>\n",
       "      <td>0.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>1.021480</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.485162</td>\n",
       "      <td>0.484537</td>\n",
       "      <td>0.478940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>1.021970</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.500646</td>\n",
       "      <td>0.487527</td>\n",
       "      <td>0.483335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>1.012064</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.503131</td>\n",
       "      <td>0.496871</td>\n",
       "      <td>0.489643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>1.003757</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.509362</td>\n",
       "      <td>0.502845</td>\n",
       "      <td>0.496085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>1.001287</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.510105</td>\n",
       "      <td>0.507581</td>\n",
       "      <td>0.500623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>1.011140</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.561516</td>\n",
       "      <td>0.529994</td>\n",
       "      <td>0.529984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.995148</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>0.518860</td>\n",
       "      <td>0.526678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:51:52,250] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 with params: {'learning_rate': 3.3046602676148886e-06, 'weight_decay': 0.0, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.52 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.884200</td>\n",
       "      <td>3.862834</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.857900</td>\n",
       "      <td>3.844604</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.022320</td>\n",
       "      <td>0.002979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.846500</td>\n",
       "      <td>3.828222</td>\n",
       "      <td>0.039413</td>\n",
       "      <td>0.030304</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.006277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.828700</td>\n",
       "      <td>3.812831</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.029928</td>\n",
       "      <td>0.008533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.819200</td>\n",
       "      <td>3.797659</td>\n",
       "      <td>0.112741</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>0.008847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.799400</td>\n",
       "      <td>3.782979</td>\n",
       "      <td>0.147571</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.785500</td>\n",
       "      <td>3.768638</td>\n",
       "      <td>0.171402</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>0.021011</td>\n",
       "      <td>0.010097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.774200</td>\n",
       "      <td>3.755530</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.010326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.763000</td>\n",
       "      <td>3.743652</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>0.023351</td>\n",
       "      <td>0.011354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.752500</td>\n",
       "      <td>3.732893</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.743600</td>\n",
       "      <td>3.723281</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.013946</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.010924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.732500</td>\n",
       "      <td>3.714337</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.014939</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.010926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.727900</td>\n",
       "      <td>3.706088</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.011408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.717100</td>\n",
       "      <td>3.698531</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.010812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.711800</td>\n",
       "      <td>3.691551</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016619</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.010884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.701800</td>\n",
       "      <td>3.685097</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.010725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.697400</td>\n",
       "      <td>3.679161</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.696900</td>\n",
       "      <td>3.673826</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.015597</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.689600</td>\n",
       "      <td>3.668744</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.690500</td>\n",
       "      <td>3.664310</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:53:40,340] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 with params: {'learning_rate': 0.0003334709121687755, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.490200</td>\n",
       "      <td>3.062112</td>\n",
       "      <td>0.365720</td>\n",
       "      <td>0.065367</td>\n",
       "      <td>0.076712</td>\n",
       "      <td>0.060397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.778100</td>\n",
       "      <td>2.449457</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.151907</td>\n",
       "      <td>0.133945</td>\n",
       "      <td>0.112452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.222600</td>\n",
       "      <td>1.976173</td>\n",
       "      <td>0.598533</td>\n",
       "      <td>0.291192</td>\n",
       "      <td>0.232137</td>\n",
       "      <td>0.221737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.780500</td>\n",
       "      <td>1.680550</td>\n",
       "      <td>0.687443</td>\n",
       "      <td>0.305205</td>\n",
       "      <td>0.303414</td>\n",
       "      <td>0.287139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.460800</td>\n",
       "      <td>1.472287</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.370137</td>\n",
       "      <td>0.359064</td>\n",
       "      <td>0.339404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.202800</td>\n",
       "      <td>1.315924</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.383769</td>\n",
       "      <td>0.373325</td>\n",
       "      <td>0.355785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>1.239117</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.376670</td>\n",
       "      <td>0.370789</td>\n",
       "      <td>0.355261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.862600</td>\n",
       "      <td>1.193823</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.450659</td>\n",
       "      <td>0.437614</td>\n",
       "      <td>0.421388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.739300</td>\n",
       "      <td>1.134481</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.462675</td>\n",
       "      <td>0.449210</td>\n",
       "      <td>0.441759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>1.101507</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.471946</td>\n",
       "      <td>0.446325</td>\n",
       "      <td>0.446922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>1.067407</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.509125</td>\n",
       "      <td>0.479299</td>\n",
       "      <td>0.480640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>1.049654</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.514149</td>\n",
       "      <td>0.486526</td>\n",
       "      <td>0.488796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>1.031397</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.496680</td>\n",
       "      <td>0.493477</td>\n",
       "      <td>0.488507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>1.000072</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.497113</td>\n",
       "      <td>0.492618</td>\n",
       "      <td>0.487174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.996318</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.512666</td>\n",
       "      <td>0.498323</td>\n",
       "      <td>0.494219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.994139</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>0.508336</td>\n",
       "      <td>0.501055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.992788</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.571033</td>\n",
       "      <td>0.535077</td>\n",
       "      <td>0.536155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.991916</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.585399</td>\n",
       "      <td>0.542019</td>\n",
       "      <td>0.547156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>1.000988</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.588612</td>\n",
       "      <td>0.558328</td>\n",
       "      <td>0.559574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.991470</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.582139</td>\n",
       "      <td>0.534558</td>\n",
       "      <td>0.542812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.992540</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.607703</td>\n",
       "      <td>0.556801</td>\n",
       "      <td>0.567895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.991308</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.601643</td>\n",
       "      <td>0.572181</td>\n",
       "      <td>0.574295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.994831</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.626262</td>\n",
       "      <td>0.578564</td>\n",
       "      <td>0.586752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.997044</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.619157</td>\n",
       "      <td>0.583678</td>\n",
       "      <td>0.584425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.623733</td>\n",
       "      <td>0.583179</td>\n",
       "      <td>0.589910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.646067</td>\n",
       "      <td>0.593633</td>\n",
       "      <td>0.604313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>1.003949</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.578550</td>\n",
       "      <td>0.585151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>1.001233</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.626957</td>\n",
       "      <td>0.581750</td>\n",
       "      <td>0.590449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>1.002375</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.642176</td>\n",
       "      <td>0.589986</td>\n",
       "      <td>0.602050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>1.003204</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.644768</td>\n",
       "      <td>0.590577</td>\n",
       "      <td>0.603315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:56:23,540] Trial 47 finished with value: 0.6033148350383309 and parameters: {'learning_rate': 0.0003334709121687755, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 3}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 with params: {'learning_rate': 8.153679865827414e-06, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:48, 6.46 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.870400</td>\n",
       "      <td>3.835476</td>\n",
       "      <td>0.028414</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.004819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.821000</td>\n",
       "      <td>3.796159</td>\n",
       "      <td>0.114574</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.033712</td>\n",
       "      <td>0.008305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.790800</td>\n",
       "      <td>3.760752</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>0.021907</td>\n",
       "      <td>0.010355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.756300</td>\n",
       "      <td>3.731040</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.011072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.736400</td>\n",
       "      <td>3.705850</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.707300</td>\n",
       "      <td>3.683157</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.023562</td>\n",
       "      <td>0.011443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.684900</td>\n",
       "      <td>3.661191</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.667000</td>\n",
       "      <td>3.640113</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.647400</td>\n",
       "      <td>3.620883</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.630300</td>\n",
       "      <td>3.602739</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:57:18,492] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 with params: {'learning_rate': 0.00048122908288791095, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.357200</td>\n",
       "      <td>2.914744</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>0.077559</td>\n",
       "      <td>0.079426</td>\n",
       "      <td>0.060781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.653800</td>\n",
       "      <td>2.398060</td>\n",
       "      <td>0.459212</td>\n",
       "      <td>0.112354</td>\n",
       "      <td>0.120477</td>\n",
       "      <td>0.095868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.190800</td>\n",
       "      <td>2.004489</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>0.187297</td>\n",
       "      <td>0.163363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.777700</td>\n",
       "      <td>1.704744</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>0.253388</td>\n",
       "      <td>0.283047</td>\n",
       "      <td>0.247977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.470300</td>\n",
       "      <td>1.500384</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.328182</td>\n",
       "      <td>0.320135</td>\n",
       "      <td>0.286760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.202100</td>\n",
       "      <td>1.342596</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.353697</td>\n",
       "      <td>0.354584</td>\n",
       "      <td>0.324434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.979800</td>\n",
       "      <td>1.243715</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.345907</td>\n",
       "      <td>0.347833</td>\n",
       "      <td>0.322668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.820800</td>\n",
       "      <td>1.178197</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.390248</td>\n",
       "      <td>0.405868</td>\n",
       "      <td>0.383413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>1.147830</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.387361</td>\n",
       "      <td>0.407827</td>\n",
       "      <td>0.386347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>1.136222</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.452949</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.406524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>1.139279</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.506212</td>\n",
       "      <td>0.466106</td>\n",
       "      <td>0.463410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>1.133375</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.539227</td>\n",
       "      <td>0.474489</td>\n",
       "      <td>0.476034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>1.119079</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>0.534764</td>\n",
       "      <td>0.548263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>1.112256</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.570047</td>\n",
       "      <td>0.526386</td>\n",
       "      <td>0.530878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>1.102239</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.619502</td>\n",
       "      <td>0.546589</td>\n",
       "      <td>0.562435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>1.108464</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.622529</td>\n",
       "      <td>0.557966</td>\n",
       "      <td>0.572254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>1.092832</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.624968</td>\n",
       "      <td>0.558294</td>\n",
       "      <td>0.572535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>1.094292</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.588496</td>\n",
       "      <td>0.559173</td>\n",
       "      <td>0.566454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>1.107713</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.585564</td>\n",
       "      <td>0.544338</td>\n",
       "      <td>0.549804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>1.122882</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.619936</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>1.113936</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.648056</td>\n",
       "      <td>0.596792</td>\n",
       "      <td>0.612047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>1.106096</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.639343</td>\n",
       "      <td>0.590453</td>\n",
       "      <td>0.602719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>1.116645</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.640320</td>\n",
       "      <td>0.599415</td>\n",
       "      <td>0.608827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>1.114418</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.645556</td>\n",
       "      <td>0.601606</td>\n",
       "      <td>0.612448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>1.124063</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.647295</td>\n",
       "      <td>0.605446</td>\n",
       "      <td>0.614950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>1.133934</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.696133</td>\n",
       "      <td>0.624102</td>\n",
       "      <td>0.643672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>1.133446</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.699707</td>\n",
       "      <td>0.629219</td>\n",
       "      <td>0.648479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.133391</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.721214</td>\n",
       "      <td>0.640819</td>\n",
       "      <td>0.664227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>1.133578</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.718765</td>\n",
       "      <td>0.640787</td>\n",
       "      <td>0.663249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>1.133904</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.719152</td>\n",
       "      <td>0.644684</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 10:59:58,552] Trial 49 finished with value: 0.6657001579407984 and parameters: {'learning_rate': 0.00048122908288791095, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 with params: {'learning_rate': 9.107811820095339e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.714200</td>\n",
       "      <td>3.536433</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.429000</td>\n",
       "      <td>3.273173</td>\n",
       "      <td>0.304308</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.051202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.198500</td>\n",
       "      <td>3.047832</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.074066</td>\n",
       "      <td>0.087804</td>\n",
       "      <td>0.064932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.974400</td>\n",
       "      <td>2.842883</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.101605</td>\n",
       "      <td>0.079107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.803900</td>\n",
       "      <td>2.655440</td>\n",
       "      <td>0.464711</td>\n",
       "      <td>0.104470</td>\n",
       "      <td>0.118435</td>\n",
       "      <td>0.094138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.610400</td>\n",
       "      <td>2.494031</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>0.122075</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>0.111744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.447100</td>\n",
       "      <td>2.360298</td>\n",
       "      <td>0.521540</td>\n",
       "      <td>0.185666</td>\n",
       "      <td>0.161207</td>\n",
       "      <td>0.145845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.323200</td>\n",
       "      <td>2.238440</td>\n",
       "      <td>0.585701</td>\n",
       "      <td>0.261894</td>\n",
       "      <td>0.211301</td>\n",
       "      <td>0.199376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.186200</td>\n",
       "      <td>2.123917</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.261217</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.203380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.065500</td>\n",
       "      <td>2.032358</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>0.311936</td>\n",
       "      <td>0.256433</td>\n",
       "      <td>0.250014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.976400</td>\n",
       "      <td>1.944392</td>\n",
       "      <td>0.648946</td>\n",
       "      <td>0.312293</td>\n",
       "      <td>0.260705</td>\n",
       "      <td>0.252823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.864500</td>\n",
       "      <td>1.868356</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.325082</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.275015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.793800</td>\n",
       "      <td>1.799525</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.359706</td>\n",
       "      <td>0.294129</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.711600</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.356765</td>\n",
       "      <td>0.314164</td>\n",
       "      <td>0.305595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.641800</td>\n",
       "      <td>1.691718</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.373297</td>\n",
       "      <td>0.322018</td>\n",
       "      <td>0.314254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.580800</td>\n",
       "      <td>1.646840</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.351085</td>\n",
       "      <td>0.325379</td>\n",
       "      <td>0.314575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.506800</td>\n",
       "      <td>1.607913</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.357634</td>\n",
       "      <td>0.327492</td>\n",
       "      <td>0.314357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.466400</td>\n",
       "      <td>1.571772</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.361383</td>\n",
       "      <td>0.335873</td>\n",
       "      <td>0.321150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.443500</td>\n",
       "      <td>1.544064</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.369064</td>\n",
       "      <td>0.342968</td>\n",
       "      <td>0.329664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.394400</td>\n",
       "      <td>1.517098</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.366859</td>\n",
       "      <td>0.335906</td>\n",
       "      <td>0.323404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:01:47,042] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 with params: {'learning_rate': 0.0004834730103326943, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.369000</td>\n",
       "      <td>2.861857</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.070218</td>\n",
       "      <td>0.091584</td>\n",
       "      <td>0.070947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.538800</td>\n",
       "      <td>2.198837</td>\n",
       "      <td>0.544455</td>\n",
       "      <td>0.173113</td>\n",
       "      <td>0.185508</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.920900</td>\n",
       "      <td>1.714238</td>\n",
       "      <td>0.652612</td>\n",
       "      <td>0.291153</td>\n",
       "      <td>0.271372</td>\n",
       "      <td>0.256125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.454500</td>\n",
       "      <td>1.443116</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.289624</td>\n",
       "      <td>0.326541</td>\n",
       "      <td>0.296017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.123800</td>\n",
       "      <td>1.286849</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.376348</td>\n",
       "      <td>0.383276</td>\n",
       "      <td>0.358556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>1.158201</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.474337</td>\n",
       "      <td>0.417139</td>\n",
       "      <td>0.413708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.710700</td>\n",
       "      <td>1.125081</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.460633</td>\n",
       "      <td>0.424482</td>\n",
       "      <td>0.422232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.593500</td>\n",
       "      <td>1.065470</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.469753</td>\n",
       "      <td>0.451588</td>\n",
       "      <td>0.447138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>1.033097</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.498372</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>0.474471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>1.026721</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.466678</td>\n",
       "      <td>0.475832</td>\n",
       "      <td>0.466241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>1.021956</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.543754</td>\n",
       "      <td>0.515295</td>\n",
       "      <td>0.514687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>1.003153</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.583116</td>\n",
       "      <td>0.529357</td>\n",
       "      <td>0.540279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>1.019525</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.575895</td>\n",
       "      <td>0.561218</td>\n",
       "      <td>0.554162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.604187</td>\n",
       "      <td>0.548251</td>\n",
       "      <td>0.558527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>1.023230</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.643592</td>\n",
       "      <td>0.585765</td>\n",
       "      <td>0.597389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>1.029318</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.658110</td>\n",
       "      <td>0.611172</td>\n",
       "      <td>0.616275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>1.017445</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>0.615775</td>\n",
       "      <td>0.624381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>1.021797</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.685028</td>\n",
       "      <td>0.628686</td>\n",
       "      <td>0.638247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>1.056877</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.694061</td>\n",
       "      <td>0.634118</td>\n",
       "      <td>0.646336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>1.046919</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.695919</td>\n",
       "      <td>0.633637</td>\n",
       "      <td>0.648451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>1.054128</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.763609</td>\n",
       "      <td>0.686804</td>\n",
       "      <td>0.707176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>1.046148</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.737222</td>\n",
       "      <td>0.667308</td>\n",
       "      <td>0.684497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>1.052932</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.733316</td>\n",
       "      <td>0.668362</td>\n",
       "      <td>0.683295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>1.059906</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.765870</td>\n",
       "      <td>0.677860</td>\n",
       "      <td>0.701612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>1.057785</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.756329</td>\n",
       "      <td>0.678461</td>\n",
       "      <td>0.697549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>1.058969</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.771099</td>\n",
       "      <td>0.689784</td>\n",
       "      <td>0.709766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>1.064506</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.768730</td>\n",
       "      <td>0.687114</td>\n",
       "      <td>0.707573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>1.061793</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.768822</td>\n",
       "      <td>0.687351</td>\n",
       "      <td>0.707651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>1.063107</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.770335</td>\n",
       "      <td>0.688632</td>\n",
       "      <td>0.708772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>1.063862</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.764746</td>\n",
       "      <td>0.688996</td>\n",
       "      <td>0.708468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:04:27,844] Trial 51 finished with value: 0.7084684707060201 and parameters: {'learning_rate': 0.0004834730103326943, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52 with params: {'learning_rate': 0.00039887994036349827, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.394900</td>\n",
       "      <td>2.955765</td>\n",
       "      <td>0.384968</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>0.080665</td>\n",
       "      <td>0.060650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.673900</td>\n",
       "      <td>2.365986</td>\n",
       "      <td>0.493126</td>\n",
       "      <td>0.137401</td>\n",
       "      <td>0.138470</td>\n",
       "      <td>0.118278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.134500</td>\n",
       "      <td>1.924014</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.250127</td>\n",
       "      <td>0.216575</td>\n",
       "      <td>0.199654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.695100</td>\n",
       "      <td>1.617447</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.314709</td>\n",
       "      <td>0.303873</td>\n",
       "      <td>0.281029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.373900</td>\n",
       "      <td>1.424857</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.344543</td>\n",
       "      <td>0.345095</td>\n",
       "      <td>0.323087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.113400</td>\n",
       "      <td>1.256131</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.377087</td>\n",
       "      <td>0.368546</td>\n",
       "      <td>0.348366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>1.185640</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.418570</td>\n",
       "      <td>0.401823</td>\n",
       "      <td>0.390949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.754400</td>\n",
       "      <td>1.128993</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.447812</td>\n",
       "      <td>0.439078</td>\n",
       "      <td>0.428191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>1.073737</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.489256</td>\n",
       "      <td>0.464884</td>\n",
       "      <td>0.461887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>1.069554</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.486316</td>\n",
       "      <td>0.483673</td>\n",
       "      <td>0.475507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>1.041325</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.519727</td>\n",
       "      <td>0.494825</td>\n",
       "      <td>0.499055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>1.011978</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.535918</td>\n",
       "      <td>0.501681</td>\n",
       "      <td>0.504706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>1.016740</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.557905</td>\n",
       "      <td>0.523838</td>\n",
       "      <td>0.523423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.579448</td>\n",
       "      <td>0.535577</td>\n",
       "      <td>0.545831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>0.992554</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.582051</td>\n",
       "      <td>0.542975</td>\n",
       "      <td>0.548750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.990014</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.577994</td>\n",
       "      <td>0.550048</td>\n",
       "      <td>0.552195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.978607</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.627197</td>\n",
       "      <td>0.575091</td>\n",
       "      <td>0.584993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.631481</td>\n",
       "      <td>0.573295</td>\n",
       "      <td>0.581269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>1.018441</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.630907</td>\n",
       "      <td>0.568189</td>\n",
       "      <td>0.578248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.993347</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.646416</td>\n",
       "      <td>0.600094</td>\n",
       "      <td>0.609896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.620844</td>\n",
       "      <td>0.570966</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.990779</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.650790</td>\n",
       "      <td>0.595447</td>\n",
       "      <td>0.607342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.687404</td>\n",
       "      <td>0.623451</td>\n",
       "      <td>0.637133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>1.000709</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.696225</td>\n",
       "      <td>0.631782</td>\n",
       "      <td>0.648171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>1.001389</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.731522</td>\n",
       "      <td>0.647301</td>\n",
       "      <td>0.669166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>1.002546</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.728354</td>\n",
       "      <td>0.653879</td>\n",
       "      <td>0.672833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>1.010527</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.734113</td>\n",
       "      <td>0.650631</td>\n",
       "      <td>0.673252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>1.009808</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.727055</td>\n",
       "      <td>0.644848</td>\n",
       "      <td>0.665147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>1.008424</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.711064</td>\n",
       "      <td>0.636061</td>\n",
       "      <td>0.654950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>1.008775</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.711001</td>\n",
       "      <td>0.636061</td>\n",
       "      <td>0.654926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:07:11,058] Trial 52 finished with value: 0.6549264952515926 and parameters: {'learning_rate': 0.00039887994036349827, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53 with params: {'learning_rate': 0.0002928943933735547, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.484000</td>\n",
       "      <td>3.111150</td>\n",
       "      <td>0.332722</td>\n",
       "      <td>0.068052</td>\n",
       "      <td>0.066877</td>\n",
       "      <td>0.052022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.868300</td>\n",
       "      <td>2.576427</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.062754</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.078479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.387300</td>\n",
       "      <td>2.161392</td>\n",
       "      <td>0.541705</td>\n",
       "      <td>0.183576</td>\n",
       "      <td>0.169522</td>\n",
       "      <td>0.149780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.978800</td>\n",
       "      <td>1.853513</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.275747</td>\n",
       "      <td>0.246069</td>\n",
       "      <td>0.231218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.667000</td>\n",
       "      <td>1.609065</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.346683</td>\n",
       "      <td>0.337253</td>\n",
       "      <td>0.319111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.385700</td>\n",
       "      <td>1.423308</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.344229</td>\n",
       "      <td>0.350176</td>\n",
       "      <td>0.331828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.158900</td>\n",
       "      <td>1.317231</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.338989</td>\n",
       "      <td>0.350847</td>\n",
       "      <td>0.331712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.006300</td>\n",
       "      <td>1.250302</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.401559</td>\n",
       "      <td>0.406404</td>\n",
       "      <td>0.383116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>1.193455</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.417354</td>\n",
       "      <td>0.415687</td>\n",
       "      <td>0.402902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.752800</td>\n",
       "      <td>1.151004</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.446563</td>\n",
       "      <td>0.417804</td>\n",
       "      <td>0.412619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>1.115957</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.484571</td>\n",
       "      <td>0.455597</td>\n",
       "      <td>0.456486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>1.087175</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.486911</td>\n",
       "      <td>0.464951</td>\n",
       "      <td>0.464698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>1.072374</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.484919</td>\n",
       "      <td>0.473968</td>\n",
       "      <td>0.471306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>1.028297</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.491484</td>\n",
       "      <td>0.483441</td>\n",
       "      <td>0.479594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>1.034084</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.494170</td>\n",
       "      <td>0.491502</td>\n",
       "      <td>0.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>1.014803</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.486179</td>\n",
       "      <td>0.489747</td>\n",
       "      <td>0.482331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>1.006214</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.513046</td>\n",
       "      <td>0.496808</td>\n",
       "      <td>0.493549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>1.001636</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.512210</td>\n",
       "      <td>0.506529</td>\n",
       "      <td>0.502186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>1.006704</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.533816</td>\n",
       "      <td>0.519939</td>\n",
       "      <td>0.516792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.986087</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.559762</td>\n",
       "      <td>0.511564</td>\n",
       "      <td>0.516957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.997566</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.587091</td>\n",
       "      <td>0.540813</td>\n",
       "      <td>0.548545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.993319</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.587614</td>\n",
       "      <td>0.556777</td>\n",
       "      <td>0.560530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.988331</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.604330</td>\n",
       "      <td>0.558679</td>\n",
       "      <td>0.566339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.219400</td>\n",
       "      <td>0.991116</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.600871</td>\n",
       "      <td>0.564074</td>\n",
       "      <td>0.569273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>1.000345</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.611632</td>\n",
       "      <td>0.562868</td>\n",
       "      <td>0.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.607810</td>\n",
       "      <td>0.567517</td>\n",
       "      <td>0.575392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>1.001627</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.603735</td>\n",
       "      <td>0.548804</td>\n",
       "      <td>0.558940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.603723</td>\n",
       "      <td>0.564062</td>\n",
       "      <td>0.571943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>1.001331</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>0.575128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>1.001304</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.606678</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>0.575259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:09:57,464] Trial 53 finished with value: 0.5752588429704385 and parameters: {'learning_rate': 0.0002928943933735547, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54 with params: {'learning_rate': 0.00035364062130837363, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:52 < 00:56, 6.18 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.451700</td>\n",
       "      <td>3.015185</td>\n",
       "      <td>0.387718</td>\n",
       "      <td>0.062319</td>\n",
       "      <td>0.082513</td>\n",
       "      <td>0.064390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.733400</td>\n",
       "      <td>2.408134</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>0.161183</td>\n",
       "      <td>0.140126</td>\n",
       "      <td>0.121293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.178200</td>\n",
       "      <td>1.941123</td>\n",
       "      <td>0.592117</td>\n",
       "      <td>0.288665</td>\n",
       "      <td>0.223054</td>\n",
       "      <td>0.211805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.733900</td>\n",
       "      <td>1.653352</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.336346</td>\n",
       "      <td>0.321970</td>\n",
       "      <td>0.296584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.412500</td>\n",
       "      <td>1.449905</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.360553</td>\n",
       "      <td>0.360047</td>\n",
       "      <td>0.337106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.156600</td>\n",
       "      <td>1.291341</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.399144</td>\n",
       "      <td>0.376377</td>\n",
       "      <td>0.360926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.954400</td>\n",
       "      <td>1.217312</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.402603</td>\n",
       "      <td>0.390184</td>\n",
       "      <td>0.374349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>1.172167</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.438635</td>\n",
       "      <td>0.435674</td>\n",
       "      <td>0.419814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>1.124901</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.479875</td>\n",
       "      <td>0.457959</td>\n",
       "      <td>0.454604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>1.091669</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.478420</td>\n",
       "      <td>0.459984</td>\n",
       "      <td>0.456300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>1.051024</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.512325</td>\n",
       "      <td>0.491996</td>\n",
       "      <td>0.492717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>1.037669</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.493456</td>\n",
       "      <td>0.471182</td>\n",
       "      <td>0.470860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>1.018527</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.507225</td>\n",
       "      <td>0.504811</td>\n",
       "      <td>0.496091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.488547</td>\n",
       "      <td>0.487169</td>\n",
       "      <td>0.481344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.998372</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.536352</td>\n",
       "      <td>0.509863</td>\n",
       "      <td>0.506924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.987486</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.545492</td>\n",
       "      <td>0.523858</td>\n",
       "      <td>0.521132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.987390</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.583084</td>\n",
       "      <td>0.551303</td>\n",
       "      <td>0.553611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.992758</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.542246</td>\n",
       "      <td>0.536809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>1.005137</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.589579</td>\n",
       "      <td>0.557601</td>\n",
       "      <td>0.559279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.592936</td>\n",
       "      <td>0.540179</td>\n",
       "      <td>0.551352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:11:51,391] Trial 54 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 55 with params: {'learning_rate': 0.00047233886380038565, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.379100</td>\n",
       "      <td>2.864584</td>\n",
       "      <td>0.413382</td>\n",
       "      <td>0.072201</td>\n",
       "      <td>0.092149</td>\n",
       "      <td>0.070052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.537000</td>\n",
       "      <td>2.187127</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.216462</td>\n",
       "      <td>0.193287</td>\n",
       "      <td>0.178052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.911900</td>\n",
       "      <td>1.708606</td>\n",
       "      <td>0.657195</td>\n",
       "      <td>0.281888</td>\n",
       "      <td>0.283391</td>\n",
       "      <td>0.267012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.450600</td>\n",
       "      <td>1.438772</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.334110</td>\n",
       "      <td>0.343002</td>\n",
       "      <td>0.318887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.126500</td>\n",
       "      <td>1.301271</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.382255</td>\n",
       "      <td>0.399170</td>\n",
       "      <td>0.373238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>1.179702</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.432423</td>\n",
       "      <td>0.402893</td>\n",
       "      <td>0.393187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.720300</td>\n",
       "      <td>1.136972</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.479599</td>\n",
       "      <td>0.430050</td>\n",
       "      <td>0.431893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>1.088824</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.461792</td>\n",
       "      <td>0.454568</td>\n",
       "      <td>0.444536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>1.050336</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.495660</td>\n",
       "      <td>0.483888</td>\n",
       "      <td>0.479549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.041816</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.531984</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.491223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>1.025029</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.511522</td>\n",
       "      <td>0.490951</td>\n",
       "      <td>0.489099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>1.015297</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.581495</td>\n",
       "      <td>0.525675</td>\n",
       "      <td>0.535871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>1.013536</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.575187</td>\n",
       "      <td>0.552718</td>\n",
       "      <td>0.551137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>1.009296</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>0.547847</td>\n",
       "      <td>0.556777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>1.031180</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.584146</td>\n",
       "      <td>0.555835</td>\n",
       "      <td>0.558795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>1.020282</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.616240</td>\n",
       "      <td>0.579664</td>\n",
       "      <td>0.581231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>1.032530</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.694316</td>\n",
       "      <td>0.626960</td>\n",
       "      <td>0.639366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>1.037885</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.664446</td>\n",
       "      <td>0.609335</td>\n",
       "      <td>0.621778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>1.063641</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.693451</td>\n",
       "      <td>0.622667</td>\n",
       "      <td>0.637918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>1.056732</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.702258</td>\n",
       "      <td>0.627369</td>\n",
       "      <td>0.646541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>1.075861</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.719402</td>\n",
       "      <td>0.646791</td>\n",
       "      <td>0.662408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>1.069086</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.714130</td>\n",
       "      <td>0.639934</td>\n",
       "      <td>0.655929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>1.072879</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.702105</td>\n",
       "      <td>0.638126</td>\n",
       "      <td>0.651783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>1.085217</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.717664</td>\n",
       "      <td>0.638227</td>\n",
       "      <td>0.658421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>1.089018</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.720748</td>\n",
       "      <td>0.646197</td>\n",
       "      <td>0.663302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>1.084936</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.742630</td>\n",
       "      <td>0.662741</td>\n",
       "      <td>0.681759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>1.086027</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.742407</td>\n",
       "      <td>0.652099</td>\n",
       "      <td>0.673545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>1.085413</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.750022</td>\n",
       "      <td>0.660361</td>\n",
       "      <td>0.681488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>1.083736</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.740765</td>\n",
       "      <td>0.662854</td>\n",
       "      <td>0.679781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>1.084834</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.741257</td>\n",
       "      <td>0.662854</td>\n",
       "      <td>0.680190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:14:35,634] Trial 55 finished with value: 0.6801901055477038 and parameters: {'learning_rate': 0.00047233886380038565, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 56 with params: {'learning_rate': 0.0004469265177335313, 'weight_decay': 0.006, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.406900</td>\n",
       "      <td>2.930498</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.056332</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.063051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.622700</td>\n",
       "      <td>2.293921</td>\n",
       "      <td>0.514207</td>\n",
       "      <td>0.160987</td>\n",
       "      <td>0.158120</td>\n",
       "      <td>0.141506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.033100</td>\n",
       "      <td>1.823972</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.249364</td>\n",
       "      <td>0.237096</td>\n",
       "      <td>0.219629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.570300</td>\n",
       "      <td>1.530018</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.313555</td>\n",
       "      <td>0.322719</td>\n",
       "      <td>0.293859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.242100</td>\n",
       "      <td>1.341606</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.369722</td>\n",
       "      <td>0.375273</td>\n",
       "      <td>0.353406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>1.219474</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.452724</td>\n",
       "      <td>0.402559</td>\n",
       "      <td>0.394954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>1.164233</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.477247</td>\n",
       "      <td>0.430299</td>\n",
       "      <td>0.426788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>1.108343</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.477189</td>\n",
       "      <td>0.452408</td>\n",
       "      <td>0.450872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>1.073026</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.498157</td>\n",
       "      <td>0.479913</td>\n",
       "      <td>0.476481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>1.069025</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.470225</td>\n",
       "      <td>0.480435</td>\n",
       "      <td>0.465463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>1.035730</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.479024</td>\n",
       "      <td>0.488771</td>\n",
       "      <td>0.478286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>1.025901</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.564699</td>\n",
       "      <td>0.510731</td>\n",
       "      <td>0.518850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>1.041054</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.550338</td>\n",
       "      <td>0.527345</td>\n",
       "      <td>0.521010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>1.013412</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.588622</td>\n",
       "      <td>0.529857</td>\n",
       "      <td>0.542514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>1.038494</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.581086</td>\n",
       "      <td>0.550990</td>\n",
       "      <td>0.551848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>1.017735</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.573114</td>\n",
       "      <td>0.562024</td>\n",
       "      <td>0.556731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>1.016453</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.635070</td>\n",
       "      <td>0.598759</td>\n",
       "      <td>0.604524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>1.020647</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.625576</td>\n",
       "      <td>0.598638</td>\n",
       "      <td>0.600275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>1.050550</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.647441</td>\n",
       "      <td>0.597218</td>\n",
       "      <td>0.605492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>1.032070</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.684642</td>\n",
       "      <td>0.625355</td>\n",
       "      <td>0.638881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>1.050546</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.667345</td>\n",
       "      <td>0.609519</td>\n",
       "      <td>0.621325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>1.045466</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.702027</td>\n",
       "      <td>0.643568</td>\n",
       "      <td>0.658495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>1.052964</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.669874</td>\n",
       "      <td>0.610056</td>\n",
       "      <td>0.623576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>1.058576</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.701665</td>\n",
       "      <td>0.633309</td>\n",
       "      <td>0.649272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>1.060737</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.705398</td>\n",
       "      <td>0.635532</td>\n",
       "      <td>0.652339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.060449</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.705781</td>\n",
       "      <td>0.641170</td>\n",
       "      <td>0.655263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>1.063164</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.707971</td>\n",
       "      <td>0.635476</td>\n",
       "      <td>0.652337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>1.062647</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.710455</td>\n",
       "      <td>0.637359</td>\n",
       "      <td>0.654242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>1.060735</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.728670</td>\n",
       "      <td>0.649211</td>\n",
       "      <td>0.669097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>1.061715</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.728670</td>\n",
       "      <td>0.649211</td>\n",
       "      <td>0.669097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:17:23,023] Trial 56 finished with value: 0.669096956577762 and parameters: {'learning_rate': 0.0004469265177335313, 'weight_decay': 0.006, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 57 with params: {'learning_rate': 0.00011590846097366166, 'weight_decay': 0.008, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.56 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.695600</td>\n",
       "      <td>3.494214</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.367900</td>\n",
       "      <td>3.197479</td>\n",
       "      <td>0.336389</td>\n",
       "      <td>0.067889</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>0.055456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.106900</td>\n",
       "      <td>2.943980</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.071563</td>\n",
       "      <td>0.089932</td>\n",
       "      <td>0.065472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.853500</td>\n",
       "      <td>2.709832</td>\n",
       "      <td>0.449129</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>0.108624</td>\n",
       "      <td>0.081199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.654800</td>\n",
       "      <td>2.498431</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>0.121235</td>\n",
       "      <td>0.132121</td>\n",
       "      <td>0.107027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.432800</td>\n",
       "      <td>2.319435</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.225758</td>\n",
       "      <td>0.170258</td>\n",
       "      <td>0.156071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.247600</td>\n",
       "      <td>2.168369</td>\n",
       "      <td>0.570119</td>\n",
       "      <td>0.227944</td>\n",
       "      <td>0.199877</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.105100</td>\n",
       "      <td>2.039421</td>\n",
       "      <td>0.609533</td>\n",
       "      <td>0.292241</td>\n",
       "      <td>0.240557</td>\n",
       "      <td>0.227617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.955100</td>\n",
       "      <td>1.924816</td>\n",
       "      <td>0.649863</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.275214</td>\n",
       "      <td>0.267304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.822500</td>\n",
       "      <td>1.824992</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.351439</td>\n",
       "      <td>0.286024</td>\n",
       "      <td>0.277963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.722400</td>\n",
       "      <td>1.736194</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.340441</td>\n",
       "      <td>0.311626</td>\n",
       "      <td>0.302931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.601900</td>\n",
       "      <td>1.659710</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.345317</td>\n",
       "      <td>0.321450</td>\n",
       "      <td>0.312953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.527200</td>\n",
       "      <td>1.590738</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.363028</td>\n",
       "      <td>0.324947</td>\n",
       "      <td>0.314731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>1.538156</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.368692</td>\n",
       "      <td>0.349549</td>\n",
       "      <td>0.335804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.370900</td>\n",
       "      <td>1.498991</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.374664</td>\n",
       "      <td>0.367190</td>\n",
       "      <td>0.351428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.310200</td>\n",
       "      <td>1.462512</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>0.359414</td>\n",
       "      <td>0.343107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.238600</td>\n",
       "      <td>1.427281</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.361097</td>\n",
       "      <td>0.379195</td>\n",
       "      <td>0.358727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.195500</td>\n",
       "      <td>1.392685</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.373716</td>\n",
       "      <td>0.380821</td>\n",
       "      <td>0.360618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>1.371589</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.354787</td>\n",
       "      <td>0.371912</td>\n",
       "      <td>0.354717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.123600</td>\n",
       "      <td>1.349362</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.374940</td>\n",
       "      <td>0.387840</td>\n",
       "      <td>0.368629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:19:10,438] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 58 with params: {'learning_rate': 0.0004914006445390331, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.365600</td>\n",
       "      <td>2.839089</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>0.095160</td>\n",
       "      <td>0.072715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.503300</td>\n",
       "      <td>2.150170</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.219634</td>\n",
       "      <td>0.203513</td>\n",
       "      <td>0.187443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.868600</td>\n",
       "      <td>1.671841</td>\n",
       "      <td>0.661778</td>\n",
       "      <td>0.292147</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.270923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.407600</td>\n",
       "      <td>1.407047</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.332893</td>\n",
       "      <td>0.331176</td>\n",
       "      <td>0.311515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.280976</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.404110</td>\n",
       "      <td>0.404992</td>\n",
       "      <td>0.380806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>1.166589</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.401828</td>\n",
       "      <td>0.394801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>1.123120</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.484075</td>\n",
       "      <td>0.436432</td>\n",
       "      <td>0.437656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>1.074963</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.472172</td>\n",
       "      <td>0.451920</td>\n",
       "      <td>0.447358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>1.045120</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.518741</td>\n",
       "      <td>0.498317</td>\n",
       "      <td>0.494165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>1.037243</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.529697</td>\n",
       "      <td>0.476034</td>\n",
       "      <td>0.485881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>1.025511</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.543077</td>\n",
       "      <td>0.504697</td>\n",
       "      <td>0.507272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>1.013909</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.591933</td>\n",
       "      <td>0.540528</td>\n",
       "      <td>0.551966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>1.016880</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.589426</td>\n",
       "      <td>0.554604</td>\n",
       "      <td>0.555308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>1.014275</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.609563</td>\n",
       "      <td>0.561057</td>\n",
       "      <td>0.571199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>1.028327</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.608472</td>\n",
       "      <td>0.573036</td>\n",
       "      <td>0.577684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>1.028375</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.628060</td>\n",
       "      <td>0.578521</td>\n",
       "      <td>0.585176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>1.035211</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.633438</td>\n",
       "      <td>0.643469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>1.040939</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.688724</td>\n",
       "      <td>0.627336</td>\n",
       "      <td>0.641639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>1.075056</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.720736</td>\n",
       "      <td>0.641522</td>\n",
       "      <td>0.658516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>1.064068</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.734429</td>\n",
       "      <td>0.638307</td>\n",
       "      <td>0.663359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>1.078323</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.752916</td>\n",
       "      <td>0.654971</td>\n",
       "      <td>0.679483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>1.070013</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.729729</td>\n",
       "      <td>0.645209</td>\n",
       "      <td>0.664941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>1.078074</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.722690</td>\n",
       "      <td>0.633905</td>\n",
       "      <td>0.655383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>1.092268</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.736075</td>\n",
       "      <td>0.646394</td>\n",
       "      <td>0.667741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.094720</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.652857</td>\n",
       "      <td>0.674112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.092479</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.755860</td>\n",
       "      <td>0.659831</td>\n",
       "      <td>0.682791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>1.094565</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.745056</td>\n",
       "      <td>0.658619</td>\n",
       "      <td>0.677739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>1.095983</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.743827</td>\n",
       "      <td>0.655579</td>\n",
       "      <td>0.676378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>1.094405</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.741824</td>\n",
       "      <td>0.655273</td>\n",
       "      <td>0.675083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>1.095067</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.755066</td>\n",
       "      <td>0.659761</td>\n",
       "      <td>0.681231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:21:57,363] Trial 58 finished with value: 0.6812307962500159 and parameters: {'learning_rate': 0.0004914006445390331, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 59 with params: {'learning_rate': 0.00039949817551036056, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.54 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.442000</td>\n",
       "      <td>3.002767</td>\n",
       "      <td>0.362970</td>\n",
       "      <td>0.064639</td>\n",
       "      <td>0.075035</td>\n",
       "      <td>0.058433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.711100</td>\n",
       "      <td>2.399761</td>\n",
       "      <td>0.476627</td>\n",
       "      <td>0.144013</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>0.104316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.157200</td>\n",
       "      <td>1.935600</td>\n",
       "      <td>0.581118</td>\n",
       "      <td>0.242791</td>\n",
       "      <td>0.209735</td>\n",
       "      <td>0.194341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.704500</td>\n",
       "      <td>1.630519</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.312064</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>0.286388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.371300</td>\n",
       "      <td>1.406162</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.383137</td>\n",
       "      <td>0.348552</td>\n",
       "      <td>0.329649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.095100</td>\n",
       "      <td>1.255681</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.400380</td>\n",
       "      <td>0.375590</td>\n",
       "      <td>0.363731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>1.183387</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.416288</td>\n",
       "      <td>0.411393</td>\n",
       "      <td>0.396639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.746900</td>\n",
       "      <td>1.142109</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.453001</td>\n",
       "      <td>0.444008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.624500</td>\n",
       "      <td>1.091348</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.496621</td>\n",
       "      <td>0.475193</td>\n",
       "      <td>0.473621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>1.086735</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>0.473215</td>\n",
       "      <td>0.461521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>1.050343</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.497734</td>\n",
       "      <td>0.487883</td>\n",
       "      <td>0.482562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>1.034374</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.514899</td>\n",
       "      <td>0.497428</td>\n",
       "      <td>0.495866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>1.032021</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.548845</td>\n",
       "      <td>0.519435</td>\n",
       "      <td>0.515324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>1.005912</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.558487</td>\n",
       "      <td>0.520939</td>\n",
       "      <td>0.523661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>1.015936</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.582812</td>\n",
       "      <td>0.551703</td>\n",
       "      <td>0.555049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.991738</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.580842</td>\n",
       "      <td>0.562722</td>\n",
       "      <td>0.561817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.996292</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.565599</td>\n",
       "      <td>0.553510</td>\n",
       "      <td>0.549007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>1.003137</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.575545</td>\n",
       "      <td>0.569342</td>\n",
       "      <td>0.562045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>1.027351</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.632849</td>\n",
       "      <td>0.597645</td>\n",
       "      <td>0.600383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>1.019979</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.664708</td>\n",
       "      <td>0.606207</td>\n",
       "      <td>0.617540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:23:45,052] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 with params: {'learning_rate': 0.0004116756042903734, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.429100</td>\n",
       "      <td>2.969216</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.060523</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.063287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>2.343322</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.167556</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>0.136394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.089900</td>\n",
       "      <td>1.865589</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.251531</td>\n",
       "      <td>0.229937</td>\n",
       "      <td>0.215786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.627900</td>\n",
       "      <td>1.571006</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.331803</td>\n",
       "      <td>0.326744</td>\n",
       "      <td>0.298787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.293500</td>\n",
       "      <td>1.378654</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.350822</td>\n",
       "      <td>0.365323</td>\n",
       "      <td>0.340278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.033200</td>\n",
       "      <td>1.232098</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.409552</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.378027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.838700</td>\n",
       "      <td>1.174715</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.454435</td>\n",
       "      <td>0.425334</td>\n",
       "      <td>0.417051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.709600</td>\n",
       "      <td>1.121793</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.455974</td>\n",
       "      <td>0.456346</td>\n",
       "      <td>0.445931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>1.073797</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.495983</td>\n",
       "      <td>0.483221</td>\n",
       "      <td>0.477557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>1.077328</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.468425</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.469029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>1.032738</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.502445</td>\n",
       "      <td>0.493579</td>\n",
       "      <td>0.486911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>1.020167</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.494773</td>\n",
       "      <td>0.495781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>1.018919</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.572802</td>\n",
       "      <td>0.526908</td>\n",
       "      <td>0.528638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.993880</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.563391</td>\n",
       "      <td>0.522341</td>\n",
       "      <td>0.525203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>1.017230</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.547428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.562959</td>\n",
       "      <td>0.560894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.998197</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.601707</td>\n",
       "      <td>0.572292</td>\n",
       "      <td>0.574701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>1.011738</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.577214</td>\n",
       "      <td>0.572260</td>\n",
       "      <td>0.563098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>1.027858</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.630666</td>\n",
       "      <td>0.590414</td>\n",
       "      <td>0.593282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>1.017377</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.668424</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.613318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>1.029784</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.661954</td>\n",
       "      <td>0.594914</td>\n",
       "      <td>0.611564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>1.019245</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.660204</td>\n",
       "      <td>0.600651</td>\n",
       "      <td>0.613265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>1.028011</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.675808</td>\n",
       "      <td>0.610088</td>\n",
       "      <td>0.622504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>1.029026</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.676846</td>\n",
       "      <td>0.610610</td>\n",
       "      <td>0.622444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>1.031454</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.709699</td>\n",
       "      <td>0.640864</td>\n",
       "      <td>0.656076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>1.033691</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.705193</td>\n",
       "      <td>0.633034</td>\n",
       "      <td>0.648509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.710774</td>\n",
       "      <td>0.643046</td>\n",
       "      <td>0.657977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>1.034061</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.707759</td>\n",
       "      <td>0.639988</td>\n",
       "      <td>0.654916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>1.033581</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.640969</td>\n",
       "      <td>0.654922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>1.034685</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.702575</td>\n",
       "      <td>0.638969</td>\n",
       "      <td>0.652094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:26:33,448] Trial 60 finished with value: 0.652094443743539 and parameters: {'learning_rate': 0.0004116756042903734, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 61 with params: {'learning_rate': 5.2665021348115615e-05, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:46, 6.55 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.789700</td>\n",
       "      <td>3.662167</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.596900</td>\n",
       "      <td>3.496139</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.449300</td>\n",
       "      <td>3.339842</td>\n",
       "      <td>0.258478</td>\n",
       "      <td>0.073419</td>\n",
       "      <td>0.043838</td>\n",
       "      <td>0.038293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.297200</td>\n",
       "      <td>3.196322</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>0.066072</td>\n",
       "      <td>0.080003</td>\n",
       "      <td>0.063746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.182900</td>\n",
       "      <td>3.067702</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.089142</td>\n",
       "      <td>0.068519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.046500</td>\n",
       "      <td>2.949612</td>\n",
       "      <td>0.432631</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.100648</td>\n",
       "      <td>0.079728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.929600</td>\n",
       "      <td>2.845383</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.085636</td>\n",
       "      <td>0.105862</td>\n",
       "      <td>0.083764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.838600</td>\n",
       "      <td>2.746836</td>\n",
       "      <td>0.462878</td>\n",
       "      <td>0.104347</td>\n",
       "      <td>0.116830</td>\n",
       "      <td>0.092684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.736900</td>\n",
       "      <td>2.655128</td>\n",
       "      <td>0.472961</td>\n",
       "      <td>0.103823</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.096759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.650100</td>\n",
       "      <td>2.573918</td>\n",
       "      <td>0.482126</td>\n",
       "      <td>0.102708</td>\n",
       "      <td>0.127921</td>\n",
       "      <td>0.101672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:27:27,571] Trial 61 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 62 with params: {'learning_rate': 0.00047095570456479925, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.375800</td>\n",
       "      <td>2.854300</td>\n",
       "      <td>0.417965</td>\n",
       "      <td>0.071747</td>\n",
       "      <td>0.094663</td>\n",
       "      <td>0.072529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.523300</td>\n",
       "      <td>2.170348</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.218082</td>\n",
       "      <td>0.201488</td>\n",
       "      <td>0.185668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.894400</td>\n",
       "      <td>1.693023</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.304668</td>\n",
       "      <td>0.289229</td>\n",
       "      <td>0.274904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.432300</td>\n",
       "      <td>1.421276</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.328292</td>\n",
       "      <td>0.322277</td>\n",
       "      <td>0.306825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.118700</td>\n",
       "      <td>1.299342</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.391791</td>\n",
       "      <td>0.396256</td>\n",
       "      <td>0.370805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.890500</td>\n",
       "      <td>1.184991</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.429955</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>0.391347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.719600</td>\n",
       "      <td>1.128783</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.471472</td>\n",
       "      <td>0.433114</td>\n",
       "      <td>0.432850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.602600</td>\n",
       "      <td>1.090915</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.468813</td>\n",
       "      <td>0.462279</td>\n",
       "      <td>0.452417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>1.039922</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.518075</td>\n",
       "      <td>0.495817</td>\n",
       "      <td>0.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>1.033286</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.534079</td>\n",
       "      <td>0.481058</td>\n",
       "      <td>0.490710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>1.025157</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>0.500358</td>\n",
       "      <td>0.495254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>1.017871</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.588787</td>\n",
       "      <td>0.530636</td>\n",
       "      <td>0.539327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>1.016020</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.597167</td>\n",
       "      <td>0.547873</td>\n",
       "      <td>0.553929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>1.012549</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.599412</td>\n",
       "      <td>0.550144</td>\n",
       "      <td>0.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>1.024170</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.596279</td>\n",
       "      <td>0.560346</td>\n",
       "      <td>0.565528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>1.023801</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.607168</td>\n",
       "      <td>0.577701</td>\n",
       "      <td>0.577744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>1.033117</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.682714</td>\n",
       "      <td>0.615229</td>\n",
       "      <td>0.629577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>1.025120</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.701205</td>\n",
       "      <td>0.621397</td>\n",
       "      <td>0.641366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>1.062970</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.715003</td>\n",
       "      <td>0.624903</td>\n",
       "      <td>0.645137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>1.067493</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.707358</td>\n",
       "      <td>0.626190</td>\n",
       "      <td>0.646959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>1.074725</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.718561</td>\n",
       "      <td>0.639732</td>\n",
       "      <td>0.658762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>1.064746</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.712354</td>\n",
       "      <td>0.639330</td>\n",
       "      <td>0.654171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>1.081393</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.725853</td>\n",
       "      <td>0.638972</td>\n",
       "      <td>0.658492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>1.087488</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.727326</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>0.661929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>1.089267</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.743797</td>\n",
       "      <td>0.645734</td>\n",
       "      <td>0.670468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.086887</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.743351</td>\n",
       "      <td>0.644731</td>\n",
       "      <td>0.668792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>1.091100</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.745885</td>\n",
       "      <td>0.647322</td>\n",
       "      <td>0.670927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>1.091656</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.745957</td>\n",
       "      <td>0.649300</td>\n",
       "      <td>0.672560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.090090</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.745685</td>\n",
       "      <td>0.659965</td>\n",
       "      <td>0.679985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.091069</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.746178</td>\n",
       "      <td>0.660180</td>\n",
       "      <td>0.680520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:30:17,321] Trial 62 finished with value: 0.6805202979398901 and parameters: {'learning_rate': 0.00047095570456479925, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 63 with params: {'learning_rate': 0.00046468365490173844, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.380400</td>\n",
       "      <td>2.862822</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.071643</td>\n",
       "      <td>0.094448</td>\n",
       "      <td>0.072352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.534900</td>\n",
       "      <td>2.183012</td>\n",
       "      <td>0.554537</td>\n",
       "      <td>0.220558</td>\n",
       "      <td>0.196845</td>\n",
       "      <td>0.182979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.909100</td>\n",
       "      <td>1.705838</td>\n",
       "      <td>0.657195</td>\n",
       "      <td>0.318780</td>\n",
       "      <td>0.287225</td>\n",
       "      <td>0.273388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.447700</td>\n",
       "      <td>1.432134</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.319172</td>\n",
       "      <td>0.302125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.132700</td>\n",
       "      <td>1.307429</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.376053</td>\n",
       "      <td>0.387236</td>\n",
       "      <td>0.359178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>1.189990</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.419966</td>\n",
       "      <td>0.400794</td>\n",
       "      <td>0.390974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.730300</td>\n",
       "      <td>1.134468</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.452014</td>\n",
       "      <td>0.430361</td>\n",
       "      <td>0.428153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>1.091539</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.469102</td>\n",
       "      <td>0.460802</td>\n",
       "      <td>0.451696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>1.043919</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.522432</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.489914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>1.033424</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.528804</td>\n",
       "      <td>0.479153</td>\n",
       "      <td>0.487763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>1.025837</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.514546</td>\n",
       "      <td>0.500016</td>\n",
       "      <td>0.494474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>1.014644</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.586145</td>\n",
       "      <td>0.531561</td>\n",
       "      <td>0.539704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>1.016111</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.595854</td>\n",
       "      <td>0.546280</td>\n",
       "      <td>0.551045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>1.007980</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.597241</td>\n",
       "      <td>0.544001</td>\n",
       "      <td>0.557059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>1.019401</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.599933</td>\n",
       "      <td>0.559105</td>\n",
       "      <td>0.566403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>1.023377</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.552636</td>\n",
       "      <td>0.551995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>1.027936</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.659520</td>\n",
       "      <td>0.610988</td>\n",
       "      <td>0.618553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>1.025058</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.648646</td>\n",
       "      <td>0.586554</td>\n",
       "      <td>0.600698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>1.061489</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.695152</td>\n",
       "      <td>0.617507</td>\n",
       "      <td>0.635020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>1.059547</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.706458</td>\n",
       "      <td>0.625703</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>1.067946</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.698790</td>\n",
       "      <td>0.635384</td>\n",
       "      <td>0.650381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>1.061003</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.711292</td>\n",
       "      <td>0.639746</td>\n",
       "      <td>0.653833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>1.078086</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.722743</td>\n",
       "      <td>0.639088</td>\n",
       "      <td>0.657316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>1.082901</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.724863</td>\n",
       "      <td>0.642910</td>\n",
       "      <td>0.663314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>1.085740</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.722444</td>\n",
       "      <td>0.636680</td>\n",
       "      <td>0.656847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>1.083798</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.743305</td>\n",
       "      <td>0.653258</td>\n",
       "      <td>0.673946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>1.086763</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.744772</td>\n",
       "      <td>0.649348</td>\n",
       "      <td>0.671681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>1.087664</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.645338</td>\n",
       "      <td>0.669081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.086339</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.745431</td>\n",
       "      <td>0.660944</td>\n",
       "      <td>0.679706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>1.087444</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.745422</td>\n",
       "      <td>0.661159</td>\n",
       "      <td>0.679790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:33:06,407] Trial 63 finished with value: 0.6797903358336547 and parameters: {'learning_rate': 0.00046468365490173844, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 2}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 with params: {'learning_rate': 1.6488779238415127e-06, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:48 < 00:54, 6.45 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.889900</td>\n",
       "      <td>3.873888</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.873100</td>\n",
       "      <td>3.863904</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.869200</td>\n",
       "      <td>3.854980</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.857800</td>\n",
       "      <td>3.846702</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.003860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.854000</td>\n",
       "      <td>3.838834</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.005147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.842100</td>\n",
       "      <td>3.831679</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.035361</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.007329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.834700</td>\n",
       "      <td>3.824736</td>\n",
       "      <td>0.055912</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>0.008695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.829600</td>\n",
       "      <td>3.818130</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.008545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.823700</td>\n",
       "      <td>3.811769</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.030964</td>\n",
       "      <td>0.008995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.817500</td>\n",
       "      <td>3.805781</td>\n",
       "      <td>0.097159</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.032104</td>\n",
       "      <td>0.008989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.812400</td>\n",
       "      <td>3.799911</td>\n",
       "      <td>0.108158</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.008853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.807100</td>\n",
       "      <td>3.794354</td>\n",
       "      <td>0.120073</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>0.034514</td>\n",
       "      <td>0.008864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.803100</td>\n",
       "      <td>3.789025</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.008873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.794700</td>\n",
       "      <td>3.783941</td>\n",
       "      <td>0.147571</td>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.037793</td>\n",
       "      <td>0.009357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.793000</td>\n",
       "      <td>3.779163</td>\n",
       "      <td>0.153987</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>0.009436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.785800</td>\n",
       "      <td>3.774624</td>\n",
       "      <td>0.160403</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.009422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.783800</td>\n",
       "      <td>3.770557</td>\n",
       "      <td>0.167736</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.019996</td>\n",
       "      <td>0.008806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.783100</td>\n",
       "      <td>3.766749</td>\n",
       "      <td>0.170486</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.020648</td>\n",
       "      <td>0.009369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.778600</td>\n",
       "      <td>3.763298</td>\n",
       "      <td>0.175985</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.010912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.780500</td>\n",
       "      <td>3.760195</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.021938</td>\n",
       "      <td>0.010821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:34:55,632] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 65 with params: {'learning_rate': 0.0003634999059172566, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:48 < 00:54, 6.41 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.462200</td>\n",
       "      <td>3.028636</td>\n",
       "      <td>0.352887</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.054350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.740800</td>\n",
       "      <td>2.419410</td>\n",
       "      <td>0.512374</td>\n",
       "      <td>0.184093</td>\n",
       "      <td>0.157874</td>\n",
       "      <td>0.140131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.184800</td>\n",
       "      <td>1.948323</td>\n",
       "      <td>0.598533</td>\n",
       "      <td>0.259347</td>\n",
       "      <td>0.230548</td>\n",
       "      <td>0.215589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.729800</td>\n",
       "      <td>1.647001</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.310253</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.287635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.397600</td>\n",
       "      <td>1.441891</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.329070</td>\n",
       "      <td>0.352840</td>\n",
       "      <td>0.327327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.131400</td>\n",
       "      <td>1.281734</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.416190</td>\n",
       "      <td>0.372216</td>\n",
       "      <td>0.356785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>1.207245</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.469200</td>\n",
       "      <td>0.425335</td>\n",
       "      <td>0.417109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>1.157291</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.449430</td>\n",
       "      <td>0.441028</td>\n",
       "      <td>0.428803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>1.105191</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.493488</td>\n",
       "      <td>0.472916</td>\n",
       "      <td>0.468836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>1.089120</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.462096</td>\n",
       "      <td>0.475126</td>\n",
       "      <td>0.462520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>1.051232</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>0.485861</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>1.038826</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.519681</td>\n",
       "      <td>0.480333</td>\n",
       "      <td>0.484577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>1.028925</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.493290</td>\n",
       "      <td>0.498382</td>\n",
       "      <td>0.489295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>1.007689</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.493409</td>\n",
       "      <td>0.496419</td>\n",
       "      <td>0.490009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>1.008991</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.576592</td>\n",
       "      <td>0.528840</td>\n",
       "      <td>0.532363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.612379</td>\n",
       "      <td>0.555895</td>\n",
       "      <td>0.560707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.995938</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.582076</td>\n",
       "      <td>0.556359</td>\n",
       "      <td>0.557077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.992836</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.616542</td>\n",
       "      <td>0.572853</td>\n",
       "      <td>0.579365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>1.009562</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.613760</td>\n",
       "      <td>0.573019</td>\n",
       "      <td>0.576504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>1.005500</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.624594</td>\n",
       "      <td>0.568282</td>\n",
       "      <td>0.580647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:36:45,750] Trial 65 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 66 with params: {'learning_rate': 8.876292630413755e-05, 'weight_decay': 0.005, 'adam_beta1': 0.92, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:48, 6.48 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.728900</td>\n",
       "      <td>3.556752</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.453200</td>\n",
       "      <td>3.305208</td>\n",
       "      <td>0.249313</td>\n",
       "      <td>0.073401</td>\n",
       "      <td>0.041764</td>\n",
       "      <td>0.036654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.230300</td>\n",
       "      <td>3.083795</td>\n",
       "      <td>0.395050</td>\n",
       "      <td>0.056053</td>\n",
       "      <td>0.083983</td>\n",
       "      <td>0.062981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.012100</td>\n",
       "      <td>2.882319</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.091044</td>\n",
       "      <td>0.099883</td>\n",
       "      <td>0.078209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.844800</td>\n",
       "      <td>2.699159</td>\n",
       "      <td>0.452796</td>\n",
       "      <td>0.084107</td>\n",
       "      <td>0.112012</td>\n",
       "      <td>0.085620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.653800</td>\n",
       "      <td>2.537355</td>\n",
       "      <td>0.487626</td>\n",
       "      <td>0.123143</td>\n",
       "      <td>0.132122</td>\n",
       "      <td>0.107957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.491300</td>\n",
       "      <td>2.400679</td>\n",
       "      <td>0.510541</td>\n",
       "      <td>0.152738</td>\n",
       "      <td>0.149576</td>\n",
       "      <td>0.131549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.368700</td>\n",
       "      <td>2.278979</td>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.253048</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>0.185555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.233200</td>\n",
       "      <td>2.167153</td>\n",
       "      <td>0.587534</td>\n",
       "      <td>0.284570</td>\n",
       "      <td>0.214568</td>\n",
       "      <td>0.205947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.113200</td>\n",
       "      <td>2.072596</td>\n",
       "      <td>0.614115</td>\n",
       "      <td>0.311682</td>\n",
       "      <td>0.241223</td>\n",
       "      <td>0.235218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:37:40,536] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 67 with params: {'learning_rate': 0.0004377148075584707, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.346700</td>\n",
       "      <td>2.839275</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>0.099959</td>\n",
       "      <td>0.077373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.524600</td>\n",
       "      <td>2.179063</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.214939</td>\n",
       "      <td>0.194352</td>\n",
       "      <td>0.179648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.922500</td>\n",
       "      <td>1.714153</td>\n",
       "      <td>0.662695</td>\n",
       "      <td>0.284322</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>0.264468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.477500</td>\n",
       "      <td>1.447072</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.318125</td>\n",
       "      <td>0.320565</td>\n",
       "      <td>0.300572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.167900</td>\n",
       "      <td>1.306155</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.344357</td>\n",
       "      <td>0.364608</td>\n",
       "      <td>0.334655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.946500</td>\n",
       "      <td>1.175856</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.427880</td>\n",
       "      <td>0.393121</td>\n",
       "      <td>0.380932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.125531</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.468888</td>\n",
       "      <td>0.431212</td>\n",
       "      <td>0.424421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>1.083332</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.485685</td>\n",
       "      <td>0.462419</td>\n",
       "      <td>0.457754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>1.042304</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.486381</td>\n",
       "      <td>0.476909</td>\n",
       "      <td>0.473124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.428600</td>\n",
       "      <td>1.008617</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.482798</td>\n",
       "      <td>0.477918</td>\n",
       "      <td>0.474812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>1.003413</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.526688</td>\n",
       "      <td>0.509356</td>\n",
       "      <td>0.506261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.987084</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.568905</td>\n",
       "      <td>0.523461</td>\n",
       "      <td>0.527796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.997823</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.587881</td>\n",
       "      <td>0.552281</td>\n",
       "      <td>0.557317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.966581</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.612523</td>\n",
       "      <td>0.557658</td>\n",
       "      <td>0.570347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.981741</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.610823</td>\n",
       "      <td>0.552457</td>\n",
       "      <td>0.560632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.986816</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.622716</td>\n",
       "      <td>0.565085</td>\n",
       "      <td>0.576136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.973930</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.627930</td>\n",
       "      <td>0.578342</td>\n",
       "      <td>0.585332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.983595</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.653938</td>\n",
       "      <td>0.588018</td>\n",
       "      <td>0.604750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>1.000292</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.643613</td>\n",
       "      <td>0.583084</td>\n",
       "      <td>0.597112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.669397</td>\n",
       "      <td>0.595679</td>\n",
       "      <td>0.610136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>1.016506</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.700367</td>\n",
       "      <td>0.627837</td>\n",
       "      <td>0.647199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>1.001910</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.693056</td>\n",
       "      <td>0.627101</td>\n",
       "      <td>0.642895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>1.008494</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>0.635677</td>\n",
       "      <td>0.654750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>1.006464</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.742594</td>\n",
       "      <td>0.662393</td>\n",
       "      <td>0.681791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>1.008137</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.747010</td>\n",
       "      <td>0.667247</td>\n",
       "      <td>0.687348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>1.009449</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.666298</td>\n",
       "      <td>0.685212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>1.014827</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.746321</td>\n",
       "      <td>0.666033</td>\n",
       "      <td>0.686509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>1.015473</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.744090</td>\n",
       "      <td>0.665598</td>\n",
       "      <td>0.685037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>1.014901</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.742529</td>\n",
       "      <td>0.668098</td>\n",
       "      <td>0.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1.015381</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.742952</td>\n",
       "      <td>0.667974</td>\n",
       "      <td>0.685049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:40:23,545] Trial 67 finished with value: 0.6850486432479292 and parameters: {'learning_rate': 0.0004377148075584707, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 68 with params: {'learning_rate': 0.00024334986693978525, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.37 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.527200</td>\n",
       "      <td>3.180024</td>\n",
       "      <td>0.297892</td>\n",
       "      <td>0.071245</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>0.046627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.956900</td>\n",
       "      <td>2.677668</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>0.106677</td>\n",
       "      <td>0.076937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.504800</td>\n",
       "      <td>2.271835</td>\n",
       "      <td>0.532539</td>\n",
       "      <td>0.185983</td>\n",
       "      <td>0.165309</td>\n",
       "      <td>0.148810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.112600</td>\n",
       "      <td>1.963127</td>\n",
       "      <td>0.626031</td>\n",
       "      <td>0.290409</td>\n",
       "      <td>0.248335</td>\n",
       "      <td>0.230793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>1.726287</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.345892</td>\n",
       "      <td>0.312999</td>\n",
       "      <td>0.301317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.539300</td>\n",
       "      <td>1.538023</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.347049</td>\n",
       "      <td>0.335377</td>\n",
       "      <td>0.319839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.322700</td>\n",
       "      <td>1.416404</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.328737</td>\n",
       "      <td>0.337407</td>\n",
       "      <td>0.318635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.170800</td>\n",
       "      <td>1.339950</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.416125</td>\n",
       "      <td>0.392454</td>\n",
       "      <td>0.372696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.036300</td>\n",
       "      <td>1.263529</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.393845</td>\n",
       "      <td>0.396704</td>\n",
       "      <td>0.382614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>1.212863</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.439292</td>\n",
       "      <td>0.408694</td>\n",
       "      <td>0.400766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.837800</td>\n",
       "      <td>1.174678</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.443645</td>\n",
       "      <td>0.413701</td>\n",
       "      <td>0.407718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>1.142638</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.448886</td>\n",
       "      <td>0.428732</td>\n",
       "      <td>0.419846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>1.111884</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.464091</td>\n",
       "      <td>0.442257</td>\n",
       "      <td>0.434270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>1.085458</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.480715</td>\n",
       "      <td>0.466811</td>\n",
       "      <td>0.463108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>1.068735</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.488513</td>\n",
       "      <td>0.475473</td>\n",
       "      <td>0.470801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>1.064325</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.481292</td>\n",
       "      <td>0.472094</td>\n",
       "      <td>0.465505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>1.047955</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.485573</td>\n",
       "      <td>0.490671</td>\n",
       "      <td>0.482438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>1.036113</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.482453</td>\n",
       "      <td>0.483873</td>\n",
       "      <td>0.477958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>1.034653</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.486675</td>\n",
       "      <td>0.490967</td>\n",
       "      <td>0.482646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>1.012332</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.499828</td>\n",
       "      <td>0.491989</td>\n",
       "      <td>0.488972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:42:14,224] Trial 68 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 69 with params: {'learning_rate': 0.0004568208696615515, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.340400</td>\n",
       "      <td>2.836654</td>\n",
       "      <td>0.417965</td>\n",
       "      <td>0.069412</td>\n",
       "      <td>0.097472</td>\n",
       "      <td>0.075715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.523500</td>\n",
       "      <td>2.179508</td>\n",
       "      <td>0.554537</td>\n",
       "      <td>0.173029</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>0.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.920500</td>\n",
       "      <td>1.714251</td>\n",
       "      <td>0.656279</td>\n",
       "      <td>0.283159</td>\n",
       "      <td>0.274885</td>\n",
       "      <td>0.255483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.475500</td>\n",
       "      <td>1.457816</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.304425</td>\n",
       "      <td>0.336298</td>\n",
       "      <td>0.303714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.159500</td>\n",
       "      <td>1.296005</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.331970</td>\n",
       "      <td>0.373923</td>\n",
       "      <td>0.342362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>1.171884</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.443897</td>\n",
       "      <td>0.394732</td>\n",
       "      <td>0.387395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>1.118713</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.474563</td>\n",
       "      <td>0.433248</td>\n",
       "      <td>0.429618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>1.073712</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.462648</td>\n",
       "      <td>0.450368</td>\n",
       "      <td>0.444452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>1.051094</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.489315</td>\n",
       "      <td>0.475471</td>\n",
       "      <td>0.470106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.016900</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.463876</td>\n",
       "      <td>0.466905</td>\n",
       "      <td>0.459667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>1.006981</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.520470</td>\n",
       "      <td>0.505178</td>\n",
       "      <td>0.504377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.992533</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.584634</td>\n",
       "      <td>0.535310</td>\n",
       "      <td>0.544473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.987361</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.610077</td>\n",
       "      <td>0.563687</td>\n",
       "      <td>0.571133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.645572</td>\n",
       "      <td>0.567704</td>\n",
       "      <td>0.585479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.995744</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.580366</td>\n",
       "      <td>0.597517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.994303</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.615482</td>\n",
       "      <td>0.564985</td>\n",
       "      <td>0.572890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.990037</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.623213</td>\n",
       "      <td>0.585448</td>\n",
       "      <td>0.589057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.991703</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.642627</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.599138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>1.014978</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.639105</td>\n",
       "      <td>0.578878</td>\n",
       "      <td>0.593336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>1.003380</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.719034</td>\n",
       "      <td>0.646327</td>\n",
       "      <td>0.665651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.018589</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.735684</td>\n",
       "      <td>0.655625</td>\n",
       "      <td>0.676434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>1.005692</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.765945</td>\n",
       "      <td>0.679185</td>\n",
       "      <td>0.699975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>1.014458</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.711419</td>\n",
       "      <td>0.650367</td>\n",
       "      <td>0.663289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>1.018302</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.741962</td>\n",
       "      <td>0.662658</td>\n",
       "      <td>0.682695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>1.015723</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>0.661123</td>\n",
       "      <td>0.682641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1.016329</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.769143</td>\n",
       "      <td>0.681519</td>\n",
       "      <td>0.702852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>1.020935</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.770018</td>\n",
       "      <td>0.675942</td>\n",
       "      <td>0.701184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>1.021038</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.771412</td>\n",
       "      <td>0.679222</td>\n",
       "      <td>0.703119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>1.020699</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.771186</td>\n",
       "      <td>0.681052</td>\n",
       "      <td>0.704035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>1.021215</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.773694</td>\n",
       "      <td>0.681416</td>\n",
       "      <td>0.705877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:45:01,432] Trial 69 finished with value: 0.7058767245019291 and parameters: {'learning_rate': 0.0004568208696615515, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 70 with params: {'learning_rate': 1.1328698100804768e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:50 < 00:55, 6.32 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.862300</td>\n",
       "      <td>3.818775</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>0.027324</td>\n",
       "      <td>0.006749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.798600</td>\n",
       "      <td>3.764533</td>\n",
       "      <td>0.170486</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.020307</td>\n",
       "      <td>0.008707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.755900</td>\n",
       "      <td>3.717805</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.011553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.711600</td>\n",
       "      <td>3.678671</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.010068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.684000</td>\n",
       "      <td>3.642524</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.010592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.643200</td>\n",
       "      <td>3.608618</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.611100</td>\n",
       "      <td>3.576872</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.584900</td>\n",
       "      <td>3.546365</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.555000</td>\n",
       "      <td>3.517286</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.531000</td>\n",
       "      <td>3.490431</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.507100</td>\n",
       "      <td>3.464976</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.023581</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.010893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.478900</td>\n",
       "      <td>3.441542</td>\n",
       "      <td>0.191567</td>\n",
       "      <td>0.043611</td>\n",
       "      <td>0.024325</td>\n",
       "      <td>0.013361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.465100</td>\n",
       "      <td>3.418977</td>\n",
       "      <td>0.202566</td>\n",
       "      <td>0.083655</td>\n",
       "      <td>0.027594</td>\n",
       "      <td>0.018283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.435700</td>\n",
       "      <td>3.397745</td>\n",
       "      <td>0.219065</td>\n",
       "      <td>0.073744</td>\n",
       "      <td>0.032364</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.423400</td>\n",
       "      <td>3.378357</td>\n",
       "      <td>0.236480</td>\n",
       "      <td>0.076506</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>0.031482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.397900</td>\n",
       "      <td>3.359749</td>\n",
       "      <td>0.262145</td>\n",
       "      <td>0.076113</td>\n",
       "      <td>0.045138</td>\n",
       "      <td>0.040209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.381200</td>\n",
       "      <td>3.342898</td>\n",
       "      <td>0.281393</td>\n",
       "      <td>0.075653</td>\n",
       "      <td>0.050710</td>\n",
       "      <td>0.045952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.375200</td>\n",
       "      <td>3.327361</td>\n",
       "      <td>0.296975</td>\n",
       "      <td>0.075138</td>\n",
       "      <td>0.055506</td>\n",
       "      <td>0.050145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.359000</td>\n",
       "      <td>3.312917</td>\n",
       "      <td>0.318057</td>\n",
       "      <td>0.074101</td>\n",
       "      <td>0.061656</td>\n",
       "      <td>0.055513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.350600</td>\n",
       "      <td>3.300085</td>\n",
       "      <td>0.324473</td>\n",
       "      <td>0.072123</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>0.056973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:46:52,832] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 71 with params: {'learning_rate': 9.262456188329795e-05, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:06, 6.91 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.714200</td>\n",
       "      <td>3.539336</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.433900</td>\n",
       "      <td>3.283654</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.072798</td>\n",
       "      <td>0.047023</td>\n",
       "      <td>0.041189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.208600</td>\n",
       "      <td>3.062359</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>0.055558</td>\n",
       "      <td>0.085719</td>\n",
       "      <td>0.063914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.989300</td>\n",
       "      <td>2.859336</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.101387</td>\n",
       "      <td>0.078108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.820300</td>\n",
       "      <td>2.674308</td>\n",
       "      <td>0.450962</td>\n",
       "      <td>0.083364</td>\n",
       "      <td>0.110814</td>\n",
       "      <td>0.082061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:47:18,886] Trial 71 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 with params: {'learning_rate': 0.00038768310345292965, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.387100</td>\n",
       "      <td>2.915336</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.074075</td>\n",
       "      <td>0.091096</td>\n",
       "      <td>0.069985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.619600</td>\n",
       "      <td>2.281554</td>\n",
       "      <td>0.539872</td>\n",
       "      <td>0.200317</td>\n",
       "      <td>0.176798</td>\n",
       "      <td>0.165078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.041700</td>\n",
       "      <td>1.818793</td>\n",
       "      <td>0.641613</td>\n",
       "      <td>0.289496</td>\n",
       "      <td>0.270814</td>\n",
       "      <td>0.257357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.600900</td>\n",
       "      <td>1.536779</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.332667</td>\n",
       "      <td>0.314596</td>\n",
       "      <td>0.298794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.288500</td>\n",
       "      <td>1.370311</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.332330</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>0.329426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.055500</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.428875</td>\n",
       "      <td>0.385229</td>\n",
       "      <td>0.372962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>1.165143</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.451837</td>\n",
       "      <td>0.406814</td>\n",
       "      <td>0.399504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.123685</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.471871</td>\n",
       "      <td>0.447642</td>\n",
       "      <td>0.442903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.608600</td>\n",
       "      <td>1.070622</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.500450</td>\n",
       "      <td>0.469464</td>\n",
       "      <td>0.468154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>1.042529</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.499573</td>\n",
       "      <td>0.467136</td>\n",
       "      <td>0.466960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>1.027206</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.494497</td>\n",
       "      <td>0.487890</td>\n",
       "      <td>0.484520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>1.006381</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.505553</td>\n",
       "      <td>0.489707</td>\n",
       "      <td>0.490760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>1.009745</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.551452</td>\n",
       "      <td>0.513964</td>\n",
       "      <td>0.516963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.975426</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.569449</td>\n",
       "      <td>0.516336</td>\n",
       "      <td>0.524946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.990812</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.588929</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>0.538007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.981176</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.575496</td>\n",
       "      <td>0.530413</td>\n",
       "      <td>0.534976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.985545</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.585543</td>\n",
       "      <td>0.553424</td>\n",
       "      <td>0.553446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.987449</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.609326</td>\n",
       "      <td>0.556345</td>\n",
       "      <td>0.565506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.647326</td>\n",
       "      <td>0.573759</td>\n",
       "      <td>0.592733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.992038</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.654887</td>\n",
       "      <td>0.572066</td>\n",
       "      <td>0.591204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.995374</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.652092</td>\n",
       "      <td>0.581239</td>\n",
       "      <td>0.599551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.990029</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.686148</td>\n",
       "      <td>0.609340</td>\n",
       "      <td>0.628983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.679871</td>\n",
       "      <td>0.600806</td>\n",
       "      <td>0.622854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.673373</td>\n",
       "      <td>0.594966</td>\n",
       "      <td>0.613560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>1.003882</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.722967</td>\n",
       "      <td>0.630374</td>\n",
       "      <td>0.656729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>1.007005</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.717029</td>\n",
       "      <td>0.625594</td>\n",
       "      <td>0.650473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>1.010647</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.722115</td>\n",
       "      <td>0.628499</td>\n",
       "      <td>0.654852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>1.010327</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.723311</td>\n",
       "      <td>0.630974</td>\n",
       "      <td>0.657336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>1.008103</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.720660</td>\n",
       "      <td>0.631430</td>\n",
       "      <td>0.656364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>1.008577</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.721023</td>\n",
       "      <td>0.631534</td>\n",
       "      <td>0.656603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:50:05,465] Trial 72 finished with value: 0.6566034200505996 and parameters: {'learning_rate': 0.00038768310345292965, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 73 with params: {'learning_rate': 4.818236733162463e-06, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:46, 6.59 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.879700</td>\n",
       "      <td>3.853867</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.845700</td>\n",
       "      <td>3.828573</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.006056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.828000</td>\n",
       "      <td>3.805074</td>\n",
       "      <td>0.091659</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.031131</td>\n",
       "      <td>0.008063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.803600</td>\n",
       "      <td>3.782104</td>\n",
       "      <td>0.148488</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.009631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.788000</td>\n",
       "      <td>3.759520</td>\n",
       "      <td>0.175985</td>\n",
       "      <td>0.033490</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.010713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.762200</td>\n",
       "      <td>3.739798</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.021585</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.744200</td>\n",
       "      <td>3.722479</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.023718</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>0.011488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.730100</td>\n",
       "      <td>3.706725</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.010476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.716100</td>\n",
       "      <td>3.692400</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.009892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.702900</td>\n",
       "      <td>3.679285</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:50:59,251] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 74 with params: {'learning_rate': 0.0003878516193309889, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:48 < 00:54, 6.44 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.428700</td>\n",
       "      <td>2.974332</td>\n",
       "      <td>0.392301</td>\n",
       "      <td>0.059899</td>\n",
       "      <td>0.083723</td>\n",
       "      <td>0.063907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.682600</td>\n",
       "      <td>2.351691</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.154410</td>\n",
       "      <td>0.137279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.112300</td>\n",
       "      <td>1.884209</td>\n",
       "      <td>0.608616</td>\n",
       "      <td>0.260426</td>\n",
       "      <td>0.234864</td>\n",
       "      <td>0.223412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>1.597839</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.308434</td>\n",
       "      <td>0.321418</td>\n",
       "      <td>0.292101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.334000</td>\n",
       "      <td>1.400643</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.372516</td>\n",
       "      <td>0.357712</td>\n",
       "      <td>0.337845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.079500</td>\n",
       "      <td>1.247879</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.439829</td>\n",
       "      <td>0.389756</td>\n",
       "      <td>0.378361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>1.184591</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.446223</td>\n",
       "      <td>0.416416</td>\n",
       "      <td>0.406605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>1.137709</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.470321</td>\n",
       "      <td>0.449616</td>\n",
       "      <td>0.441536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>1.100069</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.492562</td>\n",
       "      <td>0.479474</td>\n",
       "      <td>0.474116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>1.083221</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.480929</td>\n",
       "      <td>0.472236</td>\n",
       "      <td>0.465734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>1.022771</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.502885</td>\n",
       "      <td>0.481448</td>\n",
       "      <td>0.482038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>1.015734</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.515255</td>\n",
       "      <td>0.478857</td>\n",
       "      <td>0.485387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>1.012520</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>0.508775</td>\n",
       "      <td>0.503128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>0.990441</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.537996</td>\n",
       "      <td>0.506394</td>\n",
       "      <td>0.506775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.994117</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.557578</td>\n",
       "      <td>0.527939</td>\n",
       "      <td>0.530133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>0.979526</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.598197</td>\n",
       "      <td>0.563933</td>\n",
       "      <td>0.563965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.978643</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.560532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.983586</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.625650</td>\n",
       "      <td>0.586283</td>\n",
       "      <td>0.590378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.997226</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.652256</td>\n",
       "      <td>0.581564</td>\n",
       "      <td>0.597634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>1.000554</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.658714</td>\n",
       "      <td>0.577340</td>\n",
       "      <td>0.598631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:52:48,713] Trial 74 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 75 with params: {'learning_rate': 0.0002224376724307852, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:12, 6.62 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.553700</td>\n",
       "      <td>3.235558</td>\n",
       "      <td>0.218148</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.024391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.031600</td>\n",
       "      <td>2.780749</td>\n",
       "      <td>0.428048</td>\n",
       "      <td>0.064524</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.619100</td>\n",
       "      <td>2.394619</td>\n",
       "      <td>0.487626</td>\n",
       "      <td>0.140014</td>\n",
       "      <td>0.130567</td>\n",
       "      <td>0.108778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.244800</td>\n",
       "      <td>2.088908</td>\n",
       "      <td>0.578368</td>\n",
       "      <td>0.217201</td>\n",
       "      <td>0.199356</td>\n",
       "      <td>0.184077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.957800</td>\n",
       "      <td>1.845086</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.264783</td>\n",
       "      <td>0.260424</td>\n",
       "      <td>0.242358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:53:15,945] Trial 75 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 76 with params: {'learning_rate': 0.00035243578128822174, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:40, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.431300</td>\n",
       "      <td>3.018752</td>\n",
       "      <td>0.382218</td>\n",
       "      <td>0.061470</td>\n",
       "      <td>0.081186</td>\n",
       "      <td>0.062280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.751800</td>\n",
       "      <td>2.445083</td>\n",
       "      <td>0.458295</td>\n",
       "      <td>0.120981</td>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.095641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.229500</td>\n",
       "      <td>2.006727</td>\n",
       "      <td>0.568286</td>\n",
       "      <td>0.247851</td>\n",
       "      <td>0.195555</td>\n",
       "      <td>0.179453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.794800</td>\n",
       "      <td>1.705004</td>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.307060</td>\n",
       "      <td>0.294135</td>\n",
       "      <td>0.279825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.479700</td>\n",
       "      <td>1.478865</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>0.345037</td>\n",
       "      <td>0.324209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.213700</td>\n",
       "      <td>1.317816</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.349530</td>\n",
       "      <td>0.361410</td>\n",
       "      <td>0.341914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.996600</td>\n",
       "      <td>1.233377</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.397017</td>\n",
       "      <td>0.386884</td>\n",
       "      <td>0.373901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.455207</td>\n",
       "      <td>0.422834</td>\n",
       "      <td>0.407044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>1.123993</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.478860</td>\n",
       "      <td>0.450786</td>\n",
       "      <td>0.445924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>1.103249</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.468251</td>\n",
       "      <td>0.444797</td>\n",
       "      <td>0.442204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>1.066058</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.512766</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.483959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>1.033552</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>0.486298</td>\n",
       "      <td>0.486757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>1.032451</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.504002</td>\n",
       "      <td>0.493373</td>\n",
       "      <td>0.488665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.356700</td>\n",
       "      <td>0.997789</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.517648</td>\n",
       "      <td>0.494146</td>\n",
       "      <td>0.496090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>1.009895</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.565877</td>\n",
       "      <td>0.518045</td>\n",
       "      <td>0.524641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>0.527759</td>\n",
       "      <td>0.528110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.988451</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.581501</td>\n",
       "      <td>0.546983</td>\n",
       "      <td>0.553694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.986886</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.578940</td>\n",
       "      <td>0.554236</td>\n",
       "      <td>0.554447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>1.001524</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.612998</td>\n",
       "      <td>0.558655</td>\n",
       "      <td>0.565478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.976087</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.584153</td>\n",
       "      <td>0.546687</td>\n",
       "      <td>0.554582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.980071</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.587752</td>\n",
       "      <td>0.556925</td>\n",
       "      <td>0.562969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.977467</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.595440</td>\n",
       "      <td>0.571341</td>\n",
       "      <td>0.573839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.988735</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.665876</td>\n",
       "      <td>0.599459</td>\n",
       "      <td>0.615459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.990313</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.613552</td>\n",
       "      <td>0.575567</td>\n",
       "      <td>0.581204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.992890</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.641455</td>\n",
       "      <td>0.591537</td>\n",
       "      <td>0.601791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.994859</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.637325</td>\n",
       "      <td>0.587433</td>\n",
       "      <td>0.599002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.665439</td>\n",
       "      <td>0.606686</td>\n",
       "      <td>0.621466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>0.998492</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.669160</td>\n",
       "      <td>0.609908</td>\n",
       "      <td>0.622803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.999373</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.668696</td>\n",
       "      <td>0.610331</td>\n",
       "      <td>0.623910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.659648</td>\n",
       "      <td>0.610331</td>\n",
       "      <td>0.621069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:55:58,264] Trial 76 finished with value: 0.6210694420665043 and parameters: {'learning_rate': 0.00035243578128822174, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 77 with params: {'learning_rate': 0.00048440127575320155, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:54, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.359800</td>\n",
       "      <td>2.833707</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.099314</td>\n",
       "      <td>0.075590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.501800</td>\n",
       "      <td>2.152117</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.201734</td>\n",
       "      <td>0.192564</td>\n",
       "      <td>0.176427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.862300</td>\n",
       "      <td>1.658626</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.289547</td>\n",
       "      <td>0.295020</td>\n",
       "      <td>0.276196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.397700</td>\n",
       "      <td>1.401109</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.318462</td>\n",
       "      <td>0.330447</td>\n",
       "      <td>0.309752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.085900</td>\n",
       "      <td>1.265182</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.360064</td>\n",
       "      <td>0.382446</td>\n",
       "      <td>0.353205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>1.143666</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.467127</td>\n",
       "      <td>0.410234</td>\n",
       "      <td>0.407011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>1.118812</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.462879</td>\n",
       "      <td>0.442084</td>\n",
       "      <td>0.436215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>1.071232</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>0.462289</td>\n",
       "      <td>0.455338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>1.031641</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.492218</td>\n",
       "      <td>0.481113</td>\n",
       "      <td>0.476171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>1.017820</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.511779</td>\n",
       "      <td>0.481305</td>\n",
       "      <td>0.484285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.538398</td>\n",
       "      <td>0.510749</td>\n",
       "      <td>0.510778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.997596</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.584946</td>\n",
       "      <td>0.532775</td>\n",
       "      <td>0.543108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>1.005574</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.587980</td>\n",
       "      <td>0.550296</td>\n",
       "      <td>0.553505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.567559</td>\n",
       "      <td>0.578546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>1.000722</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.649407</td>\n",
       "      <td>0.577300</td>\n",
       "      <td>0.591809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.016030</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.680429</td>\n",
       "      <td>0.601651</td>\n",
       "      <td>0.617496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>1.008917</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.627089</td>\n",
       "      <td>0.632446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>1.012077</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.680222</td>\n",
       "      <td>0.614260</td>\n",
       "      <td>0.628652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>1.038034</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.681287</td>\n",
       "      <td>0.617543</td>\n",
       "      <td>0.632610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>1.033249</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.718594</td>\n",
       "      <td>0.637093</td>\n",
       "      <td>0.656336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>1.047299</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.743628</td>\n",
       "      <td>0.656609</td>\n",
       "      <td>0.681181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>1.039665</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.747643</td>\n",
       "      <td>0.664445</td>\n",
       "      <td>0.686674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>1.047686</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.749044</td>\n",
       "      <td>0.670290</td>\n",
       "      <td>0.689855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>1.049640</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.752389</td>\n",
       "      <td>0.673641</td>\n",
       "      <td>0.695476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.048655</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.764287</td>\n",
       "      <td>0.673389</td>\n",
       "      <td>0.699366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>1.053703</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.676936</td>\n",
       "      <td>0.695916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>1.056759</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.758336</td>\n",
       "      <td>0.693922</td>\n",
       "      <td>0.711489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>1.057781</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.756835</td>\n",
       "      <td>0.682109</td>\n",
       "      <td>0.702468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>1.058397</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.688617</td>\n",
       "      <td>0.702821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>1.059464</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.751558</td>\n",
       "      <td>0.688617</td>\n",
       "      <td>0.705213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Fri Jan 10 23:14:01 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 11:58:53,955] Trial 77 finished with value: 0.705212680296082 and parameters: {'learning_rate': 0.00048440127575320155, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 1}. Best is trial 25 with value: 0.713771496767995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 78 with params: {'learning_rate': 0.00035816540254459545, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:48, 6.45 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.448100</td>\n",
       "      <td>3.007997</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.061499</td>\n",
       "      <td>0.082840</td>\n",
       "      <td>0.064167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.724300</td>\n",
       "      <td>2.397425</td>\n",
       "      <td>0.498625</td>\n",
       "      <td>0.161494</td>\n",
       "      <td>0.141626</td>\n",
       "      <td>0.123509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.165800</td>\n",
       "      <td>1.929420</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.224776</td>\n",
       "      <td>0.213204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.720500</td>\n",
       "      <td>1.642945</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.332565</td>\n",
       "      <td>0.323094</td>\n",
       "      <td>0.297491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.399200</td>\n",
       "      <td>1.442545</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.364189</td>\n",
       "      <td>0.361380</td>\n",
       "      <td>0.339250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>1.284952</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.409835</td>\n",
       "      <td>0.378810</td>\n",
       "      <td>0.365584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.943900</td>\n",
       "      <td>1.212046</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.401225</td>\n",
       "      <td>0.390184</td>\n",
       "      <td>0.374624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>1.167189</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.438745</td>\n",
       "      <td>0.436967</td>\n",
       "      <td>0.420512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>1.121387</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.491385</td>\n",
       "      <td>0.472206</td>\n",
       "      <td>0.467807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>1.088541</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.476954</td>\n",
       "      <td>0.460511</td>\n",
       "      <td>0.456484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 11:59:48,883] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 79 with params: {'learning_rate': 0.0004790137680264927, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.317800</td>\n",
       "      <td>2.785002</td>\n",
       "      <td>0.434464</td>\n",
       "      <td>0.066547</td>\n",
       "      <td>0.105416</td>\n",
       "      <td>0.078596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.456300</td>\n",
       "      <td>2.105116</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.191514</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>0.174823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.832800</td>\n",
       "      <td>1.639565</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.282471</td>\n",
       "      <td>0.283311</td>\n",
       "      <td>0.261765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.383300</td>\n",
       "      <td>1.380098</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.309086</td>\n",
       "      <td>0.322795</td>\n",
       "      <td>0.301175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.076000</td>\n",
       "      <td>1.248396</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.332272</td>\n",
       "      <td>0.368183</td>\n",
       "      <td>0.337695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>1.142515</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.453545</td>\n",
       "      <td>0.414527</td>\n",
       "      <td>0.407480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>1.109440</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.481608</td>\n",
       "      <td>0.450676</td>\n",
       "      <td>0.444621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>1.071200</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.491991</td>\n",
       "      <td>0.472392</td>\n",
       "      <td>0.467882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>1.032886</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.475192</td>\n",
       "      <td>0.486057</td>\n",
       "      <td>0.475590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>1.000563</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.493476</td>\n",
       "      <td>0.495842</td>\n",
       "      <td>0.488353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>1.010464</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.561395</td>\n",
       "      <td>0.529751</td>\n",
       "      <td>0.532866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.991564</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>0.550794</td>\n",
       "      <td>0.559771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.994862</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.629451</td>\n",
       "      <td>0.584257</td>\n",
       "      <td>0.592338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.979679</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.654947</td>\n",
       "      <td>0.583528</td>\n",
       "      <td>0.604634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.979244</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.670229</td>\n",
       "      <td>0.598662</td>\n",
       "      <td>0.618874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.995134</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.674220</td>\n",
       "      <td>0.608623</td>\n",
       "      <td>0.621908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.988871</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.668022</td>\n",
       "      <td>0.617577</td>\n",
       "      <td>0.624234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.980480</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.693537</td>\n",
       "      <td>0.606496</td>\n",
       "      <td>0.627860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>1.008243</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.729455</td>\n",
       "      <td>0.643178</td>\n",
       "      <td>0.664891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.009387</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.744960</td>\n",
       "      <td>0.662528</td>\n",
       "      <td>0.685062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>1.026546</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.771993</td>\n",
       "      <td>0.691817</td>\n",
       "      <td>0.714553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>1.013102</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.774831</td>\n",
       "      <td>0.687146</td>\n",
       "      <td>0.710063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>1.025600</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.784738</td>\n",
       "      <td>0.692397</td>\n",
       "      <td>0.717237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.025444</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.767288</td>\n",
       "      <td>0.684372</td>\n",
       "      <td>0.704825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>1.035170</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.767107</td>\n",
       "      <td>0.683247</td>\n",
       "      <td>0.703706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.033191</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.772255</td>\n",
       "      <td>0.695626</td>\n",
       "      <td>0.715942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.035593</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.778954</td>\n",
       "      <td>0.688129</td>\n",
       "      <td>0.711588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>1.035551</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.776468</td>\n",
       "      <td>0.697441</td>\n",
       "      <td>0.718430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>1.034391</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.769791</td>\n",
       "      <td>0.696341</td>\n",
       "      <td>0.714940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>1.034800</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.694912</td>\n",
       "      <td>0.713990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:02:29,347] Trial 79 finished with value: 0.713990114113548 and parameters: {'learning_rate': 0.0004790137680264927, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 79 with value: 0.713990114113548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 80 with params: {'learning_rate': 0.0004972963156919285, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.310200</td>\n",
       "      <td>2.775468</td>\n",
       "      <td>0.433547</td>\n",
       "      <td>0.065404</td>\n",
       "      <td>0.104771</td>\n",
       "      <td>0.077661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.443100</td>\n",
       "      <td>2.094748</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>0.195725</td>\n",
       "      <td>0.173162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.812900</td>\n",
       "      <td>1.621951</td>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.303566</td>\n",
       "      <td>0.293351</td>\n",
       "      <td>0.270766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>1.368449</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.309888</td>\n",
       "      <td>0.327186</td>\n",
       "      <td>0.304760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.053700</td>\n",
       "      <td>1.240427</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.355522</td>\n",
       "      <td>0.373588</td>\n",
       "      <td>0.346643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>1.131132</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.462294</td>\n",
       "      <td>0.415910</td>\n",
       "      <td>0.411208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>1.100976</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.491727</td>\n",
       "      <td>0.462523</td>\n",
       "      <td>0.455176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>1.058402</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.489122</td>\n",
       "      <td>0.458320</td>\n",
       "      <td>0.452994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>1.026635</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.487594</td>\n",
       "      <td>0.484896</td>\n",
       "      <td>0.479528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>1.005184</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.496099</td>\n",
       "      <td>0.493904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>1.004932</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.544330</td>\n",
       "      <td>0.518641</td>\n",
       "      <td>0.518049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.984874</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.603098</td>\n",
       "      <td>0.552933</td>\n",
       "      <td>0.566685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.987577</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.638403</td>\n",
       "      <td>0.588256</td>\n",
       "      <td>0.601023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.982809</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.667609</td>\n",
       "      <td>0.605560</td>\n",
       "      <td>0.623709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.990788</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.665294</td>\n",
       "      <td>0.599847</td>\n",
       "      <td>0.618134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>1.004616</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.634499</td>\n",
       "      <td>0.594760</td>\n",
       "      <td>0.600374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.990708</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.712848</td>\n",
       "      <td>0.633484</td>\n",
       "      <td>0.649787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.984405</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.693645</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.639461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>1.014883</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.772052</td>\n",
       "      <td>0.657686</td>\n",
       "      <td>0.686693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>1.000947</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.762396</td>\n",
       "      <td>0.673076</td>\n",
       "      <td>0.696767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>1.027971</td>\n",
       "      <td>0.808433</td>\n",
       "      <td>0.769087</td>\n",
       "      <td>0.693645</td>\n",
       "      <td>0.713105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>1.009458</td>\n",
       "      <td>0.807516</td>\n",
       "      <td>0.787842</td>\n",
       "      <td>0.695595</td>\n",
       "      <td>0.719032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>1.024016</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.785090</td>\n",
       "      <td>0.689617</td>\n",
       "      <td>0.713123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>1.034826</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.768844</td>\n",
       "      <td>0.678993</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.044464</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.758436</td>\n",
       "      <td>0.678935</td>\n",
       "      <td>0.700017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>1.040738</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.780469</td>\n",
       "      <td>0.692927</td>\n",
       "      <td>0.716050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>1.045755</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.755038</td>\n",
       "      <td>0.677431</td>\n",
       "      <td>0.697248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>1.044720</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.775832</td>\n",
       "      <td>0.680026</td>\n",
       "      <td>0.703557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>1.044715</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.773281</td>\n",
       "      <td>0.682834</td>\n",
       "      <td>0.705131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>1.045204</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.775988</td>\n",
       "      <td>0.684150</td>\n",
       "      <td>0.707103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:05:09,813] Trial 80 finished with value: 0.7071028515372666 and parameters: {'learning_rate': 0.0004972963156919285, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 79 with value: 0.713990114113548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 81 with params: {'learning_rate': 0.0003753563284820992, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:19, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.400900</td>\n",
       "      <td>2.945196</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>0.054691</td>\n",
       "      <td>0.086193</td>\n",
       "      <td>0.063322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.657100</td>\n",
       "      <td>2.324746</td>\n",
       "      <td>0.526123</td>\n",
       "      <td>0.194340</td>\n",
       "      <td>0.167555</td>\n",
       "      <td>0.155975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.093300</td>\n",
       "      <td>1.866673</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.304345</td>\n",
       "      <td>0.265303</td>\n",
       "      <td>0.255791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.655900</td>\n",
       "      <td>1.586268</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.331559</td>\n",
       "      <td>0.324820</td>\n",
       "      <td>0.302363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.338600</td>\n",
       "      <td>1.396671</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.364842</td>\n",
       "      <td>0.360576</td>\n",
       "      <td>0.338285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.094300</td>\n",
       "      <td>1.249680</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.441165</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.373341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>1.181851</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.453353</td>\n",
       "      <td>0.407758</td>\n",
       "      <td>0.401119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.758500</td>\n",
       "      <td>1.134325</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.454808</td>\n",
       "      <td>0.441833</td>\n",
       "      <td>0.431719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>1.084173</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.494854</td>\n",
       "      <td>0.468814</td>\n",
       "      <td>0.467029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>1.058503</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.471956</td>\n",
       "      <td>0.456196</td>\n",
       "      <td>0.453216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>1.031134</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.494460</td>\n",
       "      <td>0.481628</td>\n",
       "      <td>0.481566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>1.007305</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.514950</td>\n",
       "      <td>0.484869</td>\n",
       "      <td>0.488792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>1.010184</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.529294</td>\n",
       "      <td>0.508149</td>\n",
       "      <td>0.505729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.979274</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.543467</td>\n",
       "      <td>0.506790</td>\n",
       "      <td>0.510816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.987021</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.573925</td>\n",
       "      <td>0.530209</td>\n",
       "      <td>0.534379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.972991</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.582267</td>\n",
       "      <td>0.533445</td>\n",
       "      <td>0.540255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.985830</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.593491</td>\n",
       "      <td>0.554252</td>\n",
       "      <td>0.556457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.985088</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.572263</td>\n",
       "      <td>0.550271</td>\n",
       "      <td>0.549682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.990911</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.645887</td>\n",
       "      <td>0.568161</td>\n",
       "      <td>0.585055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.988108</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.632858</td>\n",
       "      <td>0.564313</td>\n",
       "      <td>0.581468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.987693</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.630399</td>\n",
       "      <td>0.563849</td>\n",
       "      <td>0.578835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.986098</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.673561</td>\n",
       "      <td>0.598800</td>\n",
       "      <td>0.615761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.990277</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.669860</td>\n",
       "      <td>0.594545</td>\n",
       "      <td>0.612191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.988334</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.668063</td>\n",
       "      <td>0.594192</td>\n",
       "      <td>0.609827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.992128</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.691670</td>\n",
       "      <td>0.605865</td>\n",
       "      <td>0.626971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.995340</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.669789</td>\n",
       "      <td>0.601311</td>\n",
       "      <td>0.617627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>1.000547</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.715772</td>\n",
       "      <td>0.629423</td>\n",
       "      <td>0.653396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>1.001378</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.721889</td>\n",
       "      <td>0.635083</td>\n",
       "      <td>0.659590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>1.000216</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.718282</td>\n",
       "      <td>0.632880</td>\n",
       "      <td>0.656684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>1.000377</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.718776</td>\n",
       "      <td>0.633029</td>\n",
       "      <td>0.657011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Fri Jan 10 23:13:59 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 12:08:30,870] Trial 81 finished with value: 0.6570111498844378 and parameters: {'learning_rate': 0.0003753563284820992, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 79 with value: 0.713990114113548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 82 with params: {'learning_rate': 0.0004777337176508395, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.322600</td>\n",
       "      <td>2.798232</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.067213</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>0.077942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.473500</td>\n",
       "      <td>2.125284</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.171629</td>\n",
       "      <td>0.193921</td>\n",
       "      <td>0.172018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.853100</td>\n",
       "      <td>1.655551</td>\n",
       "      <td>0.665445</td>\n",
       "      <td>0.309153</td>\n",
       "      <td>0.288088</td>\n",
       "      <td>0.266950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.402300</td>\n",
       "      <td>1.396531</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.295548</td>\n",
       "      <td>0.316662</td>\n",
       "      <td>0.291222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.093700</td>\n",
       "      <td>1.267681</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.358043</td>\n",
       "      <td>0.375947</td>\n",
       "      <td>0.347697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>1.144303</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.455940</td>\n",
       "      <td>0.405131</td>\n",
       "      <td>0.398041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>1.111831</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.462877</td>\n",
       "      <td>0.446768</td>\n",
       "      <td>0.434937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>1.068822</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.473428</td>\n",
       "      <td>0.454274</td>\n",
       "      <td>0.445570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>1.037023</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.483511</td>\n",
       "      <td>0.474278</td>\n",
       "      <td>0.467039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.380400</td>\n",
       "      <td>1.007478</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.482980</td>\n",
       "      <td>0.484081</td>\n",
       "      <td>0.478643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>1.003226</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.542635</td>\n",
       "      <td>0.511545</td>\n",
       "      <td>0.515175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.992466</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.597817</td>\n",
       "      <td>0.541039</td>\n",
       "      <td>0.556375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.989793</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.633707</td>\n",
       "      <td>0.574921</td>\n",
       "      <td>0.586341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.970373</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.648942</td>\n",
       "      <td>0.578732</td>\n",
       "      <td>0.599224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.989902</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.668582</td>\n",
       "      <td>0.596824</td>\n",
       "      <td>0.617264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.997136</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.587517</td>\n",
       "      <td>0.601957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.990084</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.682457</td>\n",
       "      <td>0.625514</td>\n",
       "      <td>0.633045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.988324</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.685623</td>\n",
       "      <td>0.605101</td>\n",
       "      <td>0.624518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>1.024012</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.717111</td>\n",
       "      <td>0.637078</td>\n",
       "      <td>0.658730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>1.003742</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.764890</td>\n",
       "      <td>0.668698</td>\n",
       "      <td>0.695987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>1.028652</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.770407</td>\n",
       "      <td>0.682638</td>\n",
       "      <td>0.706388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>1.024722</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.782771</td>\n",
       "      <td>0.686206</td>\n",
       "      <td>0.712366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>1.033124</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.790071</td>\n",
       "      <td>0.688595</td>\n",
       "      <td>0.715132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>1.031771</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.790777</td>\n",
       "      <td>0.692927</td>\n",
       "      <td>0.717291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>1.042577</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.768972</td>\n",
       "      <td>0.685104</td>\n",
       "      <td>0.706023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>1.038930</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.782817</td>\n",
       "      <td>0.694218</td>\n",
       "      <td>0.718195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>1.043930</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.790577</td>\n",
       "      <td>0.690508</td>\n",
       "      <td>0.715855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>1.042388</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.690418</td>\n",
       "      <td>0.715972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>1.040389</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.780758</td>\n",
       "      <td>0.690165</td>\n",
       "      <td>0.713887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>1.040900</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.781266</td>\n",
       "      <td>0.690507</td>\n",
       "      <td>0.714363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:11:17,244] Trial 82 finished with value: 0.7143632508757508 and parameters: {'learning_rate': 0.0004777337176508395, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83 with params: {'learning_rate': 9.578195459423425e-05, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:45, 6.62 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.707300</td>\n",
       "      <td>3.522892</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.411000</td>\n",
       "      <td>3.249518</td>\n",
       "      <td>0.322640</td>\n",
       "      <td>0.070629</td>\n",
       "      <td>0.063402</td>\n",
       "      <td>0.055684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.171800</td>\n",
       "      <td>3.017059</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.088143</td>\n",
       "      <td>0.063878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>2.805521</td>\n",
       "      <td>0.440880</td>\n",
       "      <td>0.086602</td>\n",
       "      <td>0.103927</td>\n",
       "      <td>0.079668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.763500</td>\n",
       "      <td>2.612105</td>\n",
       "      <td>0.470211</td>\n",
       "      <td>0.104646</td>\n",
       "      <td>0.122101</td>\n",
       "      <td>0.097267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.563700</td>\n",
       "      <td>2.447549</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.131305</td>\n",
       "      <td>0.142661</td>\n",
       "      <td>0.118231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.396300</td>\n",
       "      <td>2.311339</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.221917</td>\n",
       "      <td>0.170629</td>\n",
       "      <td>0.157134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.268900</td>\n",
       "      <td>2.188558</td>\n",
       "      <td>0.593951</td>\n",
       "      <td>0.263010</td>\n",
       "      <td>0.216173</td>\n",
       "      <td>0.205256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.129300</td>\n",
       "      <td>2.073462</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.284222</td>\n",
       "      <td>0.226697</td>\n",
       "      <td>0.218458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.006000</td>\n",
       "      <td>1.980968</td>\n",
       "      <td>0.646196</td>\n",
       "      <td>0.291521</td>\n",
       "      <td>0.268441</td>\n",
       "      <td>0.258351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:12:10,758] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 with params: {'learning_rate': 0.00048741262994873283, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:16, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.324900</td>\n",
       "      <td>2.814470</td>\n",
       "      <td>0.417965</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.097628</td>\n",
       "      <td>0.075130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.496000</td>\n",
       "      <td>2.156650</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.184513</td>\n",
       "      <td>0.199188</td>\n",
       "      <td>0.177331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.885200</td>\n",
       "      <td>1.685596</td>\n",
       "      <td>0.655362</td>\n",
       "      <td>0.275380</td>\n",
       "      <td>0.272048</td>\n",
       "      <td>0.250965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.434100</td>\n",
       "      <td>1.426448</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.303296</td>\n",
       "      <td>0.330769</td>\n",
       "      <td>0.300393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.113500</td>\n",
       "      <td>1.268262</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.382220</td>\n",
       "      <td>0.379284</td>\n",
       "      <td>0.357028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>1.146421</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.456053</td>\n",
       "      <td>0.423929</td>\n",
       "      <td>0.418647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>1.116112</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.446609</td>\n",
       "      <td>0.430465</td>\n",
       "      <td>0.422981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>1.059765</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.468792</td>\n",
       "      <td>0.453310</td>\n",
       "      <td>0.444391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>1.035435</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.471296</td>\n",
       "      <td>0.473024</td>\n",
       "      <td>0.463767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>1.030957</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.476989</td>\n",
       "      <td>0.486531</td>\n",
       "      <td>0.476884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>1.019999</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.545178</td>\n",
       "      <td>0.515361</td>\n",
       "      <td>0.515262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>1.002051</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.583388</td>\n",
       "      <td>0.543401</td>\n",
       "      <td>0.552952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>1.008446</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.603827</td>\n",
       "      <td>0.569598</td>\n",
       "      <td>0.572655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.986344</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.640642</td>\n",
       "      <td>0.588065</td>\n",
       "      <td>0.602390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>1.001879</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.642259</td>\n",
       "      <td>0.595445</td>\n",
       "      <td>0.606087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>1.001383</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.655262</td>\n",
       "      <td>0.593480</td>\n",
       "      <td>0.607260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.994679</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.602707</td>\n",
       "      <td>0.607939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>1.004765</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.669547</td>\n",
       "      <td>0.605279</td>\n",
       "      <td>0.618365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>1.038404</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.701531</td>\n",
       "      <td>0.607155</td>\n",
       "      <td>0.631917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>1.017304</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.732541</td>\n",
       "      <td>0.651219</td>\n",
       "      <td>0.671982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>1.033259</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.732060</td>\n",
       "      <td>0.645472</td>\n",
       "      <td>0.668877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>1.024327</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.773519</td>\n",
       "      <td>0.680618</td>\n",
       "      <td>0.704316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>1.031585</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.741768</td>\n",
       "      <td>0.665360</td>\n",
       "      <td>0.684092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.042520</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.685313</td>\n",
       "      <td>0.711702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>1.047367</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.764334</td>\n",
       "      <td>0.666313</td>\n",
       "      <td>0.691810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.039806</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.775350</td>\n",
       "      <td>0.682156</td>\n",
       "      <td>0.706290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>1.044762</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.784719</td>\n",
       "      <td>0.683723</td>\n",
       "      <td>0.710196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.049623</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.785712</td>\n",
       "      <td>0.683203</td>\n",
       "      <td>0.709799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>1.046785</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.681697</td>\n",
       "      <td>0.707604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>1.047197</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.681697</td>\n",
       "      <td>0.707604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Fri Jan 10 23:14:01 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 12:15:28,982] Trial 84 finished with value: 0.7076037019926047 and parameters: {'learning_rate': 0.00048741262994873283, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85 with params: {'learning_rate': 0.0004705937176505686, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.327300</td>\n",
       "      <td>2.806662</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.069199</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>0.078877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.484600</td>\n",
       "      <td>2.136112</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.193336</td>\n",
       "      <td>0.196585</td>\n",
       "      <td>0.177847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.868300</td>\n",
       "      <td>1.668359</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.289213</td>\n",
       "      <td>0.286126</td>\n",
       "      <td>0.264684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.419400</td>\n",
       "      <td>1.410944</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.298708</td>\n",
       "      <td>0.318091</td>\n",
       "      <td>0.293208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.277828</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.335808</td>\n",
       "      <td>0.370530</td>\n",
       "      <td>0.339560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>1.152328</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.439246</td>\n",
       "      <td>0.403847</td>\n",
       "      <td>0.394528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>1.117365</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.447988</td>\n",
       "      <td>0.440487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.591300</td>\n",
       "      <td>1.072498</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.472906</td>\n",
       "      <td>0.455199</td>\n",
       "      <td>0.445663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.046253</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.478672</td>\n",
       "      <td>0.474826</td>\n",
       "      <td>0.468644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>1.011477</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.481380</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.474387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.999344</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.539849</td>\n",
       "      <td>0.507067</td>\n",
       "      <td>0.511786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.995149</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.587649</td>\n",
       "      <td>0.533653</td>\n",
       "      <td>0.543843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.987694</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.612616</td>\n",
       "      <td>0.566821</td>\n",
       "      <td>0.574268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.977126</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.640961</td>\n",
       "      <td>0.576228</td>\n",
       "      <td>0.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.982756</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.665942</td>\n",
       "      <td>0.592091</td>\n",
       "      <td>0.613047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.993342</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.670009</td>\n",
       "      <td>0.593726</td>\n",
       "      <td>0.613059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.986899</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.662595</td>\n",
       "      <td>0.618379</td>\n",
       "      <td>0.622460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.988252</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.663214</td>\n",
       "      <td>0.592209</td>\n",
       "      <td>0.608285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.017243</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.706884</td>\n",
       "      <td>0.617561</td>\n",
       "      <td>0.638104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>1.001152</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.739248</td>\n",
       "      <td>0.660178</td>\n",
       "      <td>0.682176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.025621</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.740419</td>\n",
       "      <td>0.664710</td>\n",
       "      <td>0.685539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>1.020457</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.785105</td>\n",
       "      <td>0.685102</td>\n",
       "      <td>0.709381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>1.027208</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.777667</td>\n",
       "      <td>0.679665</td>\n",
       "      <td>0.706517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.024589</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.793259</td>\n",
       "      <td>0.691346</td>\n",
       "      <td>0.716676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>1.031004</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.769690</td>\n",
       "      <td>0.683433</td>\n",
       "      <td>0.705036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>1.030554</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.784952</td>\n",
       "      <td>0.688152</td>\n",
       "      <td>0.713859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>1.035021</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.793885</td>\n",
       "      <td>0.688530</td>\n",
       "      <td>0.715371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.032705</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.790815</td>\n",
       "      <td>0.688687</td>\n",
       "      <td>0.714979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>1.031399</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.777653</td>\n",
       "      <td>0.687938</td>\n",
       "      <td>0.710804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>1.031670</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.779295</td>\n",
       "      <td>0.687938</td>\n",
       "      <td>0.711585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:18:12,014] Trial 85 finished with value: 0.7115847713433859 and parameters: {'learning_rate': 0.0004705937176505686, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 86 with params: {'learning_rate': 0.0003490952656150358, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.37 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.431000</td>\n",
       "      <td>3.012853</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.060820</td>\n",
       "      <td>0.082714</td>\n",
       "      <td>0.063518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.742600</td>\n",
       "      <td>2.429785</td>\n",
       "      <td>0.481210</td>\n",
       "      <td>0.138654</td>\n",
       "      <td>0.131410</td>\n",
       "      <td>0.109238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.212800</td>\n",
       "      <td>1.985369</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.239632</td>\n",
       "      <td>0.202182</td>\n",
       "      <td>0.186531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.776500</td>\n",
       "      <td>1.685714</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.316067</td>\n",
       "      <td>0.304039</td>\n",
       "      <td>0.285023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.457600</td>\n",
       "      <td>1.469721</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.350993</td>\n",
       "      <td>0.326069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.192700</td>\n",
       "      <td>1.302827</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.397005</td>\n",
       "      <td>0.377866</td>\n",
       "      <td>0.363961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>1.225322</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.399676</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.379057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>1.170874</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.428525</td>\n",
       "      <td>0.423897</td>\n",
       "      <td>0.406578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>1.124763</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.479469</td>\n",
       "      <td>0.461783</td>\n",
       "      <td>0.453461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>1.104119</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.464854</td>\n",
       "      <td>0.446902</td>\n",
       "      <td>0.443378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>1.056181</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.508181</td>\n",
       "      <td>0.473482</td>\n",
       "      <td>0.478241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>1.030596</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.510657</td>\n",
       "      <td>0.477308</td>\n",
       "      <td>0.479451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>1.025574</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.502977</td>\n",
       "      <td>0.495155</td>\n",
       "      <td>0.489382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.990478</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.520388</td>\n",
       "      <td>0.499296</td>\n",
       "      <td>0.499988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>1.004847</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.536868</td>\n",
       "      <td>0.510894</td>\n",
       "      <td>0.512437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.991486</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.570269</td>\n",
       "      <td>0.532373</td>\n",
       "      <td>0.536027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.987380</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.561220</td>\n",
       "      <td>0.535264</td>\n",
       "      <td>0.536142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.983494</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.570080</td>\n",
       "      <td>0.547497</td>\n",
       "      <td>0.547252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.995176</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.585375</td>\n",
       "      <td>0.549155</td>\n",
       "      <td>0.549879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.973937</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.589522</td>\n",
       "      <td>0.543466</td>\n",
       "      <td>0.554480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:20:02,585] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87 with params: {'learning_rate': 0.00046950442262754487, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:50, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.324000</td>\n",
       "      <td>2.796517</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.066748</td>\n",
       "      <td>0.103265</td>\n",
       "      <td>0.077836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.471700</td>\n",
       "      <td>2.120886</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.211778</td>\n",
       "      <td>0.198086</td>\n",
       "      <td>0.180193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.853300</td>\n",
       "      <td>1.656350</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.289704</td>\n",
       "      <td>0.289737</td>\n",
       "      <td>0.267288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.404700</td>\n",
       "      <td>1.394502</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.297763</td>\n",
       "      <td>0.318284</td>\n",
       "      <td>0.295230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.096500</td>\n",
       "      <td>1.262043</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.332055</td>\n",
       "      <td>0.368317</td>\n",
       "      <td>0.336713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.877400</td>\n",
       "      <td>1.147212</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.450278</td>\n",
       "      <td>0.413461</td>\n",
       "      <td>0.405583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>1.111436</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.495129</td>\n",
       "      <td>0.452325</td>\n",
       "      <td>0.446791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>1.072403</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.490546</td>\n",
       "      <td>0.473971</td>\n",
       "      <td>0.468843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>1.038799</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.472348</td>\n",
       "      <td>0.481126</td>\n",
       "      <td>0.471234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.008147</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.489461</td>\n",
       "      <td>0.487641</td>\n",
       "      <td>0.482664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>1.002357</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.559890</td>\n",
       "      <td>0.525567</td>\n",
       "      <td>0.528961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.992596</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.587344</td>\n",
       "      <td>0.545895</td>\n",
       "      <td>0.555565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.628376</td>\n",
       "      <td>0.580695</td>\n",
       "      <td>0.587134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.972473</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.657897</td>\n",
       "      <td>0.585171</td>\n",
       "      <td>0.605897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.977246</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.674565</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.618371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.994257</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.648464</td>\n",
       "      <td>0.589029</td>\n",
       "      <td>0.601160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.671903</td>\n",
       "      <td>0.623211</td>\n",
       "      <td>0.630983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.980541</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.670363</td>\n",
       "      <td>0.602062</td>\n",
       "      <td>0.620011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>1.010158</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.743065</td>\n",
       "      <td>0.646751</td>\n",
       "      <td>0.671075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>1.007936</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.754030</td>\n",
       "      <td>0.660875</td>\n",
       "      <td>0.687344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>1.022521</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.746583</td>\n",
       "      <td>0.668924</td>\n",
       "      <td>0.690721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>1.012736</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.780451</td>\n",
       "      <td>0.690217</td>\n",
       "      <td>0.713944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>1.019271</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.773720</td>\n",
       "      <td>0.690629</td>\n",
       "      <td>0.712722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>1.016842</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.761907</td>\n",
       "      <td>0.684996</td>\n",
       "      <td>0.704461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>1.027714</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.766480</td>\n",
       "      <td>0.686587</td>\n",
       "      <td>0.707950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>1.024001</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.772460</td>\n",
       "      <td>0.692692</td>\n",
       "      <td>0.713493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>1.029498</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.757324</td>\n",
       "      <td>0.682869</td>\n",
       "      <td>0.701284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.029642</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.772607</td>\n",
       "      <td>0.690669</td>\n",
       "      <td>0.712060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>1.028822</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>0.689262</td>\n",
       "      <td>0.707336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>1.029144</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.764747</td>\n",
       "      <td>0.689907</td>\n",
       "      <td>0.708224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:22:54,610] Trial 87 finished with value: 0.708224076819617 and parameters: {'learning_rate': 0.00046950442262754487, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88 with params: {'learning_rate': 0.00040487030930757444, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:53, 6.59 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.372500</td>\n",
       "      <td>2.888162</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.073933</td>\n",
       "      <td>0.094967</td>\n",
       "      <td>0.074608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.585400</td>\n",
       "      <td>2.244433</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.180898</td>\n",
       "      <td>0.177886</td>\n",
       "      <td>0.162759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.998000</td>\n",
       "      <td>1.779745</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.288013</td>\n",
       "      <td>0.274387</td>\n",
       "      <td>0.259589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.554900</td>\n",
       "      <td>1.503566</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.332714</td>\n",
       "      <td>0.316227</td>\n",
       "      <td>0.300302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.345172</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.364568</td>\n",
       "      <td>0.366232</td>\n",
       "      <td>0.340321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.013800</td>\n",
       "      <td>1.207913</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.429198</td>\n",
       "      <td>0.389039</td>\n",
       "      <td>0.377835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>1.146963</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.432071</td>\n",
       "      <td>0.407228</td>\n",
       "      <td>0.396688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>1.106863</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>0.450854</td>\n",
       "      <td>0.445211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>1.055347</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.504337</td>\n",
       "      <td>0.474075</td>\n",
       "      <td>0.474317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>1.031224</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.487189</td>\n",
       "      <td>0.467957</td>\n",
       "      <td>0.466098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>1.017744</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.516475</td>\n",
       "      <td>0.501261</td>\n",
       "      <td>0.498714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>1.000795</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.535001</td>\n",
       "      <td>0.501539</td>\n",
       "      <td>0.505224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>1.009630</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.552636</td>\n",
       "      <td>0.519018</td>\n",
       "      <td>0.519760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.972665</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.567904</td>\n",
       "      <td>0.515245</td>\n",
       "      <td>0.525048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.990314</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.583481</td>\n",
       "      <td>0.534427</td>\n",
       "      <td>0.541942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.976619</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.571742</td>\n",
       "      <td>0.538063</td>\n",
       "      <td>0.537720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.978105</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.607952</td>\n",
       "      <td>0.567227</td>\n",
       "      <td>0.569070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.617332</td>\n",
       "      <td>0.557953</td>\n",
       "      <td>0.570027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.997372</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.671612</td>\n",
       "      <td>0.590064</td>\n",
       "      <td>0.611919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.991724</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.659916</td>\n",
       "      <td>0.585016</td>\n",
       "      <td>0.603416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:24:41,495] Trial 88 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 89 with params: {'learning_rate': 0.00047242989374394394, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.326000</td>\n",
       "      <td>2.804405</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>0.078777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.481700</td>\n",
       "      <td>2.133165</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.196585</td>\n",
       "      <td>0.177894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.864300</td>\n",
       "      <td>1.664954</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.290247</td>\n",
       "      <td>0.287035</td>\n",
       "      <td>0.265820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.414900</td>\n",
       "      <td>1.407189</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.298338</td>\n",
       "      <td>0.318091</td>\n",
       "      <td>0.293151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.106200</td>\n",
       "      <td>1.275681</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.335337</td>\n",
       "      <td>0.370042</td>\n",
       "      <td>0.339011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>1.150402</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.438674</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>0.395149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>1.115736</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.476537</td>\n",
       "      <td>0.446014</td>\n",
       "      <td>0.439515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>1.071773</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.472353</td>\n",
       "      <td>0.453986</td>\n",
       "      <td>0.444843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>1.044453</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.481841</td>\n",
       "      <td>0.474826</td>\n",
       "      <td>0.468972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>1.011081</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.481221</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.474315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.561362</td>\n",
       "      <td>0.511789</td>\n",
       "      <td>0.517678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.994246</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.592768</td>\n",
       "      <td>0.533462</td>\n",
       "      <td>0.547223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.987860</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.612382</td>\n",
       "      <td>0.566821</td>\n",
       "      <td>0.574142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.643945</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.594041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.983426</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.668511</td>\n",
       "      <td>0.592091</td>\n",
       "      <td>0.613689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.993557</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.673389</td>\n",
       "      <td>0.599142</td>\n",
       "      <td>0.617555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.988263</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.682523</td>\n",
       "      <td>0.624171</td>\n",
       "      <td>0.632334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.987372</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.682886</td>\n",
       "      <td>0.602209</td>\n",
       "      <td>0.621799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>1.019775</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.683281</td>\n",
       "      <td>0.606156</td>\n",
       "      <td>0.622682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>1.001325</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.739407</td>\n",
       "      <td>0.662416</td>\n",
       "      <td>0.682713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>1.027224</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.740591</td>\n",
       "      <td>0.665140</td>\n",
       "      <td>0.685843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>1.021842</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.784632</td>\n",
       "      <td>0.684571</td>\n",
       "      <td>0.710512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>1.029206</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.787256</td>\n",
       "      <td>0.684120</td>\n",
       "      <td>0.712731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>1.027745</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.791962</td>\n",
       "      <td>0.691058</td>\n",
       "      <td>0.716517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>1.035520</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.769679</td>\n",
       "      <td>0.682907</td>\n",
       "      <td>0.704945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.033658</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.782133</td>\n",
       "      <td>0.687937</td>\n",
       "      <td>0.713244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>1.037125</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.790877</td>\n",
       "      <td>0.688003</td>\n",
       "      <td>0.714505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>1.035480</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.790892</td>\n",
       "      <td>0.688649</td>\n",
       "      <td>0.714980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>1.034030</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.778994</td>\n",
       "      <td>0.687938</td>\n",
       "      <td>0.711241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>1.034427</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.779382</td>\n",
       "      <td>0.687938</td>\n",
       "      <td>0.711643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:27:33,386] Trial 89 finished with value: 0.7116427173122123 and parameters: {'learning_rate': 0.00047242989374394394, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 with params: {'learning_rate': 0.0001441843628958433, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:17, 6.35 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.641700</td>\n",
       "      <td>3.393258</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.240400</td>\n",
       "      <td>3.037117</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.054922</td>\n",
       "      <td>0.085325</td>\n",
       "      <td>0.063413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.923400</td>\n",
       "      <td>2.726604</td>\n",
       "      <td>0.441797</td>\n",
       "      <td>0.082819</td>\n",
       "      <td>0.104958</td>\n",
       "      <td>0.076868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.620900</td>\n",
       "      <td>2.459427</td>\n",
       "      <td>0.500458</td>\n",
       "      <td>0.151289</td>\n",
       "      <td>0.141530</td>\n",
       "      <td>0.119758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.388000</td>\n",
       "      <td>2.232408</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.226089</td>\n",
       "      <td>0.190803</td>\n",
       "      <td>0.176855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:28:01,652] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91 with params: {'learning_rate': 0.0004642583255396689, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.339400</td>\n",
       "      <td>2.841142</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.070954</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.074422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.529800</td>\n",
       "      <td>2.191645</td>\n",
       "      <td>0.551787</td>\n",
       "      <td>0.173751</td>\n",
       "      <td>0.188649</td>\n",
       "      <td>0.168182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.929700</td>\n",
       "      <td>1.722729</td>\n",
       "      <td>0.647113</td>\n",
       "      <td>0.266863</td>\n",
       "      <td>0.264793</td>\n",
       "      <td>0.245135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.478800</td>\n",
       "      <td>1.453552</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.309778</td>\n",
       "      <td>0.326763</td>\n",
       "      <td>0.298307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.157400</td>\n",
       "      <td>1.289407</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.330781</td>\n",
       "      <td>0.363364</td>\n",
       "      <td>0.334583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>1.169284</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.458951</td>\n",
       "      <td>0.423005</td>\n",
       "      <td>0.415725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>1.120841</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.463898</td>\n",
       "      <td>0.436043</td>\n",
       "      <td>0.431088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.616600</td>\n",
       "      <td>1.071696</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.455643</td>\n",
       "      <td>0.447392</td>\n",
       "      <td>0.440639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>1.048815</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.477876</td>\n",
       "      <td>0.468833</td>\n",
       "      <td>0.459939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>1.026127</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.464043</td>\n",
       "      <td>0.474022</td>\n",
       "      <td>0.464703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>1.024151</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.526997</td>\n",
       "      <td>0.506227</td>\n",
       "      <td>0.508295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.994561</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.589866</td>\n",
       "      <td>0.520871</td>\n",
       "      <td>0.533920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>1.003582</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.610874</td>\n",
       "      <td>0.569270</td>\n",
       "      <td>0.575603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.984098</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.643152</td>\n",
       "      <td>0.580276</td>\n",
       "      <td>0.597754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.998822</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.641865</td>\n",
       "      <td>0.572728</td>\n",
       "      <td>0.588607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.996060</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.639101</td>\n",
       "      <td>0.584770</td>\n",
       "      <td>0.596160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.986940</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.634662</td>\n",
       "      <td>0.595346</td>\n",
       "      <td>0.599713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.994596</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.688284</td>\n",
       "      <td>0.608128</td>\n",
       "      <td>0.628904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>1.020761</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.681261</td>\n",
       "      <td>0.600519</td>\n",
       "      <td>0.620904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>1.007720</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.711053</td>\n",
       "      <td>0.650065</td>\n",
       "      <td>0.666137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>1.016838</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.737828</td>\n",
       "      <td>0.662847</td>\n",
       "      <td>0.683772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>1.005605</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.761165</td>\n",
       "      <td>0.681603</td>\n",
       "      <td>0.702184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>1.011606</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.750587</td>\n",
       "      <td>0.673869</td>\n",
       "      <td>0.693778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.020521</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.760150</td>\n",
       "      <td>0.670726</td>\n",
       "      <td>0.693933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>1.020399</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.769577</td>\n",
       "      <td>0.683822</td>\n",
       "      <td>0.706794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1.017444</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.769916</td>\n",
       "      <td>0.681014</td>\n",
       "      <td>0.703912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>1.023630</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.769844</td>\n",
       "      <td>0.676441</td>\n",
       "      <td>0.702305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.022202</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.769288</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>0.704248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>1.021501</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.766331</td>\n",
       "      <td>0.679358</td>\n",
       "      <td>0.701261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>1.021822</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.767408</td>\n",
       "      <td>0.678827</td>\n",
       "      <td>0.701602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:30:44,306] Trial 91 finished with value: 0.7016015758735259 and parameters: {'learning_rate': 0.0004642583255396689, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 92 with params: {'learning_rate': 0.00044653297878477884, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.347500</td>\n",
       "      <td>2.850225</td>\n",
       "      <td>0.416132</td>\n",
       "      <td>0.071476</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.075404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.540800</td>\n",
       "      <td>2.198291</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.173462</td>\n",
       "      <td>0.188201</td>\n",
       "      <td>0.168284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.943300</td>\n",
       "      <td>1.733757</td>\n",
       "      <td>0.650779</td>\n",
       "      <td>0.263211</td>\n",
       "      <td>0.268853</td>\n",
       "      <td>0.250924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.497400</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.301238</td>\n",
       "      <td>0.332750</td>\n",
       "      <td>0.300752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.180200</td>\n",
       "      <td>1.307922</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.324358</td>\n",
       "      <td>0.368861</td>\n",
       "      <td>0.337181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>1.180088</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.460874</td>\n",
       "      <td>0.408885</td>\n",
       "      <td>0.405511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>1.121903</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.481962</td>\n",
       "      <td>0.432449</td>\n",
       "      <td>0.428674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>1.077269</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.460393</td>\n",
       "      <td>0.453863</td>\n",
       "      <td>0.446257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>1.050235</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.486606</td>\n",
       "      <td>0.474155</td>\n",
       "      <td>0.467847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>1.017237</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.470072</td>\n",
       "      <td>0.458267</td>\n",
       "      <td>0.451644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>1.003820</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.525737</td>\n",
       "      <td>0.502929</td>\n",
       "      <td>0.502997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.567713</td>\n",
       "      <td>0.527730</td>\n",
       "      <td>0.533776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.992628</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.607218</td>\n",
       "      <td>0.561069</td>\n",
       "      <td>0.568368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.983429</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.624613</td>\n",
       "      <td>0.565711</td>\n",
       "      <td>0.580026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.654718</td>\n",
       "      <td>0.575902</td>\n",
       "      <td>0.594436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.989021</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.611565</td>\n",
       "      <td>0.566956</td>\n",
       "      <td>0.572065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.981536</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.621448</td>\n",
       "      <td>0.577594</td>\n",
       "      <td>0.580950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.990547</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.640689</td>\n",
       "      <td>0.579770</td>\n",
       "      <td>0.594308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>1.007954</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.663710</td>\n",
       "      <td>0.586294</td>\n",
       "      <td>0.606438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.996890</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.716374</td>\n",
       "      <td>0.649821</td>\n",
       "      <td>0.666810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>1.014605</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.714894</td>\n",
       "      <td>0.641545</td>\n",
       "      <td>0.661447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.000999</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.759216</td>\n",
       "      <td>0.665934</td>\n",
       "      <td>0.690011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>1.005655</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.665633</td>\n",
       "      <td>0.680964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>1.009387</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.739827</td>\n",
       "      <td>0.661195</td>\n",
       "      <td>0.680061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>1.008152</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.743896</td>\n",
       "      <td>0.664643</td>\n",
       "      <td>0.684513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>1.010258</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.762265</td>\n",
       "      <td>0.666753</td>\n",
       "      <td>0.691743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>1.014053</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.772514</td>\n",
       "      <td>0.675756</td>\n",
       "      <td>0.701073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1.013984</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.765298</td>\n",
       "      <td>0.672965</td>\n",
       "      <td>0.696291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>1.013428</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.770207</td>\n",
       "      <td>0.681419</td>\n",
       "      <td>0.702811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>1.013871</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.771652</td>\n",
       "      <td>0.682472</td>\n",
       "      <td>0.704248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:33:29,084] Trial 92 finished with value: 0.7042484657254029 and parameters: {'learning_rate': 0.00044653297878477884, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 93 with params: {'learning_rate': 0.00043535613421081, 'weight_decay': 0.003, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:50 < 00:55, 6.34 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.352000</td>\n",
       "      <td>2.853487</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.070272</td>\n",
       "      <td>0.098669</td>\n",
       "      <td>0.076626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.543300</td>\n",
       "      <td>2.199224</td>\n",
       "      <td>0.552704</td>\n",
       "      <td>0.185270</td>\n",
       "      <td>0.188077</td>\n",
       "      <td>0.170397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.945500</td>\n",
       "      <td>1.734118</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.266721</td>\n",
       "      <td>0.279477</td>\n",
       "      <td>0.258597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>1.475904</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.306569</td>\n",
       "      <td>0.334274</td>\n",
       "      <td>0.305952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.189800</td>\n",
       "      <td>1.313780</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.329763</td>\n",
       "      <td>0.372191</td>\n",
       "      <td>0.340791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>1.185422</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.443184</td>\n",
       "      <td>0.396324</td>\n",
       "      <td>0.388273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.771500</td>\n",
       "      <td>1.124083</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>0.426932</td>\n",
       "      <td>0.420445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>1.085651</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.481082</td>\n",
       "      <td>0.459249</td>\n",
       "      <td>0.452656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.530300</td>\n",
       "      <td>1.049778</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.491288</td>\n",
       "      <td>0.479093</td>\n",
       "      <td>0.474244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>1.013320</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.492546</td>\n",
       "      <td>0.476312</td>\n",
       "      <td>0.476718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>1.002153</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.497464</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.985493</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.582958</td>\n",
       "      <td>0.523485</td>\n",
       "      <td>0.532296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.592209</td>\n",
       "      <td>0.550842</td>\n",
       "      <td>0.554814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.966121</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.630937</td>\n",
       "      <td>0.561885</td>\n",
       "      <td>0.577928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.983082</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.611172</td>\n",
       "      <td>0.555270</td>\n",
       "      <td>0.565318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.981939</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.623009</td>\n",
       "      <td>0.566542</td>\n",
       "      <td>0.576379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.977685</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.606474</td>\n",
       "      <td>0.572276</td>\n",
       "      <td>0.572379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>0.987371</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.656872</td>\n",
       "      <td>0.583419</td>\n",
       "      <td>0.600815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.999115</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.648429</td>\n",
       "      <td>0.580007</td>\n",
       "      <td>0.596140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.997876</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.702986</td>\n",
       "      <td>0.622872</td>\n",
       "      <td>0.644170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:35:20,187] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 94 with params: {'learning_rate': 0.0004558842215169161, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.333400</td>\n",
       "      <td>2.813756</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.069463</td>\n",
       "      <td>0.102858</td>\n",
       "      <td>0.078927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>2.144271</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.210593</td>\n",
       "      <td>0.196523</td>\n",
       "      <td>0.179227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.882700</td>\n",
       "      <td>1.680812</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.286069</td>\n",
       "      <td>0.288270</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.436200</td>\n",
       "      <td>1.417723</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.316781</td>\n",
       "      <td>0.319431</td>\n",
       "      <td>0.299506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.128200</td>\n",
       "      <td>1.284216</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.332225</td>\n",
       "      <td>0.367654</td>\n",
       "      <td>0.336337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>1.161013</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.438197</td>\n",
       "      <td>0.403858</td>\n",
       "      <td>0.394519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>1.121810</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.474853</td>\n",
       "      <td>0.446708</td>\n",
       "      <td>0.440433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>1.080330</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.489350</td>\n",
       "      <td>0.462913</td>\n",
       "      <td>0.458651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>1.047243</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.474270</td>\n",
       "      <td>0.479884</td>\n",
       "      <td>0.471035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>1.005780</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.491102</td>\n",
       "      <td>0.486567</td>\n",
       "      <td>0.483417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>1.001538</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.547503</td>\n",
       "      <td>0.517137</td>\n",
       "      <td>0.521412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.989462</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.592606</td>\n",
       "      <td>0.536415</td>\n",
       "      <td>0.547747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.994550</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.608721</td>\n",
       "      <td>0.567399</td>\n",
       "      <td>0.573536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.969602</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>0.586962</td>\n",
       "      <td>0.608593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.984641</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.647852</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.593544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.990284</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.671143</td>\n",
       "      <td>0.590541</td>\n",
       "      <td>0.608553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.981993</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.674565</td>\n",
       "      <td>0.605308</td>\n",
       "      <td>0.619864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.987038</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.652525</td>\n",
       "      <td>0.585613</td>\n",
       "      <td>0.601898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>1.007894</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.695186</td>\n",
       "      <td>0.617948</td>\n",
       "      <td>0.635241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>1.003935</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.744066</td>\n",
       "      <td>0.653786</td>\n",
       "      <td>0.675695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.020970</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.750459</td>\n",
       "      <td>0.666803</td>\n",
       "      <td>0.690021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>1.010665</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.758060</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.698294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>1.023904</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.758452</td>\n",
       "      <td>0.674082</td>\n",
       "      <td>0.696528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>1.021558</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.745606</td>\n",
       "      <td>0.664519</td>\n",
       "      <td>0.684879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>1.028806</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.664886</td>\n",
       "      <td>0.685585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>1.028423</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.756076</td>\n",
       "      <td>0.669623</td>\n",
       "      <td>0.693028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>1.030185</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.772655</td>\n",
       "      <td>0.680197</td>\n",
       "      <td>0.703572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.031390</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.775603</td>\n",
       "      <td>0.689531</td>\n",
       "      <td>0.711696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.030453</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.775107</td>\n",
       "      <td>0.689745</td>\n",
       "      <td>0.711501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>1.030882</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.775330</td>\n",
       "      <td>0.689257</td>\n",
       "      <td>0.711420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:38:00,414] Trial 94 finished with value: 0.7114203169346847 and parameters: {'learning_rate': 0.0004558842215169161, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95 with params: {'learning_rate': 0.0004019951057688599, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:57 < 00:58, 5.94 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.378100</td>\n",
       "      <td>2.902429</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.074903</td>\n",
       "      <td>0.093313</td>\n",
       "      <td>0.073333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.603900</td>\n",
       "      <td>2.265015</td>\n",
       "      <td>0.538955</td>\n",
       "      <td>0.178774</td>\n",
       "      <td>0.173472</td>\n",
       "      <td>0.158690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.023400</td>\n",
       "      <td>1.803291</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.268935</td>\n",
       "      <td>0.251522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.583800</td>\n",
       "      <td>1.533855</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.330877</td>\n",
       "      <td>0.328912</td>\n",
       "      <td>0.305578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.267200</td>\n",
       "      <td>1.357666</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.373829</td>\n",
       "      <td>0.345858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.030500</td>\n",
       "      <td>1.216750</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.444921</td>\n",
       "      <td>0.395097</td>\n",
       "      <td>0.386980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>1.151662</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.454794</td>\n",
       "      <td>0.414391</td>\n",
       "      <td>0.408358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>1.109756</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.480048</td>\n",
       "      <td>0.448569</td>\n",
       "      <td>0.445737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>1.058818</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.498926</td>\n",
       "      <td>0.467691</td>\n",
       "      <td>0.466736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>1.029884</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.488249</td>\n",
       "      <td>0.468394</td>\n",
       "      <td>0.466648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>1.012821</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.492640</td>\n",
       "      <td>0.484347</td>\n",
       "      <td>0.481724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.993029</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.544059</td>\n",
       "      <td>0.504369</td>\n",
       "      <td>0.509845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>1.001402</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.557982</td>\n",
       "      <td>0.524339</td>\n",
       "      <td>0.528214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.973065</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.565990</td>\n",
       "      <td>0.516711</td>\n",
       "      <td>0.524184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.990376</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.580334</td>\n",
       "      <td>0.534912</td>\n",
       "      <td>0.540739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.975978</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.562732</td>\n",
       "      <td>0.540836</td>\n",
       "      <td>0.535037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.979895</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.615595</td>\n",
       "      <td>0.565307</td>\n",
       "      <td>0.571430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.984715</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.602337</td>\n",
       "      <td>0.562542</td>\n",
       "      <td>0.566524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.999545</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.665020</td>\n",
       "      <td>0.588572</td>\n",
       "      <td>0.608776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.991170</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.651968</td>\n",
       "      <td>0.585217</td>\n",
       "      <td>0.602105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:39:58,960] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 96 with params: {'learning_rate': 0.0002768236517832991, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.66 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.516900</td>\n",
       "      <td>3.138341</td>\n",
       "      <td>0.311641</td>\n",
       "      <td>0.070356</td>\n",
       "      <td>0.060025</td>\n",
       "      <td>0.048852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.891700</td>\n",
       "      <td>2.590642</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.081776</td>\n",
       "      <td>0.112773</td>\n",
       "      <td>0.081386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.397500</td>\n",
       "      <td>2.157327</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.238140</td>\n",
       "      <td>0.186180</td>\n",
       "      <td>0.171105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.981800</td>\n",
       "      <td>1.845193</td>\n",
       "      <td>0.656279</td>\n",
       "      <td>0.298529</td>\n",
       "      <td>0.277381</td>\n",
       "      <td>0.263173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.667000</td>\n",
       "      <td>1.615433</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.356967</td>\n",
       "      <td>0.342363</td>\n",
       "      <td>0.327255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.393800</td>\n",
       "      <td>1.433415</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.343283</td>\n",
       "      <td>0.341711</td>\n",
       "      <td>0.323314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.181300</td>\n",
       "      <td>1.334191</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.349516</td>\n",
       "      <td>0.353494</td>\n",
       "      <td>0.336844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.034400</td>\n",
       "      <td>1.279820</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.381192</td>\n",
       "      <td>0.403538</td>\n",
       "      <td>0.376167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.907300</td>\n",
       "      <td>1.206099</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.411168</td>\n",
       "      <td>0.409922</td>\n",
       "      <td>0.398091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.788100</td>\n",
       "      <td>1.158741</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.485124</td>\n",
       "      <td>0.421730</td>\n",
       "      <td>0.420705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.714900</td>\n",
       "      <td>1.127655</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.474718</td>\n",
       "      <td>0.443634</td>\n",
       "      <td>0.442803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>1.105764</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.492907</td>\n",
       "      <td>0.464380</td>\n",
       "      <td>0.466941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>1.076207</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.490728</td>\n",
       "      <td>0.469862</td>\n",
       "      <td>0.467836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>1.044597</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.492043</td>\n",
       "      <td>0.485658</td>\n",
       "      <td>0.482593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>1.039283</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.488732</td>\n",
       "      <td>0.484875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>1.034840</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.491666</td>\n",
       "      <td>0.490561</td>\n",
       "      <td>0.485262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>1.024724</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.492235</td>\n",
       "      <td>0.490053</td>\n",
       "      <td>0.483646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>1.016997</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.509028</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.492323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>1.022199</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.500912</td>\n",
       "      <td>0.487902</td>\n",
       "      <td>0.481423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>1.004024</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.499642</td>\n",
       "      <td>0.490292</td>\n",
       "      <td>0.487600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:41:44,831] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 97 with params: {'learning_rate': 0.0003121423766556698, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.457100</td>\n",
       "      <td>3.048447</td>\n",
       "      <td>0.381302</td>\n",
       "      <td>0.061191</td>\n",
       "      <td>0.080688</td>\n",
       "      <td>0.062555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.787700</td>\n",
       "      <td>2.473094</td>\n",
       "      <td>0.480293</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>0.129172</td>\n",
       "      <td>0.105765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.264900</td>\n",
       "      <td>2.027138</td>\n",
       "      <td>0.585701</td>\n",
       "      <td>0.279752</td>\n",
       "      <td>0.223854</td>\n",
       "      <td>0.213406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.838300</td>\n",
       "      <td>1.729682</td>\n",
       "      <td>0.678277</td>\n",
       "      <td>0.341301</td>\n",
       "      <td>0.302217</td>\n",
       "      <td>0.285955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.527800</td>\n",
       "      <td>1.513347</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.333014</td>\n",
       "      <td>0.344993</td>\n",
       "      <td>0.323681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.270100</td>\n",
       "      <td>1.357136</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.364850</td>\n",
       "      <td>0.361155</td>\n",
       "      <td>0.342229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.067000</td>\n",
       "      <td>1.269866</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.373721</td>\n",
       "      <td>0.369742</td>\n",
       "      <td>0.351096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>1.219948</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>0.416393</td>\n",
       "      <td>0.399463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>1.152969</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.489824</td>\n",
       "      <td>0.428747</td>\n",
       "      <td>0.426355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>1.110487</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.470753</td>\n",
       "      <td>0.425716</td>\n",
       "      <td>0.426620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>1.084267</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.453858</td>\n",
       "      <td>0.453652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.530800</td>\n",
       "      <td>1.055796</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.501131</td>\n",
       "      <td>0.486632</td>\n",
       "      <td>0.484653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>1.039526</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.517194</td>\n",
       "      <td>0.495727</td>\n",
       "      <td>0.495745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>1.005578</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.501308</td>\n",
       "      <td>0.493085</td>\n",
       "      <td>0.489535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.384400</td>\n",
       "      <td>1.007192</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.517316</td>\n",
       "      <td>0.493627</td>\n",
       "      <td>0.493580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.992846</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.516754</td>\n",
       "      <td>0.510210</td>\n",
       "      <td>0.505953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.536038</td>\n",
       "      <td>0.510609</td>\n",
       "      <td>0.507237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.988355</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.529123</td>\n",
       "      <td>0.535204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.565945</td>\n",
       "      <td>0.532264</td>\n",
       "      <td>0.533215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0.982472</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.593491</td>\n",
       "      <td>0.539956</td>\n",
       "      <td>0.552578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:43:33,365] Trial 97 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98 with params: {'learning_rate': 0.00020799099595853834, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:29 < 02:27, 5.93 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.567800</td>\n",
       "      <td>3.258699</td>\n",
       "      <td>0.205316</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>0.028027</td>\n",
       "      <td>0.019281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.060800</td>\n",
       "      <td>2.815453</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.066742</td>\n",
       "      <td>0.097369</td>\n",
       "      <td>0.073137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.659300</td>\n",
       "      <td>2.436573</td>\n",
       "      <td>0.480293</td>\n",
       "      <td>0.139365</td>\n",
       "      <td>0.126757</td>\n",
       "      <td>0.104407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.294800</td>\n",
       "      <td>2.133754</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.222372</td>\n",
       "      <td>0.196743</td>\n",
       "      <td>0.180757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.014700</td>\n",
       "      <td>1.894134</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.287143</td>\n",
       "      <td>0.254482</td>\n",
       "      <td>0.238017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:44:03,720] Trial 98 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 99 with params: {'learning_rate': 0.00030690900468464494, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.40 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.488600</td>\n",
       "      <td>3.082783</td>\n",
       "      <td>0.358387</td>\n",
       "      <td>0.067767</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.060385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.822000</td>\n",
       "      <td>2.508441</td>\n",
       "      <td>0.462878</td>\n",
       "      <td>0.115407</td>\n",
       "      <td>0.117495</td>\n",
       "      <td>0.089829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.299200</td>\n",
       "      <td>2.058732</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.273911</td>\n",
       "      <td>0.203072</td>\n",
       "      <td>0.188941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.871600</td>\n",
       "      <td>1.758078</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.319694</td>\n",
       "      <td>0.291644</td>\n",
       "      <td>0.276104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.556300</td>\n",
       "      <td>1.537537</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.347039</td>\n",
       "      <td>0.353978</td>\n",
       "      <td>0.333941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.289900</td>\n",
       "      <td>1.368103</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.360619</td>\n",
       "      <td>0.364462</td>\n",
       "      <td>0.347372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.082800</td>\n",
       "      <td>1.280535</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.347266</td>\n",
       "      <td>0.363344</td>\n",
       "      <td>0.342208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>1.234635</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.405506</td>\n",
       "      <td>0.418038</td>\n",
       "      <td>0.393501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>1.166609</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.447338</td>\n",
       "      <td>0.422637</td>\n",
       "      <td>0.414985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>1.124783</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.472975</td>\n",
       "      <td>0.419082</td>\n",
       "      <td>0.418830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>1.095482</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.484522</td>\n",
       "      <td>0.463111</td>\n",
       "      <td>0.464724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.545100</td>\n",
       "      <td>1.075427</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.498766</td>\n",
       "      <td>0.479989</td>\n",
       "      <td>0.478242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>1.056526</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.478402</td>\n",
       "      <td>0.482652</td>\n",
       "      <td>0.475140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>1.020429</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.488625</td>\n",
       "      <td>0.485335</td>\n",
       "      <td>0.480804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>1.018538</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.505955</td>\n",
       "      <td>0.489741</td>\n",
       "      <td>0.487351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>1.009831</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.485087</td>\n",
       "      <td>0.489073</td>\n",
       "      <td>0.479460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>1.005540</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.487854</td>\n",
       "      <td>0.499151</td>\n",
       "      <td>0.488001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>1.002558</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.511841</td>\n",
       "      <td>0.502980</td>\n",
       "      <td>0.497566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>1.010596</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.557808</td>\n",
       "      <td>0.524716</td>\n",
       "      <td>0.525767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.995433</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.562546</td>\n",
       "      <td>0.512807</td>\n",
       "      <td>0.520072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:45:53,831] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 with params: {'learning_rate': 0.0003973352855901667, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.382000</td>\n",
       "      <td>2.909615</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.092022</td>\n",
       "      <td>0.071664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.612800</td>\n",
       "      <td>2.274921</td>\n",
       "      <td>0.538955</td>\n",
       "      <td>0.179491</td>\n",
       "      <td>0.173472</td>\n",
       "      <td>0.158958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.035100</td>\n",
       "      <td>1.813856</td>\n",
       "      <td>0.638863</td>\n",
       "      <td>0.276099</td>\n",
       "      <td>0.265482</td>\n",
       "      <td>0.249104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.596000</td>\n",
       "      <td>1.542432</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.334698</td>\n",
       "      <td>0.329275</td>\n",
       "      <td>0.306208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.279000</td>\n",
       "      <td>1.364062</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.347723</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>0.344150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.041000</td>\n",
       "      <td>1.222071</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.441695</td>\n",
       "      <td>0.390609</td>\n",
       "      <td>0.380972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>1.155713</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.454539</td>\n",
       "      <td>0.414391</td>\n",
       "      <td>0.408147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>1.112671</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.469578</td>\n",
       "      <td>0.448466</td>\n",
       "      <td>0.442732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>1.060912</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.496440</td>\n",
       "      <td>0.468575</td>\n",
       "      <td>0.466780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>1.032474</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.488506</td>\n",
       "      <td>0.466478</td>\n",
       "      <td>0.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>1.017101</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.491739</td>\n",
       "      <td>0.483804</td>\n",
       "      <td>0.481618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.995277</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.541332</td>\n",
       "      <td>0.501856</td>\n",
       "      <td>0.508434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>1.002972</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.553658</td>\n",
       "      <td>0.515975</td>\n",
       "      <td>0.518923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.975706</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.562783</td>\n",
       "      <td>0.516361</td>\n",
       "      <td>0.522430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.990469</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.576082</td>\n",
       "      <td>0.529083</td>\n",
       "      <td>0.534216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.975462</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.563076</td>\n",
       "      <td>0.538888</td>\n",
       "      <td>0.534250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.602123</td>\n",
       "      <td>0.563449</td>\n",
       "      <td>0.565218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.985092</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.620948</td>\n",
       "      <td>0.565332</td>\n",
       "      <td>0.573392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.995919</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.644225</td>\n",
       "      <td>0.572432</td>\n",
       "      <td>0.589038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.988573</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.635245</td>\n",
       "      <td>0.573274</td>\n",
       "      <td>0.587881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.995104</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.680020</td>\n",
       "      <td>0.592350</td>\n",
       "      <td>0.618223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.988052</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>0.614632</td>\n",
       "      <td>0.635640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.994221</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.705049</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.638888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.991884</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.675909</td>\n",
       "      <td>0.605617</td>\n",
       "      <td>0.622933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.994373</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.743342</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.688171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.719485</td>\n",
       "      <td>0.633883</td>\n",
       "      <td>0.655855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>1.000959</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.743931</td>\n",
       "      <td>0.665024</td>\n",
       "      <td>0.687780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.002318</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.725333</td>\n",
       "      <td>0.647210</td>\n",
       "      <td>0.668852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>1.000682</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.720796</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>0.665980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>1.001153</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.720433</td>\n",
       "      <td>0.645771</td>\n",
       "      <td>0.665571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:48:39,483] Trial 100 finished with value: 0.6655710656038671 and parameters: {'learning_rate': 0.0003973352855901667, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 101 with params: {'learning_rate': 0.00033577375407781135, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.52 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.434400</td>\n",
       "      <td>3.005298</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.056541</td>\n",
       "      <td>0.084880</td>\n",
       "      <td>0.063615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.732500</td>\n",
       "      <td>2.410330</td>\n",
       "      <td>0.494959</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.137051</td>\n",
       "      <td>0.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.191700</td>\n",
       "      <td>1.957805</td>\n",
       "      <td>0.606783</td>\n",
       "      <td>0.287882</td>\n",
       "      <td>0.242998</td>\n",
       "      <td>0.233485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.760500</td>\n",
       "      <td>1.667919</td>\n",
       "      <td>0.687443</td>\n",
       "      <td>0.338323</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.293562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.449800</td>\n",
       "      <td>1.464416</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.340816</td>\n",
       "      <td>0.352977</td>\n",
       "      <td>0.330382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.199600</td>\n",
       "      <td>1.313282</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.386161</td>\n",
       "      <td>0.366891</td>\n",
       "      <td>0.352002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>1.235761</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.398725</td>\n",
       "      <td>0.379929</td>\n",
       "      <td>0.363529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>1.181113</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.427092</td>\n",
       "      <td>0.424803</td>\n",
       "      <td>0.408445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.732300</td>\n",
       "      <td>1.129041</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.469494</td>\n",
       "      <td>0.433123</td>\n",
       "      <td>0.427237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>1.091623</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.486332</td>\n",
       "      <td>0.445278</td>\n",
       "      <td>0.450458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.557300</td>\n",
       "      <td>1.054348</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.474877</td>\n",
       "      <td>0.479706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>1.029479</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.500165</td>\n",
       "      <td>0.486761</td>\n",
       "      <td>0.485565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>1.019683</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.516193</td>\n",
       "      <td>0.495336</td>\n",
       "      <td>0.495803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.985541</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.497826</td>\n",
       "      <td>0.491063</td>\n",
       "      <td>0.488678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.994394</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.560293</td>\n",
       "      <td>0.513867</td>\n",
       "      <td>0.516379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.979914</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.557418</td>\n",
       "      <td>0.523818</td>\n",
       "      <td>0.524387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.985899</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.537863</td>\n",
       "      <td>0.525745</td>\n",
       "      <td>0.521040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.981210</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.567497</td>\n",
       "      <td>0.541781</td>\n",
       "      <td>0.543136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.981697</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.602728</td>\n",
       "      <td>0.555878</td>\n",
       "      <td>0.565128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.975986</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.592773</td>\n",
       "      <td>0.543661</td>\n",
       "      <td>0.556298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:50:27,717] Trial 101 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 102 with params: {'learning_rate': 0.00027497643387713064, 'weight_decay': 0.004, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.70 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.496700</td>\n",
       "      <td>3.126118</td>\n",
       "      <td>0.331806</td>\n",
       "      <td>0.068579</td>\n",
       "      <td>0.066831</td>\n",
       "      <td>0.053061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.886500</td>\n",
       "      <td>2.592237</td>\n",
       "      <td>0.449129</td>\n",
       "      <td>0.082566</td>\n",
       "      <td>0.111002</td>\n",
       "      <td>0.079272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.405400</td>\n",
       "      <td>2.170680</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.228511</td>\n",
       "      <td>0.178202</td>\n",
       "      <td>0.163302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.995400</td>\n",
       "      <td>1.860813</td>\n",
       "      <td>0.650779</td>\n",
       "      <td>0.279835</td>\n",
       "      <td>0.267660</td>\n",
       "      <td>0.250620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.686000</td>\n",
       "      <td>1.623758</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.321679</td>\n",
       "      <td>0.333349</td>\n",
       "      <td>0.311660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.411200</td>\n",
       "      <td>1.445042</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.340446</td>\n",
       "      <td>0.349996</td>\n",
       "      <td>0.328294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.337676</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.346097</td>\n",
       "      <td>0.350675</td>\n",
       "      <td>0.333755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.045800</td>\n",
       "      <td>1.277403</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.394715</td>\n",
       "      <td>0.404764</td>\n",
       "      <td>0.378471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.918600</td>\n",
       "      <td>1.211984</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.430403</td>\n",
       "      <td>0.419228</td>\n",
       "      <td>0.405291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.797700</td>\n",
       "      <td>1.157890</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.485583</td>\n",
       "      <td>0.425654</td>\n",
       "      <td>0.425438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>1.128860</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.446340</td>\n",
       "      <td>0.434075</td>\n",
       "      <td>0.426973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>1.100658</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.493837</td>\n",
       "      <td>0.466176</td>\n",
       "      <td>0.467425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>1.073756</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.482109</td>\n",
       "      <td>0.462992</td>\n",
       "      <td>0.460803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.514100</td>\n",
       "      <td>1.042028</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.488414</td>\n",
       "      <td>0.482119</td>\n",
       "      <td>0.478329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>1.036622</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.501854</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>0.486515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>1.029095</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.486385</td>\n",
       "      <td>0.490384</td>\n",
       "      <td>0.482514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>1.017060</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.493661</td>\n",
       "      <td>0.496286</td>\n",
       "      <td>0.489539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>1.006961</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.505231</td>\n",
       "      <td>0.494153</td>\n",
       "      <td>0.491510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>1.009441</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.501404</td>\n",
       "      <td>0.497314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.989544</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.538444</td>\n",
       "      <td>0.503749</td>\n",
       "      <td>0.506861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:52:13,078] Trial 102 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 103 with params: {'learning_rate': 1.4169288463186063e-05, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:46, 6.55 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.860400</td>\n",
       "      <td>3.809244</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.783700</td>\n",
       "      <td>3.741844</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>0.009642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.731500</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.022910</td>\n",
       "      <td>0.010564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.679500</td>\n",
       "      <td>3.641033</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.645000</td>\n",
       "      <td>3.595852</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.595700</td>\n",
       "      <td>3.553714</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.555200</td>\n",
       "      <td>3.513590</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.522700</td>\n",
       "      <td>3.476527</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.023561</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.486600</td>\n",
       "      <td>3.441344</td>\n",
       "      <td>0.192484</td>\n",
       "      <td>0.063611</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>0.013832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.457300</td>\n",
       "      <td>3.408248</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.083687</td>\n",
       "      <td>0.029440</td>\n",
       "      <td>0.020702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:53:07,343] Trial 103 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 104 with params: {'learning_rate': 0.0004722321097158757, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.412800</td>\n",
       "      <td>2.901514</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.066090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.567300</td>\n",
       "      <td>2.208069</td>\n",
       "      <td>0.533456</td>\n",
       "      <td>0.186301</td>\n",
       "      <td>0.171446</td>\n",
       "      <td>0.155301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.930800</td>\n",
       "      <td>1.714222</td>\n",
       "      <td>0.650779</td>\n",
       "      <td>0.293817</td>\n",
       "      <td>0.281854</td>\n",
       "      <td>0.262214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.462000</td>\n",
       "      <td>1.449397</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.325876</td>\n",
       "      <td>0.331309</td>\n",
       "      <td>0.306138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.136900</td>\n",
       "      <td>1.310620</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.430456</td>\n",
       "      <td>0.408886</td>\n",
       "      <td>0.387720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>1.171513</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.477080</td>\n",
       "      <td>0.411005</td>\n",
       "      <td>0.407285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.719500</td>\n",
       "      <td>1.142840</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.478884</td>\n",
       "      <td>0.445018</td>\n",
       "      <td>0.445580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.100064</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.473989</td>\n",
       "      <td>0.460569</td>\n",
       "      <td>0.450417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>1.051618</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.498562</td>\n",
       "      <td>0.480897</td>\n",
       "      <td>0.476387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.392100</td>\n",
       "      <td>1.047729</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.491424</td>\n",
       "      <td>0.479433</td>\n",
       "      <td>0.478053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>1.013328</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.486321</td>\n",
       "      <td>0.483226</td>\n",
       "      <td>0.477044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>1.023872</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.578075</td>\n",
       "      <td>0.521886</td>\n",
       "      <td>0.532223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>1.016067</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.560750</td>\n",
       "      <td>0.535972</td>\n",
       "      <td>0.535689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>1.023474</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.580595</td>\n",
       "      <td>0.542408</td>\n",
       "      <td>0.548020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>1.015221</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.624305</td>\n",
       "      <td>0.571064</td>\n",
       "      <td>0.583677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>1.016915</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.648274</td>\n",
       "      <td>0.596654</td>\n",
       "      <td>0.606081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>1.019374</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.676706</td>\n",
       "      <td>0.621717</td>\n",
       "      <td>0.633392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>1.013613</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.686348</td>\n",
       "      <td>0.621016</td>\n",
       "      <td>0.635591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>1.050387</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.676718</td>\n",
       "      <td>0.618713</td>\n",
       "      <td>0.629620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>1.048156</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.717291</td>\n",
       "      <td>0.638354</td>\n",
       "      <td>0.658179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.059972</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.709616</td>\n",
       "      <td>0.637801</td>\n",
       "      <td>0.655029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>1.059325</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.722579</td>\n",
       "      <td>0.657739</td>\n",
       "      <td>0.672532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>1.062814</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.720630</td>\n",
       "      <td>0.653334</td>\n",
       "      <td>0.670822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>1.068998</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.720684</td>\n",
       "      <td>0.653547</td>\n",
       "      <td>0.671504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1.066830</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.753763</td>\n",
       "      <td>0.666083</td>\n",
       "      <td>0.689935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.066549</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.739065</td>\n",
       "      <td>0.667748</td>\n",
       "      <td>0.686062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>1.070895</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.752808</td>\n",
       "      <td>0.666292</td>\n",
       "      <td>0.689237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>1.069883</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.752675</td>\n",
       "      <td>0.666746</td>\n",
       "      <td>0.689368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.068159</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.751163</td>\n",
       "      <td>0.664596</td>\n",
       "      <td>0.687203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>1.069378</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.751877</td>\n",
       "      <td>0.664596</td>\n",
       "      <td>0.687538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:55:54,410] Trial 104 finished with value: 0.687537608681543 and parameters: {'learning_rate': 0.0004722321097158757, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 4}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 105 with params: {'learning_rate': 1.4771448129559617e-06, 'weight_decay': 0.007, 'adam_beta1': 0.98, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:50 < 01:41, 6.88 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.890200</td>\n",
       "      <td>3.874850</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.874400</td>\n",
       "      <td>3.865748</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.871300</td>\n",
       "      <td>3.857579</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.860600</td>\n",
       "      <td>3.850141</td>\n",
       "      <td>0.016499</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.003121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.857400</td>\n",
       "      <td>3.843163</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>0.003255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.846300</td>\n",
       "      <td>3.836663</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>0.004450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.839600</td>\n",
       "      <td>3.830513</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.030221</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.006073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.835200</td>\n",
       "      <td>3.824734</td>\n",
       "      <td>0.051329</td>\n",
       "      <td>0.030935</td>\n",
       "      <td>0.027057</td>\n",
       "      <td>0.007247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>3.819167</td>\n",
       "      <td>0.055912</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>0.006915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.824400</td>\n",
       "      <td>3.813950</td>\n",
       "      <td>0.076077</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.029370</td>\n",
       "      <td>0.007855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:56:46,485] Trial 105 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106 with params: {'learning_rate': 0.0004422979949673411, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.343300</td>\n",
       "      <td>2.832730</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.099959</td>\n",
       "      <td>0.077034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.516500</td>\n",
       "      <td>2.169928</td>\n",
       "      <td>0.556370</td>\n",
       "      <td>0.213170</td>\n",
       "      <td>0.194114</td>\n",
       "      <td>0.178849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.912400</td>\n",
       "      <td>1.705527</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.286557</td>\n",
       "      <td>0.286007</td>\n",
       "      <td>0.266318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.467200</td>\n",
       "      <td>1.439922</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.314613</td>\n",
       "      <td>0.320292</td>\n",
       "      <td>0.300363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>1.301110</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.364906</td>\n",
       "      <td>0.333941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.938200</td>\n",
       "      <td>1.172266</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.435230</td>\n",
       "      <td>0.396082</td>\n",
       "      <td>0.385796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>1.122827</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.470741</td>\n",
       "      <td>0.436181</td>\n",
       "      <td>0.430889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>1.081494</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.489523</td>\n",
       "      <td>0.462829</td>\n",
       "      <td>0.458259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>1.041723</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.489476</td>\n",
       "      <td>0.477272</td>\n",
       "      <td>0.474438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>1.005922</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.487336</td>\n",
       "      <td>0.483437</td>\n",
       "      <td>0.480676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>1.001194</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.552342</td>\n",
       "      <td>0.518674</td>\n",
       "      <td>0.521480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.986672</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.570323</td>\n",
       "      <td>0.527742</td>\n",
       "      <td>0.533018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.995921</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.586173</td>\n",
       "      <td>0.553373</td>\n",
       "      <td>0.556894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.967155</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.618936</td>\n",
       "      <td>0.555080</td>\n",
       "      <td>0.570318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.984025</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.608806</td>\n",
       "      <td>0.554493</td>\n",
       "      <td>0.562266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.986407</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.628232</td>\n",
       "      <td>0.569061</td>\n",
       "      <td>0.580868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.974692</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.661916</td>\n",
       "      <td>0.594829</td>\n",
       "      <td>0.610036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.982838</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.654570</td>\n",
       "      <td>0.588099</td>\n",
       "      <td>0.605063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.998509</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.672137</td>\n",
       "      <td>0.593667</td>\n",
       "      <td>0.611682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.689222</td>\n",
       "      <td>0.613804</td>\n",
       "      <td>0.629207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>1.017840</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.721816</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.668182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>1.003814</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.721745</td>\n",
       "      <td>0.639839</td>\n",
       "      <td>0.661267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>1.010870</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.714526</td>\n",
       "      <td>0.635202</td>\n",
       "      <td>0.652824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>1.006859</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.743716</td>\n",
       "      <td>0.664209</td>\n",
       "      <td>0.683698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>1.012736</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.747016</td>\n",
       "      <td>0.669275</td>\n",
       "      <td>0.688427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>1.014935</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.765689</td>\n",
       "      <td>0.672931</td>\n",
       "      <td>0.696447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>1.018245</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.743262</td>\n",
       "      <td>0.663474</td>\n",
       "      <td>0.682984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>1.019063</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.743798</td>\n",
       "      <td>0.665110</td>\n",
       "      <td>0.684655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.018545</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.750573</td>\n",
       "      <td>0.675598</td>\n",
       "      <td>0.692971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>1.018854</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.750059</td>\n",
       "      <td>0.675598</td>\n",
       "      <td>0.692709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 12:59:33,186] Trial 106 finished with value: 0.692708547304787 and parameters: {'learning_rate': 0.0004422979949673411, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 107 with params: {'learning_rate': 0.0004980467405049822, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.305500</td>\n",
       "      <td>2.761910</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.064144</td>\n",
       "      <td>0.106276</td>\n",
       "      <td>0.077407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.425600</td>\n",
       "      <td>2.073035</td>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.209299</td>\n",
       "      <td>0.199378</td>\n",
       "      <td>0.177179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.792700</td>\n",
       "      <td>1.606267</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.279732</td>\n",
       "      <td>0.288879</td>\n",
       "      <td>0.267591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.344700</td>\n",
       "      <td>1.356084</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.329054</td>\n",
       "      <td>0.330410</td>\n",
       "      <td>0.312055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.039100</td>\n",
       "      <td>1.225432</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.331365</td>\n",
       "      <td>0.368682</td>\n",
       "      <td>0.338287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>1.131178</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.449852</td>\n",
       "      <td>0.412132</td>\n",
       "      <td>0.405395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>1.090843</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.495028</td>\n",
       "      <td>0.461904</td>\n",
       "      <td>0.455904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>1.056312</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.482825</td>\n",
       "      <td>0.469666</td>\n",
       "      <td>0.466106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>1.015088</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.511113</td>\n",
       "      <td>0.498309</td>\n",
       "      <td>0.492440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.991901</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.508422</td>\n",
       "      <td>0.496431</td>\n",
       "      <td>0.491619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>1.012208</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.581653</td>\n",
       "      <td>0.536017</td>\n",
       "      <td>0.541985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.995015</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.604030</td>\n",
       "      <td>0.562050</td>\n",
       "      <td>0.570246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.992660</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.611559</td>\n",
       "      <td>0.582107</td>\n",
       "      <td>0.585850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.994166</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.677154</td>\n",
       "      <td>0.596097</td>\n",
       "      <td>0.621492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.989453</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.690641</td>\n",
       "      <td>0.613085</td>\n",
       "      <td>0.633780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>1.014332</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.668661</td>\n",
       "      <td>0.612716</td>\n",
       "      <td>0.623279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>1.002229</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.687915</td>\n",
       "      <td>0.632431</td>\n",
       "      <td>0.638901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.984966</td>\n",
       "      <td>0.807516</td>\n",
       "      <td>0.723354</td>\n",
       "      <td>0.636155</td>\n",
       "      <td>0.658555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>1.016726</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.746380</td>\n",
       "      <td>0.656324</td>\n",
       "      <td>0.680645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>1.016183</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.770923</td>\n",
       "      <td>0.683683</td>\n",
       "      <td>0.706309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.036198</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.769730</td>\n",
       "      <td>0.687723</td>\n",
       "      <td>0.710334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>1.020633</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.768569</td>\n",
       "      <td>0.689652</td>\n",
       "      <td>0.709955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1.034493</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.767618</td>\n",
       "      <td>0.683863</td>\n",
       "      <td>0.703676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>1.035588</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.771958</td>\n",
       "      <td>0.686944</td>\n",
       "      <td>0.707887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>1.051751</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.759035</td>\n",
       "      <td>0.680125</td>\n",
       "      <td>0.700229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>1.045403</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.755347</td>\n",
       "      <td>0.684733</td>\n",
       "      <td>0.702695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>1.048918</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.754587</td>\n",
       "      <td>0.688896</td>\n",
       "      <td>0.704999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>1.046745</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.751136</td>\n",
       "      <td>0.680280</td>\n",
       "      <td>0.696367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>1.044488</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.752728</td>\n",
       "      <td>0.689134</td>\n",
       "      <td>0.703968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>1.046030</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.751150</td>\n",
       "      <td>0.679706</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:02:24,172] Trial 107 finished with value: 0.6956523753244204 and parameters: {'learning_rate': 0.0004980467405049822, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 108 with params: {'learning_rate': 1.5293018718400694e-05, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:12, 6.58 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.858100</td>\n",
       "      <td>3.803776</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.008032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.776300</td>\n",
       "      <td>3.731242</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.011622</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.010142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.720500</td>\n",
       "      <td>3.674609</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.664800</td>\n",
       "      <td>3.624470</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.010592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.627400</td>\n",
       "      <td>3.574342</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:02:54,350] Trial 108 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 109 with params: {'learning_rate': 0.00016595892826902088, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:43 < 00:51, 6.73 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.636400</td>\n",
       "      <td>3.373672</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.212500</td>\n",
       "      <td>3.007541</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.057122</td>\n",
       "      <td>0.084418</td>\n",
       "      <td>0.063683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.882700</td>\n",
       "      <td>2.683587</td>\n",
       "      <td>0.457379</td>\n",
       "      <td>0.081345</td>\n",
       "      <td>0.115570</td>\n",
       "      <td>0.085452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.561000</td>\n",
       "      <td>2.400956</td>\n",
       "      <td>0.506874</td>\n",
       "      <td>0.141377</td>\n",
       "      <td>0.147972</td>\n",
       "      <td>0.125969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.310300</td>\n",
       "      <td>2.159630</td>\n",
       "      <td>0.569203</td>\n",
       "      <td>0.218145</td>\n",
       "      <td>0.193843</td>\n",
       "      <td>0.177672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.050500</td>\n",
       "      <td>1.967952</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.273736</td>\n",
       "      <td>0.227649</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>1.815635</td>\n",
       "      <td>0.653529</td>\n",
       "      <td>0.318187</td>\n",
       "      <td>0.273342</td>\n",
       "      <td>0.261621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.683000</td>\n",
       "      <td>1.687450</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.369431</td>\n",
       "      <td>0.325625</td>\n",
       "      <td>0.315410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.519900</td>\n",
       "      <td>1.575008</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.349163</td>\n",
       "      <td>0.324740</td>\n",
       "      <td>0.309741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.379800</td>\n",
       "      <td>1.491096</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.373637</td>\n",
       "      <td>0.349232</td>\n",
       "      <td>0.335777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.283900</td>\n",
       "      <td>1.419970</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.359487</td>\n",
       "      <td>0.370809</td>\n",
       "      <td>0.352378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>1.370851</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.356553</td>\n",
       "      <td>0.361980</td>\n",
       "      <td>0.343239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.104500</td>\n",
       "      <td>1.318538</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.376609</td>\n",
       "      <td>0.386559</td>\n",
       "      <td>0.365202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.013400</td>\n",
       "      <td>1.273528</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.384495</td>\n",
       "      <td>0.397842</td>\n",
       "      <td>0.378120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.958200</td>\n",
       "      <td>1.257684</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.381137</td>\n",
       "      <td>0.401748</td>\n",
       "      <td>0.380078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>1.236610</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.416746</td>\n",
       "      <td>0.413646</td>\n",
       "      <td>0.400994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.842400</td>\n",
       "      <td>1.217342</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.414444</td>\n",
       "      <td>0.397802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>1.187736</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.397371</td>\n",
       "      <td>0.403232</td>\n",
       "      <td>0.388235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>1.180787</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.433997</td>\n",
       "      <td>0.425485</td>\n",
       "      <td>0.414960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.741400</td>\n",
       "      <td>1.162699</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.457778</td>\n",
       "      <td>0.433976</td>\n",
       "      <td>0.428599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:04:39,065] Trial 109 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 110 with params: {'learning_rate': 0.0004264632669406636, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:52, 6.20 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.362300</td>\n",
       "      <td>2.877786</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.074439</td>\n",
       "      <td>0.094180</td>\n",
       "      <td>0.074381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.574600</td>\n",
       "      <td>2.234995</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.176492</td>\n",
       "      <td>0.183563</td>\n",
       "      <td>0.166290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.987900</td>\n",
       "      <td>1.772440</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.267605</td>\n",
       "      <td>0.260992</td>\n",
       "      <td>0.245006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.542700</td>\n",
       "      <td>1.505696</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.302143</td>\n",
       "      <td>0.325043</td>\n",
       "      <td>0.292802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.224100</td>\n",
       "      <td>1.333420</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.345310</td>\n",
       "      <td>0.366846</td>\n",
       "      <td>0.339091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.987200</td>\n",
       "      <td>1.194474</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.449986</td>\n",
       "      <td>0.393907</td>\n",
       "      <td>0.385726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.792700</td>\n",
       "      <td>1.133364</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.440156</td>\n",
       "      <td>0.415263</td>\n",
       "      <td>0.407005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>1.094494</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.489651</td>\n",
       "      <td>0.459098</td>\n",
       "      <td>0.456536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>1.049623</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.488607</td>\n",
       "      <td>0.474648</td>\n",
       "      <td>0.469393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>1.021571</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.469008</td>\n",
       "      <td>0.458460</td>\n",
       "      <td>0.452554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:05:36,252] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 111 with params: {'learning_rate': 0.00035593962840501514, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:19, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.424700</td>\n",
       "      <td>3.001115</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.083790</td>\n",
       "      <td>0.064168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.727300</td>\n",
       "      <td>2.413249</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.177948</td>\n",
       "      <td>0.137841</td>\n",
       "      <td>0.118640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.192800</td>\n",
       "      <td>1.966419</td>\n",
       "      <td>0.577452</td>\n",
       "      <td>0.249556</td>\n",
       "      <td>0.210863</td>\n",
       "      <td>0.195603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.755000</td>\n",
       "      <td>1.666570</td>\n",
       "      <td>0.678277</td>\n",
       "      <td>0.335364</td>\n",
       "      <td>0.306073</td>\n",
       "      <td>0.287383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.434800</td>\n",
       "      <td>1.453949</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.329655</td>\n",
       "      <td>0.350719</td>\n",
       "      <td>0.323923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.171300</td>\n",
       "      <td>1.289436</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.362241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>1.215231</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.431154</td>\n",
       "      <td>0.406333</td>\n",
       "      <td>0.394977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>1.159761</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.446620</td>\n",
       "      <td>0.430385</td>\n",
       "      <td>0.416787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>1.116768</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.490818</td>\n",
       "      <td>0.465840</td>\n",
       "      <td>0.460388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>1.098527</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.469907</td>\n",
       "      <td>0.449337</td>\n",
       "      <td>0.447238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>1.050694</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.509974</td>\n",
       "      <td>0.478922</td>\n",
       "      <td>0.482320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>1.027156</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.513963</td>\n",
       "      <td>0.479702</td>\n",
       "      <td>0.482481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>1.021971</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.501720</td>\n",
       "      <td>0.493386</td>\n",
       "      <td>0.487358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.522164</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.499571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>1.001499</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.561113</td>\n",
       "      <td>0.515021</td>\n",
       "      <td>0.519941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.988129</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.590276</td>\n",
       "      <td>0.535367</td>\n",
       "      <td>0.542737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.986441</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.580931</td>\n",
       "      <td>0.544363</td>\n",
       "      <td>0.547088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.982812</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.556329</td>\n",
       "      <td>0.560268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.995684</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.592860</td>\n",
       "      <td>0.556376</td>\n",
       "      <td>0.557101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.975117</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.585725</td>\n",
       "      <td>0.548418</td>\n",
       "      <td>0.555412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.976633</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.580231</td>\n",
       "      <td>0.548921</td>\n",
       "      <td>0.552746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.973892</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.573534</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.982583</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.647955</td>\n",
       "      <td>0.597698</td>\n",
       "      <td>0.609716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.983472</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.630939</td>\n",
       "      <td>0.578367</td>\n",
       "      <td>0.586849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.987201</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.662358</td>\n",
       "      <td>0.599342</td>\n",
       "      <td>0.613016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.987336</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.659137</td>\n",
       "      <td>0.600805</td>\n",
       "      <td>0.614972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.995212</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.684997</td>\n",
       "      <td>0.612949</td>\n",
       "      <td>0.630147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.993603</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.689940</td>\n",
       "      <td>0.611413</td>\n",
       "      <td>0.630052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.995570</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.691282</td>\n",
       "      <td>0.619699</td>\n",
       "      <td>0.636329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.995325</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.617477</td>\n",
       "      <td>0.631912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Fri Jan 10 23:13:59 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 13:08:57,445] Trial 111 finished with value: 0.6319116918647526 and parameters: {'learning_rate': 0.00035593962840501514, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 112 with params: {'learning_rate': 0.0004676508625031947, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.333100</td>\n",
       "      <td>2.823088</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.068967</td>\n",
       "      <td>0.097985</td>\n",
       "      <td>0.075811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.506000</td>\n",
       "      <td>2.160608</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.193674</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.172319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.897000</td>\n",
       "      <td>1.694160</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.284402</td>\n",
       "      <td>0.278322</td>\n",
       "      <td>0.258767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.451700</td>\n",
       "      <td>1.442897</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.303608</td>\n",
       "      <td>0.336709</td>\n",
       "      <td>0.303954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.137400</td>\n",
       "      <td>1.283657</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.364802</td>\n",
       "      <td>0.381661</td>\n",
       "      <td>0.354687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>1.162328</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.397314</td>\n",
       "      <td>0.389963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>1.115969</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.460927</td>\n",
       "      <td>0.432143</td>\n",
       "      <td>0.426830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>1.069580</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.463688</td>\n",
       "      <td>0.449096</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>1.046996</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.472166</td>\n",
       "      <td>0.467764</td>\n",
       "      <td>0.458854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>1.013773</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.471690</td>\n",
       "      <td>0.471220</td>\n",
       "      <td>0.465610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>1.004311</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.525143</td>\n",
       "      <td>0.504197</td>\n",
       "      <td>0.507323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.993813</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.589131</td>\n",
       "      <td>0.540651</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.981292</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.615492</td>\n",
       "      <td>0.564824</td>\n",
       "      <td>0.575609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.642550</td>\n",
       "      <td>0.576934</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.986162</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.669680</td>\n",
       "      <td>0.595174</td>\n",
       "      <td>0.616609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.995359</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.633893</td>\n",
       "      <td>0.579518</td>\n",
       "      <td>0.589250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.988136</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.631829</td>\n",
       "      <td>0.590036</td>\n",
       "      <td>0.592874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.646371</td>\n",
       "      <td>0.591335</td>\n",
       "      <td>0.603653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>1.022450</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.630617</td>\n",
       "      <td>0.579984</td>\n",
       "      <td>0.590821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>1.009072</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.735253</td>\n",
       "      <td>0.647825</td>\n",
       "      <td>0.671498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.025635</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.740671</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.683670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>1.016252</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.767778</td>\n",
       "      <td>0.680505</td>\n",
       "      <td>0.701912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>1.021062</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.768437</td>\n",
       "      <td>0.681177</td>\n",
       "      <td>0.702492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>1.021983</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.754662</td>\n",
       "      <td>0.670231</td>\n",
       "      <td>0.693785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1.020452</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.683853</td>\n",
       "      <td>0.704529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>1.020421</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.785327</td>\n",
       "      <td>0.691427</td>\n",
       "      <td>0.716030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.026585</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.769848</td>\n",
       "      <td>0.677208</td>\n",
       "      <td>0.702278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>1.025403</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.793160</td>\n",
       "      <td>0.686351</td>\n",
       "      <td>0.714618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>1.025061</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.779845</td>\n",
       "      <td>0.686313</td>\n",
       "      <td>0.710731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>1.025376</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.781463</td>\n",
       "      <td>0.686715</td>\n",
       "      <td>0.711901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:11:44,873] Trial 112 finished with value: 0.7119012417170726 and parameters: {'learning_rate': 0.0004676508625031947, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 113 with params: {'learning_rate': 0.00048823044269866357, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.311900</td>\n",
       "      <td>2.773949</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.065291</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.078104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.441000</td>\n",
       "      <td>2.089547</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.212092</td>\n",
       "      <td>0.197930</td>\n",
       "      <td>0.177070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.813000</td>\n",
       "      <td>1.623415</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.282847</td>\n",
       "      <td>0.285526</td>\n",
       "      <td>0.264839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.364200</td>\n",
       "      <td>1.368361</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.332339</td>\n",
       "      <td>0.331485</td>\n",
       "      <td>0.312891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.057700</td>\n",
       "      <td>1.237123</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.331167</td>\n",
       "      <td>0.368897</td>\n",
       "      <td>0.338254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>1.138285</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.454818</td>\n",
       "      <td>0.416316</td>\n",
       "      <td>0.409072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>1.101627</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.489505</td>\n",
       "      <td>0.452765</td>\n",
       "      <td>0.447735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.556500</td>\n",
       "      <td>1.060639</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.491046</td>\n",
       "      <td>0.473412</td>\n",
       "      <td>0.467539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>1.024054</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.486821</td>\n",
       "      <td>0.475312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>0.996840</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.492860</td>\n",
       "      <td>0.485338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>1.005342</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.532749</td>\n",
       "      <td>0.536014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.983983</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.591744</td>\n",
       "      <td>0.558207</td>\n",
       "      <td>0.564360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.599166</td>\n",
       "      <td>0.572948</td>\n",
       "      <td>0.574430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.666860</td>\n",
       "      <td>0.598702</td>\n",
       "      <td>0.617537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.979389</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.664547</td>\n",
       "      <td>0.596964</td>\n",
       "      <td>0.614457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>1.003335</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.636028</td>\n",
       "      <td>0.592843</td>\n",
       "      <td>0.599704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.990877</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.620898</td>\n",
       "      <td>0.627483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.981125</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.719936</td>\n",
       "      <td>0.633680</td>\n",
       "      <td>0.655356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>1.008162</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>0.652076</td>\n",
       "      <td>0.674275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.738827</td>\n",
       "      <td>0.666669</td>\n",
       "      <td>0.684121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>1.021256</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.750441</td>\n",
       "      <td>0.681968</td>\n",
       "      <td>0.700142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>1.011883</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.765475</td>\n",
       "      <td>0.684099</td>\n",
       "      <td>0.705083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>1.028000</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.763536</td>\n",
       "      <td>0.681203</td>\n",
       "      <td>0.701261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.034081</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.769709</td>\n",
       "      <td>0.683603</td>\n",
       "      <td>0.705216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.042304</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.768260</td>\n",
       "      <td>0.685728</td>\n",
       "      <td>0.705684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.034251</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.757671</td>\n",
       "      <td>0.689819</td>\n",
       "      <td>0.706601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>1.037105</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.760524</td>\n",
       "      <td>0.691693</td>\n",
       "      <td>0.709181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>1.036893</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.748223</td>\n",
       "      <td>0.680998</td>\n",
       "      <td>0.694793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>1.036542</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.688509</td>\n",
       "      <td>0.700560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>1.037156</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.752746</td>\n",
       "      <td>0.684873</td>\n",
       "      <td>0.701186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:14:32,620] Trial 113 finished with value: 0.70118613130093 and parameters: {'learning_rate': 0.00048823044269866357, 'weight_decay': 0.003, 'adam_beta1': 0.9, 'warmup_steps': 0}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 114 with params: {'learning_rate': 0.0004693590307705194, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.91, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:43, 6.78 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.397200</td>\n",
       "      <td>2.885262</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.073106</td>\n",
       "      <td>0.093269</td>\n",
       "      <td>0.072770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.550600</td>\n",
       "      <td>2.193174</td>\n",
       "      <td>0.530706</td>\n",
       "      <td>0.176215</td>\n",
       "      <td>0.168496</td>\n",
       "      <td>0.151482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.917500</td>\n",
       "      <td>1.695232</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.283820</td>\n",
       "      <td>0.288779</td>\n",
       "      <td>0.267697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.449500</td>\n",
       "      <td>1.432570</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.314376</td>\n",
       "      <td>0.321409</td>\n",
       "      <td>0.298244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.124900</td>\n",
       "      <td>1.288111</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.374735</td>\n",
       "      <td>0.382385</td>\n",
       "      <td>0.357238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>1.163637</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.431751</td>\n",
       "      <td>0.401573</td>\n",
       "      <td>0.396215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.711700</td>\n",
       "      <td>1.116448</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.467282</td>\n",
       "      <td>0.458743</td>\n",
       "      <td>0.449249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>1.080504</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.471542</td>\n",
       "      <td>0.476555</td>\n",
       "      <td>0.463785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>1.036642</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.491899</td>\n",
       "      <td>0.483324</td>\n",
       "      <td>0.479738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.392800</td>\n",
       "      <td>1.025573</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.461571</td>\n",
       "      <td>0.464228</td>\n",
       "      <td>0.454569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:15:24,864] Trial 114 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 115 with params: {'learning_rate': 0.00037282715336766075, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.447000</td>\n",
       "      <td>3.023668</td>\n",
       "      <td>0.360220</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>0.073944</td>\n",
       "      <td>0.058428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.745600</td>\n",
       "      <td>2.434915</td>\n",
       "      <td>0.468378</td>\n",
       "      <td>0.151214</td>\n",
       "      <td>0.126836</td>\n",
       "      <td>0.101534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.207600</td>\n",
       "      <td>1.986398</td>\n",
       "      <td>0.571952</td>\n",
       "      <td>0.245983</td>\n",
       "      <td>0.200101</td>\n",
       "      <td>0.185597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.763500</td>\n",
       "      <td>1.682265</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.302015</td>\n",
       "      <td>0.292017</td>\n",
       "      <td>0.274210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.440100</td>\n",
       "      <td>1.453401</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.350769</td>\n",
       "      <td>0.352042</td>\n",
       "      <td>0.329439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.163900</td>\n",
       "      <td>1.291748</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.384460</td>\n",
       "      <td>0.368132</td>\n",
       "      <td>0.352599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.945700</td>\n",
       "      <td>1.213730</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.411057</td>\n",
       "      <td>0.397661</td>\n",
       "      <td>0.381413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>1.166164</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.463811</td>\n",
       "      <td>0.442089</td>\n",
       "      <td>0.431670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>1.117752</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.493761</td>\n",
       "      <td>0.469643</td>\n",
       "      <td>0.464917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>1.102473</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.506589</td>\n",
       "      <td>0.476726</td>\n",
       "      <td>0.475944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>1.057925</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>0.490729</td>\n",
       "      <td>0.493708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>1.031268</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.513826</td>\n",
       "      <td>0.488829</td>\n",
       "      <td>0.488077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>1.033470</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.483246</td>\n",
       "      <td>0.498450</td>\n",
       "      <td>0.484047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.525350</td>\n",
       "      <td>0.505494</td>\n",
       "      <td>0.503177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>1.006726</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.586281</td>\n",
       "      <td>0.528572</td>\n",
       "      <td>0.536220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.992528</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.601126</td>\n",
       "      <td>0.553828</td>\n",
       "      <td>0.560346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.986685</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.593137</td>\n",
       "      <td>0.551267</td>\n",
       "      <td>0.560003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.994185</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.589542</td>\n",
       "      <td>0.561636</td>\n",
       "      <td>0.561281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>1.008300</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.594792</td>\n",
       "      <td>0.561354</td>\n",
       "      <td>0.562292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>1.005878</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.597780</td>\n",
       "      <td>0.555276</td>\n",
       "      <td>0.562795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.993340</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.591485</td>\n",
       "      <td>0.558199</td>\n",
       "      <td>0.563118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.989639</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.646642</td>\n",
       "      <td>0.591342</td>\n",
       "      <td>0.603144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>1.000631</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.657613</td>\n",
       "      <td>0.599548</td>\n",
       "      <td>0.614379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>1.004432</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.649352</td>\n",
       "      <td>0.590663</td>\n",
       "      <td>0.604408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>1.005179</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.650841</td>\n",
       "      <td>0.597728</td>\n",
       "      <td>0.609283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>1.007814</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.647575</td>\n",
       "      <td>0.594315</td>\n",
       "      <td>0.606572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>1.012934</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.652095</td>\n",
       "      <td>0.593878</td>\n",
       "      <td>0.605074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>1.011422</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.656260</td>\n",
       "      <td>0.594109</td>\n",
       "      <td>0.606824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>1.010762</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.642756</td>\n",
       "      <td>0.595828</td>\n",
       "      <td>0.603877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>1.011341</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.649013</td>\n",
       "      <td>0.595828</td>\n",
       "      <td>0.604234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:18:14,386] Trial 115 finished with value: 0.6042338226625042 and parameters: {'learning_rate': 0.00037282715336766075, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 116 with params: {'learning_rate': 1.274731960791765e-06, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:49, 6.41 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.890700</td>\n",
       "      <td>3.876062</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.876200</td>\n",
       "      <td>3.868097</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.874200</td>\n",
       "      <td>3.860952</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.864300</td>\n",
       "      <td>3.854377</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.862000</td>\n",
       "      <td>3.848276</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.003519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.851900</td>\n",
       "      <td>3.842513</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.004898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.845700</td>\n",
       "      <td>3.837060</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.842100</td>\n",
       "      <td>3.831947</td>\n",
       "      <td>0.038497</td>\n",
       "      <td>0.027794</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>0.006461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.837500</td>\n",
       "      <td>3.826997</td>\n",
       "      <td>0.050412</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.027475</td>\n",
       "      <td>0.008263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.832500</td>\n",
       "      <td>3.822375</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.032754</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.008660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:19:09,620] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 117 with params: {'learning_rate': 0.00041782570595644595, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.407300</td>\n",
       "      <td>2.931126</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.076443</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.064335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.627500</td>\n",
       "      <td>2.288162</td>\n",
       "      <td>0.524290</td>\n",
       "      <td>0.163787</td>\n",
       "      <td>0.158379</td>\n",
       "      <td>0.139147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.039300</td>\n",
       "      <td>1.818049</td>\n",
       "      <td>0.637947</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>0.254892</td>\n",
       "      <td>0.240076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.582400</td>\n",
       "      <td>1.541424</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.297561</td>\n",
       "      <td>0.320352</td>\n",
       "      <td>0.292265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.256100</td>\n",
       "      <td>1.361193</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.363080</td>\n",
       "      <td>0.372554</td>\n",
       "      <td>0.346358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.010400</td>\n",
       "      <td>1.215136</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.458967</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.397058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>1.161806</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.455614</td>\n",
       "      <td>0.417750</td>\n",
       "      <td>0.410928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>1.113319</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.473405</td>\n",
       "      <td>0.451954</td>\n",
       "      <td>0.445190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>1.070449</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.496040</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.475834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>1.055263</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.471885</td>\n",
       "      <td>0.467444</td>\n",
       "      <td>0.460793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>1.013869</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.492632</td>\n",
       "      <td>0.481189</td>\n",
       "      <td>0.481001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>1.005808</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.538257</td>\n",
       "      <td>0.492164</td>\n",
       "      <td>0.498266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>1.009493</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.549966</td>\n",
       "      <td>0.524731</td>\n",
       "      <td>0.523696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.978465</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.565584</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.533146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.986586</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.585407</td>\n",
       "      <td>0.544925</td>\n",
       "      <td>0.551009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.639263</td>\n",
       "      <td>0.580770</td>\n",
       "      <td>0.588639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.985158</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.619581</td>\n",
       "      <td>0.574273</td>\n",
       "      <td>0.580960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.986979</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.648655</td>\n",
       "      <td>0.598929</td>\n",
       "      <td>0.606706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>1.005456</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.676383</td>\n",
       "      <td>0.604191</td>\n",
       "      <td>0.624037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>1.015696</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>0.614962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>1.018320</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.700861</td>\n",
       "      <td>0.610655</td>\n",
       "      <td>0.636627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>1.005535</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.685947</td>\n",
       "      <td>0.625261</td>\n",
       "      <td>0.639690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>1.016189</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.707725</td>\n",
       "      <td>0.640308</td>\n",
       "      <td>0.658589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>1.016801</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.713289</td>\n",
       "      <td>0.645481</td>\n",
       "      <td>0.664169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>1.022158</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.737548</td>\n",
       "      <td>0.662596</td>\n",
       "      <td>0.682861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>1.024636</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.660185</td>\n",
       "      <td>0.680153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>1.029387</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.741585</td>\n",
       "      <td>0.661973</td>\n",
       "      <td>0.682787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>1.029679</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.746576</td>\n",
       "      <td>0.668442</td>\n",
       "      <td>0.689522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>1.028400</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.741712</td>\n",
       "      <td>0.667668</td>\n",
       "      <td>0.686477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>1.029058</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.743970</td>\n",
       "      <td>0.668156</td>\n",
       "      <td>0.687944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:21:54,933] Trial 117 finished with value: 0.6879438343160191 and parameters: {'learning_rate': 0.00041782570595644595, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 1}. Best is trial 82 with value: 0.7143632508757508.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 118 with params: {'learning_rate': 0.0004762367229644209, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.331800</td>\n",
       "      <td>2.826898</td>\n",
       "      <td>0.416132</td>\n",
       "      <td>0.068680</td>\n",
       "      <td>0.096745</td>\n",
       "      <td>0.074965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.511900</td>\n",
       "      <td>2.172861</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.181060</td>\n",
       "      <td>0.194306</td>\n",
       "      <td>0.173241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.906500</td>\n",
       "      <td>1.703373</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.268859</td>\n",
       "      <td>0.247134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.455800</td>\n",
       "      <td>1.437585</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.301886</td>\n",
       "      <td>0.328543</td>\n",
       "      <td>0.298427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.134600</td>\n",
       "      <td>1.277180</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.372120</td>\n",
       "      <td>0.379308</td>\n",
       "      <td>0.356024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>1.158073</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.466233</td>\n",
       "      <td>0.427509</td>\n",
       "      <td>0.421307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.719300</td>\n",
       "      <td>1.120244</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.467244</td>\n",
       "      <td>0.432564</td>\n",
       "      <td>0.427266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>1.069207</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.460692</td>\n",
       "      <td>0.444485</td>\n",
       "      <td>0.436370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>1.039061</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.482781</td>\n",
       "      <td>0.471679</td>\n",
       "      <td>0.464435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>1.024606</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.473203</td>\n",
       "      <td>0.474558</td>\n",
       "      <td>0.467497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.017669</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.522923</td>\n",
       "      <td>0.511373</td>\n",
       "      <td>0.509501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.996214</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.589793</td>\n",
       "      <td>0.538787</td>\n",
       "      <td>0.551638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.997036</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.602668</td>\n",
       "      <td>0.569986</td>\n",
       "      <td>0.572091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.987812</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.637502</td>\n",
       "      <td>0.579922</td>\n",
       "      <td>0.595852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.998053</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.648766</td>\n",
       "      <td>0.587288</td>\n",
       "      <td>0.603854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.997179</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.663974</td>\n",
       "      <td>0.596306</td>\n",
       "      <td>0.614287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.987775</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.674863</td>\n",
       "      <td>0.613678</td>\n",
       "      <td>0.622547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.998455</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.683480</td>\n",
       "      <td>0.607781</td>\n",
       "      <td>0.625729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>1.028253</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.653026</td>\n",
       "      <td>0.589111</td>\n",
       "      <td>0.604412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>1.011095</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.732271</td>\n",
       "      <td>0.659608</td>\n",
       "      <td>0.678373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>1.019436</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>0.670276</td>\n",
       "      <td>0.697003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>1.010140</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.777926</td>\n",
       "      <td>0.685503</td>\n",
       "      <td>0.709518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>1.019815</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.772839</td>\n",
       "      <td>0.679427</td>\n",
       "      <td>0.704460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>1.033438</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.786922</td>\n",
       "      <td>0.688010</td>\n",
       "      <td>0.713150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>1.034303</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.787215</td>\n",
       "      <td>0.687181</td>\n",
       "      <td>0.715143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>1.028214</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.781906</td>\n",
       "      <td>0.690740</td>\n",
       "      <td>0.715060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>1.034945</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.791496</td>\n",
       "      <td>0.687401</td>\n",
       "      <td>0.715793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>1.034242</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.794873</td>\n",
       "      <td>0.692966</td>\n",
       "      <td>0.719939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>1.033561</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.788776</td>\n",
       "      <td>0.688057</td>\n",
       "      <td>0.714423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>1.033766</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.791617</td>\n",
       "      <td>0.687629</td>\n",
       "      <td>0.715615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:24:43,083] Trial 118 finished with value: 0.7156145737360871 and parameters: {'learning_rate': 0.0004762367229644209, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 119 with params: {'learning_rate': 0.00048357804883733345, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.336200</td>\n",
       "      <td>2.847936</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.072376</td>\n",
       "      <td>0.088198</td>\n",
       "      <td>0.069852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.540300</td>\n",
       "      <td>2.216488</td>\n",
       "      <td>0.539872</td>\n",
       "      <td>0.190707</td>\n",
       "      <td>0.175506</td>\n",
       "      <td>0.158603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.956500</td>\n",
       "      <td>1.762036</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.243979</td>\n",
       "      <td>0.262791</td>\n",
       "      <td>0.237417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.505500</td>\n",
       "      <td>1.480479</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.290492</td>\n",
       "      <td>0.314977</td>\n",
       "      <td>0.289042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.182700</td>\n",
       "      <td>1.297445</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.367541</td>\n",
       "      <td>0.375413</td>\n",
       "      <td>0.354476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.164880</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.391720</td>\n",
       "      <td>0.384994</td>\n",
       "      <td>0.368497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.739700</td>\n",
       "      <td>1.135142</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.473757</td>\n",
       "      <td>0.453046</td>\n",
       "      <td>0.443482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.619900</td>\n",
       "      <td>1.085724</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.472944</td>\n",
       "      <td>0.472275</td>\n",
       "      <td>0.461880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>1.047664</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.480202</td>\n",
       "      <td>0.467530</td>\n",
       "      <td>0.463185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>1.065252</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.488894</td>\n",
       "      <td>0.495697</td>\n",
       "      <td>0.485601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>1.032832</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.547989</td>\n",
       "      <td>0.506601</td>\n",
       "      <td>0.512709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>1.007301</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.581560</td>\n",
       "      <td>0.537009</td>\n",
       "      <td>0.545533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>1.031264</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.602794</td>\n",
       "      <td>0.562022</td>\n",
       "      <td>0.566796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.204700</td>\n",
       "      <td>1.005717</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.627805</td>\n",
       "      <td>0.570255</td>\n",
       "      <td>0.587318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>1.016168</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.615545</td>\n",
       "      <td>0.576058</td>\n",
       "      <td>0.582842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.996214</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>0.605034</td>\n",
       "      <td>0.619743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>1.002439</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.652235</td>\n",
       "      <td>0.594979</td>\n",
       "      <td>0.609178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>1.015331</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.679493</td>\n",
       "      <td>0.606123</td>\n",
       "      <td>0.625470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>1.048076</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.673519</td>\n",
       "      <td>0.612946</td>\n",
       "      <td>0.625586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>1.037384</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>0.649875</td>\n",
       "      <td>0.668673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>1.039753</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.731175</td>\n",
       "      <td>0.657826</td>\n",
       "      <td>0.675836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>1.037676</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.753865</td>\n",
       "      <td>0.690207</td>\n",
       "      <td>0.706347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>1.047934</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.776525</td>\n",
       "      <td>0.689780</td>\n",
       "      <td>0.713217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>1.049062</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.766723</td>\n",
       "      <td>0.685239</td>\n",
       "      <td>0.706159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>1.065632</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.739471</td>\n",
       "      <td>0.664239</td>\n",
       "      <td>0.682844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>1.055162</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.780970</td>\n",
       "      <td>0.687807</td>\n",
       "      <td>0.711432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.062179</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.763739</td>\n",
       "      <td>0.684118</td>\n",
       "      <td>0.704970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>1.065662</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.784337</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>0.714844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>1.063091</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.782884</td>\n",
       "      <td>0.690851</td>\n",
       "      <td>0.713956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>1.063663</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.782981</td>\n",
       "      <td>0.690851</td>\n",
       "      <td>0.714200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:27:32,059] Trial 119 finished with value: 0.7142002441346605 and parameters: {'learning_rate': 0.00048357804883733345, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 120 with params: {'learning_rate': 0.00020311718523518122, 'weight_decay': 0.005, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:50 < 00:55, 6.34 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.582400</td>\n",
       "      <td>3.295966</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.123600</td>\n",
       "      <td>2.916826</td>\n",
       "      <td>0.405133</td>\n",
       "      <td>0.050599</td>\n",
       "      <td>0.087194</td>\n",
       "      <td>0.061937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.778700</td>\n",
       "      <td>2.573864</td>\n",
       "      <td>0.456462</td>\n",
       "      <td>0.083758</td>\n",
       "      <td>0.113326</td>\n",
       "      <td>0.080349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.432800</td>\n",
       "      <td>2.276532</td>\n",
       "      <td>0.528873</td>\n",
       "      <td>0.192056</td>\n",
       "      <td>0.168112</td>\n",
       "      <td>0.149927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.164500</td>\n",
       "      <td>2.034426</td>\n",
       "      <td>0.587534</td>\n",
       "      <td>0.266761</td>\n",
       "      <td>0.219413</td>\n",
       "      <td>0.201701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.891300</td>\n",
       "      <td>1.832912</td>\n",
       "      <td>0.637030</td>\n",
       "      <td>0.295896</td>\n",
       "      <td>0.257241</td>\n",
       "      <td>0.245832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.668100</td>\n",
       "      <td>1.677978</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.326011</td>\n",
       "      <td>0.301302</td>\n",
       "      <td>0.285160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.499100</td>\n",
       "      <td>1.548382</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.347711</td>\n",
       "      <td>0.328166</td>\n",
       "      <td>0.310006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.336300</td>\n",
       "      <td>1.462773</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.346510</td>\n",
       "      <td>0.357031</td>\n",
       "      <td>0.335524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.196100</td>\n",
       "      <td>1.382344</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.372149</td>\n",
       "      <td>0.375584</td>\n",
       "      <td>0.355335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.103900</td>\n",
       "      <td>1.321860</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.413744</td>\n",
       "      <td>0.390948</td>\n",
       "      <td>0.375430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>1.273698</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.392022</td>\n",
       "      <td>0.381578</td>\n",
       "      <td>0.367782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>1.238129</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.423011</td>\n",
       "      <td>0.406708</td>\n",
       "      <td>0.391487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.843600</td>\n",
       "      <td>1.197214</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.437514</td>\n",
       "      <td>0.421132</td>\n",
       "      <td>0.412383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>1.180081</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.437836</td>\n",
       "      <td>0.428471</td>\n",
       "      <td>0.420033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>1.168968</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.420659</td>\n",
       "      <td>0.426757</td>\n",
       "      <td>0.408661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>1.150724</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.446845</td>\n",
       "      <td>0.438100</td>\n",
       "      <td>0.427989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>1.113612</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.435439</td>\n",
       "      <td>0.427416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>1.108772</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.474613</td>\n",
       "      <td>0.454924</td>\n",
       "      <td>0.453218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>1.092900</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.487796</td>\n",
       "      <td>0.457544</td>\n",
       "      <td>0.455261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:29:25,526] Trial 120 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 121 with params: {'learning_rate': 0.0001384093567788085, 'weight_decay': 0.001, 'adam_beta1': 0.99, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:46, 6.55 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.660500</td>\n",
       "      <td>3.447104</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.322200</td>\n",
       "      <td>3.169027</td>\n",
       "      <td>0.256645</td>\n",
       "      <td>0.074824</td>\n",
       "      <td>0.042754</td>\n",
       "      <td>0.034949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.086900</td>\n",
       "      <td>2.951412</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.055639</td>\n",
       "      <td>0.084879</td>\n",
       "      <td>0.063586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.870200</td>\n",
       "      <td>2.759159</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.063868</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>0.073193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.711400</td>\n",
       "      <td>2.573675</td>\n",
       "      <td>0.445463</td>\n",
       "      <td>0.061975</td>\n",
       "      <td>0.107626</td>\n",
       "      <td>0.077057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>2.414034</td>\n",
       "      <td>0.461962</td>\n",
       "      <td>0.099280</td>\n",
       "      <td>0.121264</td>\n",
       "      <td>0.094104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.347600</td>\n",
       "      <td>2.282103</td>\n",
       "      <td>0.498625</td>\n",
       "      <td>0.201791</td>\n",
       "      <td>0.153044</td>\n",
       "      <td>0.133895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.216800</td>\n",
       "      <td>2.148837</td>\n",
       "      <td>0.509624</td>\n",
       "      <td>0.199889</td>\n",
       "      <td>0.161631</td>\n",
       "      <td>0.144988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.067000</td>\n",
       "      <td>2.037517</td>\n",
       "      <td>0.580202</td>\n",
       "      <td>0.214483</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>0.186689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.938100</td>\n",
       "      <td>1.930723</td>\n",
       "      <td>0.589368</td>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.200296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:30:19,702] Trial 121 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 122 with params: {'learning_rate': 8.637271504022808e-05, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.54 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.724800</td>\n",
       "      <td>3.560950</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.465100</td>\n",
       "      <td>3.329740</td>\n",
       "      <td>0.196150</td>\n",
       "      <td>0.063607</td>\n",
       "      <td>0.025753</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.259800</td>\n",
       "      <td>3.124934</td>\n",
       "      <td>0.382218</td>\n",
       "      <td>0.060724</td>\n",
       "      <td>0.080583</td>\n",
       "      <td>0.063089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.058200</td>\n",
       "      <td>2.936909</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.072398</td>\n",
       "      <td>0.092616</td>\n",
       "      <td>0.068923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.904200</td>\n",
       "      <td>2.767869</td>\n",
       "      <td>0.437214</td>\n",
       "      <td>0.063287</td>\n",
       "      <td>0.103901</td>\n",
       "      <td>0.076735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.722700</td>\n",
       "      <td>2.609559</td>\n",
       "      <td>0.461962</td>\n",
       "      <td>0.103439</td>\n",
       "      <td>0.117341</td>\n",
       "      <td>0.089735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.564600</td>\n",
       "      <td>2.470390</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.120572</td>\n",
       "      <td>0.128088</td>\n",
       "      <td>0.103630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.442900</td>\n",
       "      <td>2.348713</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.203945</td>\n",
       "      <td>0.171295</td>\n",
       "      <td>0.158182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.310300</td>\n",
       "      <td>2.240109</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.225650</td>\n",
       "      <td>0.196514</td>\n",
       "      <td>0.183231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.190600</td>\n",
       "      <td>2.145131</td>\n",
       "      <td>0.590284</td>\n",
       "      <td>0.286602</td>\n",
       "      <td>0.218015</td>\n",
       "      <td>0.206209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:31:13,901] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 123 with params: {'learning_rate': 0.0003058278334151859, 'weight_decay': 0.0, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:58 < 00:59, 5.87 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.474200</td>\n",
       "      <td>3.098110</td>\n",
       "      <td>0.337305</td>\n",
       "      <td>0.067361</td>\n",
       "      <td>0.067793</td>\n",
       "      <td>0.051943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.853500</td>\n",
       "      <td>2.563045</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.082024</td>\n",
       "      <td>0.114333</td>\n",
       "      <td>0.084644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.372400</td>\n",
       "      <td>2.144683</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.184655</td>\n",
       "      <td>0.171495</td>\n",
       "      <td>0.151198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.958700</td>\n",
       "      <td>1.834716</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.271214</td>\n",
       "      <td>0.249753</td>\n",
       "      <td>0.232462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.642200</td>\n",
       "      <td>1.593393</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.327305</td>\n",
       "      <td>0.330619</td>\n",
       "      <td>0.309725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>1.405730</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.338661</td>\n",
       "      <td>0.347052</td>\n",
       "      <td>0.327821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.132200</td>\n",
       "      <td>1.305800</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.385206</td>\n",
       "      <td>0.370141</td>\n",
       "      <td>0.356880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>1.243044</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.423162</td>\n",
       "      <td>0.410122</td>\n",
       "      <td>0.390665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.850200</td>\n",
       "      <td>1.180094</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.448885</td>\n",
       "      <td>0.419265</td>\n",
       "      <td>0.410102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>1.142304</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.454672</td>\n",
       "      <td>0.426608</td>\n",
       "      <td>0.421639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>1.110994</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.498907</td>\n",
       "      <td>0.457391</td>\n",
       "      <td>0.459633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.565300</td>\n",
       "      <td>1.074665</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.499555</td>\n",
       "      <td>0.470755</td>\n",
       "      <td>0.470418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>1.065463</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.490550</td>\n",
       "      <td>0.481945</td>\n",
       "      <td>0.475696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>1.015424</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.499265</td>\n",
       "      <td>0.482060</td>\n",
       "      <td>0.481108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>1.029706</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.490533</td>\n",
       "      <td>0.493091</td>\n",
       "      <td>0.485783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>1.005986</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.510836</td>\n",
       "      <td>0.499284</td>\n",
       "      <td>0.496321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.536670</td>\n",
       "      <td>0.507615</td>\n",
       "      <td>0.508489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.996625</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.516044</td>\n",
       "      <td>0.513447</td>\n",
       "      <td>0.507841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>1.004244</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.575034</td>\n",
       "      <td>0.540769</td>\n",
       "      <td>0.545880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.985563</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.565919</td>\n",
       "      <td>0.529216</td>\n",
       "      <td>0.535978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:33:13,914] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 124 with params: {'learning_rate': 0.00044950350781444275, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.345500</td>\n",
       "      <td>2.846181</td>\n",
       "      <td>0.414299</td>\n",
       "      <td>0.070087</td>\n",
       "      <td>0.095752</td>\n",
       "      <td>0.074678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.535700</td>\n",
       "      <td>2.192880</td>\n",
       "      <td>0.551787</td>\n",
       "      <td>0.173450</td>\n",
       "      <td>0.188871</td>\n",
       "      <td>0.168621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.936800</td>\n",
       "      <td>1.728170</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.265671</td>\n",
       "      <td>0.271006</td>\n",
       "      <td>0.253181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.491100</td>\n",
       "      <td>1.468371</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.301552</td>\n",
       "      <td>0.332898</td>\n",
       "      <td>0.300722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.174200</td>\n",
       "      <td>1.304418</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.328344</td>\n",
       "      <td>0.370369</td>\n",
       "      <td>0.338765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.941500</td>\n",
       "      <td>1.177941</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.463397</td>\n",
       "      <td>0.404121</td>\n",
       "      <td>0.399527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>1.120845</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.431922</td>\n",
       "      <td>0.428056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>1.075808</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.459834</td>\n",
       "      <td>0.451870</td>\n",
       "      <td>0.444735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>1.050558</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.485525</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.466278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>1.017040</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.472758</td>\n",
       "      <td>0.464767</td>\n",
       "      <td>0.459147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>1.004770</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.529091</td>\n",
       "      <td>0.503324</td>\n",
       "      <td>0.504620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.993060</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.567002</td>\n",
       "      <td>0.522931</td>\n",
       "      <td>0.529622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.992263</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.608577</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>0.569493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.984253</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.625470</td>\n",
       "      <td>0.562497</td>\n",
       "      <td>0.576241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.992719</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.652713</td>\n",
       "      <td>0.573340</td>\n",
       "      <td>0.591833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.990002</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.614660</td>\n",
       "      <td>0.565301</td>\n",
       "      <td>0.572628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.622716</td>\n",
       "      <td>0.584410</td>\n",
       "      <td>0.588153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.990468</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.640194</td>\n",
       "      <td>0.579621</td>\n",
       "      <td>0.593941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>1.008554</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.584378</td>\n",
       "      <td>0.604274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>1.002068</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.715434</td>\n",
       "      <td>0.645373</td>\n",
       "      <td>0.662779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>1.018009</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.712787</td>\n",
       "      <td>0.641773</td>\n",
       "      <td>0.660916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.760188</td>\n",
       "      <td>0.665103</td>\n",
       "      <td>0.689711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>1.004466</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.730508</td>\n",
       "      <td>0.661705</td>\n",
       "      <td>0.677493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>1.009689</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.740770</td>\n",
       "      <td>0.661228</td>\n",
       "      <td>0.680525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.008169</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.742229</td>\n",
       "      <td>0.662188</td>\n",
       "      <td>0.682184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>1.010322</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.762845</td>\n",
       "      <td>0.670466</td>\n",
       "      <td>0.693672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>1.013858</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.774201</td>\n",
       "      <td>0.677639</td>\n",
       "      <td>0.703449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1.013872</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.764686</td>\n",
       "      <td>0.671010</td>\n",
       "      <td>0.694634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>1.013110</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.770228</td>\n",
       "      <td>0.681984</td>\n",
       "      <td>0.703257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>1.013755</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.770430</td>\n",
       "      <td>0.680555</td>\n",
       "      <td>0.702615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:36:01,162] Trial 124 finished with value: 0.7026151622943182 and parameters: {'learning_rate': 0.00044950350781444275, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 125 with params: {'learning_rate': 0.0002808306023385571, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:57 < 01:56, 6.01 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.500900</td>\n",
       "      <td>3.151452</td>\n",
       "      <td>0.274977</td>\n",
       "      <td>0.071977</td>\n",
       "      <td>0.047888</td>\n",
       "      <td>0.036941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.928300</td>\n",
       "      <td>2.663586</td>\n",
       "      <td>0.439963</td>\n",
       "      <td>0.063078</td>\n",
       "      <td>0.107863</td>\n",
       "      <td>0.077547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.487500</td>\n",
       "      <td>2.264583</td>\n",
       "      <td>0.534372</td>\n",
       "      <td>0.184451</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.154229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.080600</td>\n",
       "      <td>1.938045</td>\n",
       "      <td>0.601283</td>\n",
       "      <td>0.237480</td>\n",
       "      <td>0.230929</td>\n",
       "      <td>0.208023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.762600</td>\n",
       "      <td>1.689424</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.321757</td>\n",
       "      <td>0.304361</td>\n",
       "      <td>0.285483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.475900</td>\n",
       "      <td>1.491277</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.333017</td>\n",
       "      <td>0.340299</td>\n",
       "      <td>0.318528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.240300</td>\n",
       "      <td>1.371252</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.360124</td>\n",
       "      <td>0.346606</td>\n",
       "      <td>0.329013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.085900</td>\n",
       "      <td>1.293081</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.422767</td>\n",
       "      <td>0.411116</td>\n",
       "      <td>0.391425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>1.227282</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.400978</td>\n",
       "      <td>0.391538</td>\n",
       "      <td>0.375856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>1.181811</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.427989</td>\n",
       "      <td>0.412392</td>\n",
       "      <td>0.400039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:37:00,118] Trial 125 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 126 with params: {'learning_rate': 0.0003066437425122062, 'weight_decay': 0.002, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.53 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.479100</td>\n",
       "      <td>3.117218</td>\n",
       "      <td>0.298808</td>\n",
       "      <td>0.070994</td>\n",
       "      <td>0.054165</td>\n",
       "      <td>0.039407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.888400</td>\n",
       "      <td>2.625527</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.062039</td>\n",
       "      <td>0.108939</td>\n",
       "      <td>0.077255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.445300</td>\n",
       "      <td>2.229374</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.202925</td>\n",
       "      <td>0.179315</td>\n",
       "      <td>0.162642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.029700</td>\n",
       "      <td>1.894075</td>\n",
       "      <td>0.599450</td>\n",
       "      <td>0.263966</td>\n",
       "      <td>0.233693</td>\n",
       "      <td>0.211562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.703600</td>\n",
       "      <td>1.645789</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.312197</td>\n",
       "      <td>0.317941</td>\n",
       "      <td>0.289639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.416100</td>\n",
       "      <td>1.454179</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.346966</td>\n",
       "      <td>0.355309</td>\n",
       "      <td>0.334149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.179400</td>\n",
       "      <td>1.340920</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.393220</td>\n",
       "      <td>0.360640</td>\n",
       "      <td>0.343259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.026200</td>\n",
       "      <td>1.256390</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.431712</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.387637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.892300</td>\n",
       "      <td>1.211104</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.396840</td>\n",
       "      <td>0.375625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.772300</td>\n",
       "      <td>1.162590</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.468125</td>\n",
       "      <td>0.421732</td>\n",
       "      <td>0.412672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>1.124585</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.485775</td>\n",
       "      <td>0.448602</td>\n",
       "      <td>0.447019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>1.091188</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.493985</td>\n",
       "      <td>0.447690</td>\n",
       "      <td>0.448371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>1.078586</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.511742</td>\n",
       "      <td>0.484060</td>\n",
       "      <td>0.483417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>1.040939</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.495926</td>\n",
       "      <td>0.479140</td>\n",
       "      <td>0.474837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.439100</td>\n",
       "      <td>1.038530</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.487197</td>\n",
       "      <td>0.482848</td>\n",
       "      <td>0.478070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>1.030631</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.493411</td>\n",
       "      <td>0.490329</td>\n",
       "      <td>0.486164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>1.019555</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.508878</td>\n",
       "      <td>0.493833</td>\n",
       "      <td>0.489818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>1.015422</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.489864</td>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.489711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>1.019487</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.504779</td>\n",
       "      <td>0.500079</td>\n",
       "      <td>0.497623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>1.003938</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.581211</td>\n",
       "      <td>0.527418</td>\n",
       "      <td>0.536343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:38:47,982] Trial 126 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 127 with params: {'learning_rate': 0.00022812103953476773, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:52, 6.61 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.584700</td>\n",
       "      <td>3.250943</td>\n",
       "      <td>0.203483</td>\n",
       "      <td>0.038648</td>\n",
       "      <td>0.027515</td>\n",
       "      <td>0.018527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.036400</td>\n",
       "      <td>2.769402</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.060698</td>\n",
       "      <td>0.102545</td>\n",
       "      <td>0.074101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.367241</td>\n",
       "      <td>0.505958</td>\n",
       "      <td>0.138965</td>\n",
       "      <td>0.144059</td>\n",
       "      <td>0.122543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.216700</td>\n",
       "      <td>2.053342</td>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.247164</td>\n",
       "      <td>0.208946</td>\n",
       "      <td>0.193259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.918700</td>\n",
       "      <td>1.806792</td>\n",
       "      <td>0.673694</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>0.297278</td>\n",
       "      <td>0.289745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.634500</td>\n",
       "      <td>1.608154</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.374791</td>\n",
       "      <td>0.320615</td>\n",
       "      <td>0.312727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.406900</td>\n",
       "      <td>1.470955</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.329822</td>\n",
       "      <td>0.330010</td>\n",
       "      <td>0.315205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.248900</td>\n",
       "      <td>1.381845</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.368030</td>\n",
       "      <td>0.382543</td>\n",
       "      <td>0.360201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.101600</td>\n",
       "      <td>1.302592</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.364099</td>\n",
       "      <td>0.380476</td>\n",
       "      <td>0.363449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.972400</td>\n",
       "      <td>1.248597</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.424234</td>\n",
       "      <td>0.405285</td>\n",
       "      <td>0.396049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>1.209060</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.450126</td>\n",
       "      <td>0.415223</td>\n",
       "      <td>0.409631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>1.180553</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.419269</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.406729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>1.141669</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.479648</td>\n",
       "      <td>0.433928</td>\n",
       "      <td>0.432221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>1.112104</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.477226</td>\n",
       "      <td>0.454649</td>\n",
       "      <td>0.452044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>1.097874</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.490564</td>\n",
       "      <td>0.470284</td>\n",
       "      <td>0.468461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>1.093752</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.484532</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>0.464412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>1.074448</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.489260</td>\n",
       "      <td>0.482228</td>\n",
       "      <td>0.478051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>1.063536</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.490396</td>\n",
       "      <td>0.484473</td>\n",
       "      <td>0.479098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>1.058389</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.496286</td>\n",
       "      <td>0.497572</td>\n",
       "      <td>0.490898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>1.038929</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.505992</td>\n",
       "      <td>0.490255</td>\n",
       "      <td>0.487967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:40:34,961] Trial 127 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 128 with params: {'learning_rate': 0.0004920368163252596, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.326600</td>\n",
       "      <td>2.824339</td>\n",
       "      <td>0.405133</td>\n",
       "      <td>0.068196</td>\n",
       "      <td>0.092546</td>\n",
       "      <td>0.071908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.509200</td>\n",
       "      <td>2.176996</td>\n",
       "      <td>0.556370</td>\n",
       "      <td>0.196124</td>\n",
       "      <td>0.192885</td>\n",
       "      <td>0.173585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.906400</td>\n",
       "      <td>1.705657</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.250341</td>\n",
       "      <td>0.264807</td>\n",
       "      <td>0.244108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.454600</td>\n",
       "      <td>1.438911</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.287662</td>\n",
       "      <td>0.319191</td>\n",
       "      <td>0.289156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.133000</td>\n",
       "      <td>1.270812</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.362746</td>\n",
       "      <td>0.372730</td>\n",
       "      <td>0.351040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>1.148274</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.439766</td>\n",
       "      <td>0.392617</td>\n",
       "      <td>0.382844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>1.115926</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.456206</td>\n",
       "      <td>0.427618</td>\n",
       "      <td>0.423715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>1.069764</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.485806</td>\n",
       "      <td>0.483412</td>\n",
       "      <td>0.475943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>1.042754</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.474712</td>\n",
       "      <td>0.470958</td>\n",
       "      <td>0.463316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>1.032361</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.508365</td>\n",
       "      <td>0.507088</td>\n",
       "      <td>0.500936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.328600</td>\n",
       "      <td>1.032537</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.554080</td>\n",
       "      <td>0.528046</td>\n",
       "      <td>0.527599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>1.000707</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.600141</td>\n",
       "      <td>0.542434</td>\n",
       "      <td>0.556111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>1.019269</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.579580</td>\n",
       "      <td>0.562387</td>\n",
       "      <td>0.557751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.631529</td>\n",
       "      <td>0.571824</td>\n",
       "      <td>0.588844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>1.017291</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.636288</td>\n",
       "      <td>0.583043</td>\n",
       "      <td>0.594282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>1.013789</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.632874</td>\n",
       "      <td>0.591276</td>\n",
       "      <td>0.597712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>1.008603</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.654822</td>\n",
       "      <td>0.608481</td>\n",
       "      <td>0.615243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>1.019186</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.676961</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.635499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>1.043934</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.743695</td>\n",
       "      <td>0.645424</td>\n",
       "      <td>0.669512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>1.032905</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.725178</td>\n",
       "      <td>0.672074</td>\n",
       "      <td>0.681758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>1.046855</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.740745</td>\n",
       "      <td>0.675495</td>\n",
       "      <td>0.691756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.043920</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.762794</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>0.708280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>1.058249</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.774156</td>\n",
       "      <td>0.686739</td>\n",
       "      <td>0.709958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>1.064635</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.788110</td>\n",
       "      <td>0.689398</td>\n",
       "      <td>0.714240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>1.069056</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.787824</td>\n",
       "      <td>0.690299</td>\n",
       "      <td>0.716829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>1.060211</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.767118</td>\n",
       "      <td>0.683615</td>\n",
       "      <td>0.706814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.065795</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.685178</td>\n",
       "      <td>0.711689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.068992</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785268</td>\n",
       "      <td>0.684391</td>\n",
       "      <td>0.710881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>1.065546</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.767955</td>\n",
       "      <td>0.684174</td>\n",
       "      <td>0.706717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>1.065781</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.778372</td>\n",
       "      <td>0.686674</td>\n",
       "      <td>0.710076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:43:25,766] Trial 128 finished with value: 0.7100760490936798 and parameters: {'learning_rate': 0.0004920368163252596, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 129 with params: {'learning_rate': 0.0001297138581409507, 'weight_decay': 0.0, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:05, 6.95 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.686900</td>\n",
       "      <td>3.472370</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.336500</td>\n",
       "      <td>3.159709</td>\n",
       "      <td>0.345555</td>\n",
       "      <td>0.064974</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.054466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.061500</td>\n",
       "      <td>2.891986</td>\n",
       "      <td>0.419798</td>\n",
       "      <td>0.070418</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.071550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.793900</td>\n",
       "      <td>2.645740</td>\n",
       "      <td>0.458295</td>\n",
       "      <td>0.098970</td>\n",
       "      <td>0.115098</td>\n",
       "      <td>0.087404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.581200</td>\n",
       "      <td>2.423324</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.172279</td>\n",
       "      <td>0.148422</td>\n",
       "      <td>0.127996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:43:51,587] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 130 with params: {'learning_rate': 0.00038155303056576956, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:45, 6.63 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.405500</td>\n",
       "      <td>2.969987</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.056837</td>\n",
       "      <td>0.082120</td>\n",
       "      <td>0.061572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.689200</td>\n",
       "      <td>2.376675</td>\n",
       "      <td>0.500458</td>\n",
       "      <td>0.182310</td>\n",
       "      <td>0.142770</td>\n",
       "      <td>0.122801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.146400</td>\n",
       "      <td>1.928937</td>\n",
       "      <td>0.582951</td>\n",
       "      <td>0.252266</td>\n",
       "      <td>0.210268</td>\n",
       "      <td>0.196601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.705100</td>\n",
       "      <td>1.627403</td>\n",
       "      <td>0.678277</td>\n",
       "      <td>0.300107</td>\n",
       "      <td>0.301766</td>\n",
       "      <td>0.283442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.386400</td>\n",
       "      <td>1.413743</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.358401</td>\n",
       "      <td>0.339841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.125500</td>\n",
       "      <td>1.264238</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>0.380750</td>\n",
       "      <td>0.369388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.195619</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.437789</td>\n",
       "      <td>0.404065</td>\n",
       "      <td>0.396877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>1.135765</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.467261</td>\n",
       "      <td>0.441639</td>\n",
       "      <td>0.431958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.652600</td>\n",
       "      <td>1.088737</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.494213</td>\n",
       "      <td>0.476922</td>\n",
       "      <td>0.471852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>1.079813</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.481627</td>\n",
       "      <td>0.462860</td>\n",
       "      <td>0.459946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:44:45,258] Trial 130 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 131 with params: {'learning_rate': 0.0004901839825889945, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.327700</td>\n",
       "      <td>2.826270</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.092182</td>\n",
       "      <td>0.071653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.511700</td>\n",
       "      <td>2.179556</td>\n",
       "      <td>0.555454</td>\n",
       "      <td>0.195885</td>\n",
       "      <td>0.192397</td>\n",
       "      <td>0.173001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.909900</td>\n",
       "      <td>1.708843</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.249503</td>\n",
       "      <td>0.263855</td>\n",
       "      <td>0.243243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.458000</td>\n",
       "      <td>1.440946</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.288348</td>\n",
       "      <td>0.319769</td>\n",
       "      <td>0.289793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.136200</td>\n",
       "      <td>1.272527</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.351782</td>\n",
       "      <td>0.371301</td>\n",
       "      <td>0.346210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>1.149060</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.447385</td>\n",
       "      <td>0.396617</td>\n",
       "      <td>0.387967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>1.116704</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.456839</td>\n",
       "      <td>0.427618</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>1.070194</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.473412</td>\n",
       "      <td>0.462493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>1.042105</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.474712</td>\n",
       "      <td>0.470958</td>\n",
       "      <td>0.463316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>1.032462</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.508847</td>\n",
       "      <td>0.507542</td>\n",
       "      <td>0.501802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>1.031965</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>0.527969</td>\n",
       "      <td>0.528274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>1.000172</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.596360</td>\n",
       "      <td>0.541842</td>\n",
       "      <td>0.555873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>1.019265</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.578773</td>\n",
       "      <td>0.561233</td>\n",
       "      <td>0.556755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.991630</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.632749</td>\n",
       "      <td>0.571973</td>\n",
       "      <td>0.589692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>1.014528</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.631910</td>\n",
       "      <td>0.571430</td>\n",
       "      <td>0.584955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>1.013518</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.588434</td>\n",
       "      <td>0.594239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>1.006888</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.661161</td>\n",
       "      <td>0.609066</td>\n",
       "      <td>0.619293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>1.016962</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.677698</td>\n",
       "      <td>0.633042</td>\n",
       "      <td>0.636347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>1.047766</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.717514</td>\n",
       "      <td>0.638954</td>\n",
       "      <td>0.656516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>1.037338</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.720592</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.670665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>1.049079</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.750318</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.697741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>1.046629</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.760733</td>\n",
       "      <td>0.687042</td>\n",
       "      <td>0.705560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>1.057930</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.773853</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.709835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.065904</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.787131</td>\n",
       "      <td>0.687680</td>\n",
       "      <td>0.712880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>1.069283</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.786923</td>\n",
       "      <td>0.688711</td>\n",
       "      <td>0.715617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>1.060111</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.767556</td>\n",
       "      <td>0.684341</td>\n",
       "      <td>0.707351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>1.065722</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.786511</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.712401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.068433</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.788727</td>\n",
       "      <td>0.689819</td>\n",
       "      <td>0.715482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>1.065172</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.769643</td>\n",
       "      <td>0.686296</td>\n",
       "      <td>0.709268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>1.065622</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.769643</td>\n",
       "      <td>0.686296</td>\n",
       "      <td>0.709268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:47:25,688] Trial 131 finished with value: 0.7092676994101437 and parameters: {'learning_rate': 0.0004901839825889945, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132 with params: {'learning_rate': 0.0003008083994533252, 'weight_decay': 0.0, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:59 < 01:00, 5.82 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.484600</td>\n",
       "      <td>3.126962</td>\n",
       "      <td>0.292392</td>\n",
       "      <td>0.071148</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>0.038219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.901400</td>\n",
       "      <td>2.641612</td>\n",
       "      <td>0.441797</td>\n",
       "      <td>0.062556</td>\n",
       "      <td>0.108294</td>\n",
       "      <td>0.077395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.463100</td>\n",
       "      <td>2.246101</td>\n",
       "      <td>0.541705</td>\n",
       "      <td>0.203590</td>\n",
       "      <td>0.179960</td>\n",
       "      <td>0.163168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.049500</td>\n",
       "      <td>1.911590</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.240169</td>\n",
       "      <td>0.225112</td>\n",
       "      <td>0.200904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.725900</td>\n",
       "      <td>1.664008</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.311228</td>\n",
       "      <td>0.313814</td>\n",
       "      <td>0.285513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.438500</td>\n",
       "      <td>1.470469</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.329517</td>\n",
       "      <td>0.341486</td>\n",
       "      <td>0.317690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.201400</td>\n",
       "      <td>1.354151</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.386183</td>\n",
       "      <td>0.356309</td>\n",
       "      <td>0.337903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>1.267196</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.431876</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.387856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>1.220213</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.389604</td>\n",
       "      <td>0.390515</td>\n",
       "      <td>0.369909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.790100</td>\n",
       "      <td>1.170942</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.472617</td>\n",
       "      <td>0.421381</td>\n",
       "      <td>0.414038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>1.132152</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.488403</td>\n",
       "      <td>0.441789</td>\n",
       "      <td>0.441964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>1.095979</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.485426</td>\n",
       "      <td>0.449114</td>\n",
       "      <td>0.449968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>1.083007</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.507456</td>\n",
       "      <td>0.474691</td>\n",
       "      <td>0.475181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>1.046495</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.501040</td>\n",
       "      <td>0.481047</td>\n",
       "      <td>0.478252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>1.041736</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.491007</td>\n",
       "      <td>0.483019</td>\n",
       "      <td>0.480269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>1.035138</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.490032</td>\n",
       "      <td>0.482702</td>\n",
       "      <td>0.480203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>1.025269</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.491185</td>\n",
       "      <td>0.493278</td>\n",
       "      <td>0.487549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>1.018611</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.488835</td>\n",
       "      <td>0.495848</td>\n",
       "      <td>0.488403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>1.022185</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.504978</td>\n",
       "      <td>0.501518</td>\n",
       "      <td>0.498376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>1.006889</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.579630</td>\n",
       "      <td>0.520077</td>\n",
       "      <td>0.527033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:49:26,534] Trial 132 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 133 with params: {'learning_rate': 0.00048359108302686323, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.336200</td>\n",
       "      <td>2.847959</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.072376</td>\n",
       "      <td>0.088198</td>\n",
       "      <td>0.069852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.540300</td>\n",
       "      <td>2.216562</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.190729</td>\n",
       "      <td>0.178006</td>\n",
       "      <td>0.160386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.956500</td>\n",
       "      <td>1.762216</td>\n",
       "      <td>0.632447</td>\n",
       "      <td>0.244820</td>\n",
       "      <td>0.263436</td>\n",
       "      <td>0.238085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.505500</td>\n",
       "      <td>1.480443</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.291022</td>\n",
       "      <td>0.316516</td>\n",
       "      <td>0.290558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.182600</td>\n",
       "      <td>1.297512</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.367645</td>\n",
       "      <td>0.375413</td>\n",
       "      <td>0.354457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>1.164794</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.391720</td>\n",
       "      <td>0.384994</td>\n",
       "      <td>0.368497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.739700</td>\n",
       "      <td>1.134906</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.473757</td>\n",
       "      <td>0.453046</td>\n",
       "      <td>0.443482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.619800</td>\n",
       "      <td>1.085670</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.472944</td>\n",
       "      <td>0.472275</td>\n",
       "      <td>0.461880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>1.047502</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.479673</td>\n",
       "      <td>0.467530</td>\n",
       "      <td>0.462840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>1.065021</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.487943</td>\n",
       "      <td>0.495423</td>\n",
       "      <td>0.484944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>1.032691</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.507056</td>\n",
       "      <td>0.513050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.293900</td>\n",
       "      <td>1.007209</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.581560</td>\n",
       "      <td>0.537009</td>\n",
       "      <td>0.545533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.242300</td>\n",
       "      <td>1.030761</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.604479</td>\n",
       "      <td>0.562022</td>\n",
       "      <td>0.567507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>1.005687</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.628055</td>\n",
       "      <td>0.570255</td>\n",
       "      <td>0.587455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>1.015433</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.614656</td>\n",
       "      <td>0.575005</td>\n",
       "      <td>0.581547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.670258</td>\n",
       "      <td>0.605034</td>\n",
       "      <td>0.618635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>1.001791</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.652235</td>\n",
       "      <td>0.594979</td>\n",
       "      <td>0.609178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>1.015077</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.679493</td>\n",
       "      <td>0.606123</td>\n",
       "      <td>0.625470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>1.049591</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.673878</td>\n",
       "      <td>0.613898</td>\n",
       "      <td>0.626393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>1.038195</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>0.649875</td>\n",
       "      <td>0.668673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>1.040627</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.729763</td>\n",
       "      <td>0.656186</td>\n",
       "      <td>0.675096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>1.038541</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.752802</td>\n",
       "      <td>0.694662</td>\n",
       "      <td>0.707611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>1.047408</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.755493</td>\n",
       "      <td>0.682626</td>\n",
       "      <td>0.703055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>1.050107</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.768111</td>\n",
       "      <td>0.684470</td>\n",
       "      <td>0.706322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>1.067620</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.738354</td>\n",
       "      <td>0.664374</td>\n",
       "      <td>0.682385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>1.056382</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.781218</td>\n",
       "      <td>0.687658</td>\n",
       "      <td>0.711605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>1.063040</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.763894</td>\n",
       "      <td>0.681833</td>\n",
       "      <td>0.703597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>1.066418</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.762286</td>\n",
       "      <td>0.684036</td>\n",
       "      <td>0.704254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>1.063931</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783212</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>1.064589</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.782396</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.713982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:52:04,966] Trial 133 finished with value: 0.7139824172173519 and parameters: {'learning_rate': 0.00048359108302686323, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 134 with params: {'learning_rate': 0.0004148912984969679, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.421100</td>\n",
       "      <td>2.980070</td>\n",
       "      <td>0.366636</td>\n",
       "      <td>0.062774</td>\n",
       "      <td>0.075747</td>\n",
       "      <td>0.058925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.690800</td>\n",
       "      <td>2.381382</td>\n",
       "      <td>0.472961</td>\n",
       "      <td>0.141099</td>\n",
       "      <td>0.126538</td>\n",
       "      <td>0.099816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.142600</td>\n",
       "      <td>1.930989</td>\n",
       "      <td>0.586618</td>\n",
       "      <td>0.252259</td>\n",
       "      <td>0.212814</td>\n",
       "      <td>0.194065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.691700</td>\n",
       "      <td>1.619503</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.312734</td>\n",
       "      <td>0.314054</td>\n",
       "      <td>0.286641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.361900</td>\n",
       "      <td>1.418070</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.342508</td>\n",
       "      <td>0.347083</td>\n",
       "      <td>0.321501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.094800</td>\n",
       "      <td>1.251088</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.392734</td>\n",
       "      <td>0.386020</td>\n",
       "      <td>0.368869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>1.182856</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.428880</td>\n",
       "      <td>0.408947</td>\n",
       "      <td>0.397253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>1.124665</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.477103</td>\n",
       "      <td>0.452270</td>\n",
       "      <td>0.445073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>1.081501</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.509686</td>\n",
       "      <td>0.478664</td>\n",
       "      <td>0.478025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>1.070312</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.486382</td>\n",
       "      <td>0.483878</td>\n",
       "      <td>0.476239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>1.036758</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>0.486662</td>\n",
       "      <td>0.489578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.012324</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.539230</td>\n",
       "      <td>0.502955</td>\n",
       "      <td>0.502492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>1.029245</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.559269</td>\n",
       "      <td>0.532683</td>\n",
       "      <td>0.532003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.992625</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.590178</td>\n",
       "      <td>0.536886</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>1.002817</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.583372</td>\n",
       "      <td>0.546913</td>\n",
       "      <td>0.552046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.984791</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.624323</td>\n",
       "      <td>0.572032</td>\n",
       "      <td>0.580975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.977212</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.629534</td>\n",
       "      <td>0.577070</td>\n",
       "      <td>0.585582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.990681</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.577536</td>\n",
       "      <td>0.584151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>1.011685</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.640102</td>\n",
       "      <td>0.590348</td>\n",
       "      <td>0.598044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>1.003522</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.671049</td>\n",
       "      <td>0.601823</td>\n",
       "      <td>0.617846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>1.004142</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.654154</td>\n",
       "      <td>0.589962</td>\n",
       "      <td>0.605851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>1.002360</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.664432</td>\n",
       "      <td>0.604506</td>\n",
       "      <td>0.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.005799</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.678404</td>\n",
       "      <td>0.626765</td>\n",
       "      <td>0.637842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>1.013308</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.680837</td>\n",
       "      <td>0.625129</td>\n",
       "      <td>0.638308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>1.012914</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.704513</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>0.653050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>1.017256</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.709639</td>\n",
       "      <td>0.635195</td>\n",
       "      <td>0.653407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>1.020971</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.715515</td>\n",
       "      <td>0.641749</td>\n",
       "      <td>0.661926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.022835</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.717936</td>\n",
       "      <td>0.649554</td>\n",
       "      <td>0.667735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.020791</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.712738</td>\n",
       "      <td>0.649626</td>\n",
       "      <td>0.665183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>1.021113</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.735276</td>\n",
       "      <td>0.656661</td>\n",
       "      <td>0.677251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:54:44,900] Trial 134 finished with value: 0.6772508813803748 and parameters: {'learning_rate': 0.0004148912984969679, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 135 with params: {'learning_rate': 0.0003168514525961264, 'weight_decay': 0.004, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:51 < 00:55, 6.25 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.469700</td>\n",
       "      <td>3.100386</td>\n",
       "      <td>0.314390</td>\n",
       "      <td>0.070477</td>\n",
       "      <td>0.059199</td>\n",
       "      <td>0.044105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.865800</td>\n",
       "      <td>2.597438</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.101882</td>\n",
       "      <td>0.115027</td>\n",
       "      <td>0.086090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.414500</td>\n",
       "      <td>2.200370</td>\n",
       "      <td>0.541705</td>\n",
       "      <td>0.206145</td>\n",
       "      <td>0.187802</td>\n",
       "      <td>0.169155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.995300</td>\n",
       "      <td>1.864039</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.269157</td>\n",
       "      <td>0.239083</td>\n",
       "      <td>0.217194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.665200</td>\n",
       "      <td>1.615220</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.305414</td>\n",
       "      <td>0.320628</td>\n",
       "      <td>0.291279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.378100</td>\n",
       "      <td>1.426910</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.341324</td>\n",
       "      <td>0.351859</td>\n",
       "      <td>0.329770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.142900</td>\n",
       "      <td>1.320099</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.406370</td>\n",
       "      <td>0.364943</td>\n",
       "      <td>0.350180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>1.238951</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.430049</td>\n",
       "      <td>0.406613</td>\n",
       "      <td>0.390278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.428242</td>\n",
       "      <td>0.410976</td>\n",
       "      <td>0.394332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>1.147944</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.455560</td>\n",
       "      <td>0.429280</td>\n",
       "      <td>0.420079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>1.114824</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.478325</td>\n",
       "      <td>0.446979</td>\n",
       "      <td>0.446054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>1.083356</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.496232</td>\n",
       "      <td>0.449837</td>\n",
       "      <td>0.453506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>1.072739</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.488094</td>\n",
       "      <td>0.474907</td>\n",
       "      <td>0.468781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>1.031992</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.497259</td>\n",
       "      <td>0.481567</td>\n",
       "      <td>0.479930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>1.031909</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.492543</td>\n",
       "      <td>0.484866</td>\n",
       "      <td>0.481309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>1.023314</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.491546</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.485774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>1.012416</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.528553</td>\n",
       "      <td>0.505281</td>\n",
       "      <td>0.503627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>1.010954</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.509882</td>\n",
       "      <td>0.502922</td>\n",
       "      <td>0.497101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>1.016619</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.573907</td>\n",
       "      <td>0.528246</td>\n",
       "      <td>0.537701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>1.002186</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.572320</td>\n",
       "      <td>0.533330</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:56:37,513] Trial 135 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 136 with params: {'learning_rate': 0.0002758057047481744, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:50 < 01:40, 6.94 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.503300</td>\n",
       "      <td>3.151397</td>\n",
       "      <td>0.291476</td>\n",
       "      <td>0.072643</td>\n",
       "      <td>0.053249</td>\n",
       "      <td>0.042615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.924000</td>\n",
       "      <td>2.649382</td>\n",
       "      <td>0.439963</td>\n",
       "      <td>0.062403</td>\n",
       "      <td>0.107863</td>\n",
       "      <td>0.077217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.470800</td>\n",
       "      <td>2.241923</td>\n",
       "      <td>0.532539</td>\n",
       "      <td>0.168412</td>\n",
       "      <td>0.165952</td>\n",
       "      <td>0.147332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.066800</td>\n",
       "      <td>1.926888</td>\n",
       "      <td>0.604033</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.230502</td>\n",
       "      <td>0.210496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.755700</td>\n",
       "      <td>1.680570</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.317829</td>\n",
       "      <td>0.298956</td>\n",
       "      <td>0.281682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.473300</td>\n",
       "      <td>1.486103</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.323044</td>\n",
       "      <td>0.330162</td>\n",
       "      <td>0.312097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.243300</td>\n",
       "      <td>1.371599</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.332140</td>\n",
       "      <td>0.345073</td>\n",
       "      <td>0.325859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.089700</td>\n",
       "      <td>1.293383</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.422519</td>\n",
       "      <td>0.410226</td>\n",
       "      <td>0.390862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>1.231093</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.393230</td>\n",
       "      <td>0.401914</td>\n",
       "      <td>0.384912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>1.180663</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.468451</td>\n",
       "      <td>0.422722</td>\n",
       "      <td>0.417901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:57:28,806] Trial 136 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 137 with params: {'learning_rate': 0.00016880670250329712, 'weight_decay': 0.0, 'adam_beta1': 0.96, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:03, 7.11 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.660000</td>\n",
       "      <td>3.406834</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.246700</td>\n",
       "      <td>3.049823</td>\n",
       "      <td>0.372136</td>\n",
       "      <td>0.060763</td>\n",
       "      <td>0.078560</td>\n",
       "      <td>0.058688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.931700</td>\n",
       "      <td>2.743094</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.102913</td>\n",
       "      <td>0.075822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.622400</td>\n",
       "      <td>2.464241</td>\n",
       "      <td>0.480293</td>\n",
       "      <td>0.104990</td>\n",
       "      <td>0.132183</td>\n",
       "      <td>0.103325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.377200</td>\n",
       "      <td>2.224661</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.221058</td>\n",
       "      <td>0.184113</td>\n",
       "      <td>0.168986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:57:54,225] Trial 137 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 138 with params: {'learning_rate': 0.00043741356248507994, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:45, 6.60 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.402500</td>\n",
       "      <td>2.934013</td>\n",
       "      <td>0.387718</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>0.082773</td>\n",
       "      <td>0.062342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.628700</td>\n",
       "      <td>2.298649</td>\n",
       "      <td>0.518790</td>\n",
       "      <td>0.159594</td>\n",
       "      <td>0.154543</td>\n",
       "      <td>0.134793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.045200</td>\n",
       "      <td>1.834099</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.268467</td>\n",
       "      <td>0.228970</td>\n",
       "      <td>0.212457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.584700</td>\n",
       "      <td>1.532031</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.313521</td>\n",
       "      <td>0.326942</td>\n",
       "      <td>0.296180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.251300</td>\n",
       "      <td>1.344590</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.386303</td>\n",
       "      <td>0.364355</td>\n",
       "      <td>0.343111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>1.193027</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.441921</td>\n",
       "      <td>0.403929</td>\n",
       "      <td>0.397785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.804600</td>\n",
       "      <td>1.165455</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.403465</td>\n",
       "      <td>0.401139</td>\n",
       "      <td>0.388527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>1.103342</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.482339</td>\n",
       "      <td>0.459248</td>\n",
       "      <td>0.456005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>1.061804</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.504846</td>\n",
       "      <td>0.487509</td>\n",
       "      <td>0.482465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>1.061798</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.452179</td>\n",
       "      <td>0.475223</td>\n",
       "      <td>0.458128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:58:48,023] Trial 138 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 139 with params: {'learning_rate': 1.1619982946199614e-06, 'weight_decay': 0.001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:57 < 01:55, 6.05 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.891400</td>\n",
       "      <td>3.877134</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.877600</td>\n",
       "      <td>3.869755</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.876100</td>\n",
       "      <td>3.863133</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.866700</td>\n",
       "      <td>3.857069</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.864700</td>\n",
       "      <td>3.851296</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.003353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.855100</td>\n",
       "      <td>3.846063</td>\n",
       "      <td>0.023831</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.849200</td>\n",
       "      <td>3.841056</td>\n",
       "      <td>0.027498</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.023707</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.846000</td>\n",
       "      <td>3.836272</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.005391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.841800</td>\n",
       "      <td>3.831838</td>\n",
       "      <td>0.038497</td>\n",
       "      <td>0.027817</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>0.006472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.837200</td>\n",
       "      <td>3.827570</td>\n",
       "      <td>0.050412</td>\n",
       "      <td>0.036071</td>\n",
       "      <td>0.027475</td>\n",
       "      <td>0.008323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 13:59:46,426] Trial 139 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 140 with params: {'learning_rate': 0.00034495997412051937, 'weight_decay': 0.003, 'adam_beta1': 0.96, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.55 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.444600</td>\n",
       "      <td>3.055337</td>\n",
       "      <td>0.353804</td>\n",
       "      <td>0.065009</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>0.055845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.805400</td>\n",
       "      <td>2.522484</td>\n",
       "      <td>0.452796</td>\n",
       "      <td>0.094242</td>\n",
       "      <td>0.116413</td>\n",
       "      <td>0.087045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>2.116951</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.197879</td>\n",
       "      <td>0.190976</td>\n",
       "      <td>0.164466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.902700</td>\n",
       "      <td>1.785079</td>\n",
       "      <td>0.626031</td>\n",
       "      <td>0.313759</td>\n",
       "      <td>0.265209</td>\n",
       "      <td>0.240440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.564500</td>\n",
       "      <td>1.541760</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.318718</td>\n",
       "      <td>0.328456</td>\n",
       "      <td>0.301085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.281800</td>\n",
       "      <td>1.363593</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>0.358921</td>\n",
       "      <td>0.337609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.053300</td>\n",
       "      <td>1.275015</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.390659</td>\n",
       "      <td>0.369216</td>\n",
       "      <td>0.353697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>1.199670</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.427520</td>\n",
       "      <td>0.414147</td>\n",
       "      <td>0.399249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>1.153547</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.462232</td>\n",
       "      <td>0.439968</td>\n",
       "      <td>0.426765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>1.122587</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.457817</td>\n",
       "      <td>0.440736</td>\n",
       "      <td>0.429848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>1.091260</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.526495</td>\n",
       "      <td>0.478413</td>\n",
       "      <td>0.483123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>1.061351</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.498711</td>\n",
       "      <td>0.469886</td>\n",
       "      <td>0.469668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>1.053113</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.492910</td>\n",
       "      <td>0.475214</td>\n",
       "      <td>0.470243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>1.014804</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.501483</td>\n",
       "      <td>0.481213</td>\n",
       "      <td>0.480125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>1.018585</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.490906</td>\n",
       "      <td>0.494925</td>\n",
       "      <td>0.487866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.316500</td>\n",
       "      <td>1.007141</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.508178</td>\n",
       "      <td>0.505478</td>\n",
       "      <td>0.501360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.995601</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.550076</td>\n",
       "      <td>0.526343</td>\n",
       "      <td>0.524489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.260700</td>\n",
       "      <td>1.001955</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.568180</td>\n",
       "      <td>0.545672</td>\n",
       "      <td>0.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>1.016821</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.577329</td>\n",
       "      <td>0.542251</td>\n",
       "      <td>0.547818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>1.001493</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.582157</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>0.551908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:01:33,933] Trial 140 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 141 with params: {'learning_rate': 0.0004981796332020285, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:33, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.327500</td>\n",
       "      <td>2.833004</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.070364</td>\n",
       "      <td>0.089058</td>\n",
       "      <td>0.069912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.521000</td>\n",
       "      <td>2.197813</td>\n",
       "      <td>0.544455</td>\n",
       "      <td>0.189935</td>\n",
       "      <td>0.182521</td>\n",
       "      <td>0.164565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.931900</td>\n",
       "      <td>1.740990</td>\n",
       "      <td>0.637030</td>\n",
       "      <td>0.254894</td>\n",
       "      <td>0.269276</td>\n",
       "      <td>0.245524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.479300</td>\n",
       "      <td>1.461324</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.315726</td>\n",
       "      <td>0.324588</td>\n",
       "      <td>0.302047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>1.281775</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.348126</td>\n",
       "      <td>0.370314</td>\n",
       "      <td>0.346289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>1.155271</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.395620</td>\n",
       "      <td>0.385468</td>\n",
       "      <td>0.370935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.714600</td>\n",
       "      <td>1.128517</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.479145</td>\n",
       "      <td>0.438716</td>\n",
       "      <td>0.432861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>1.078983</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.469338</td>\n",
       "      <td>0.466084</td>\n",
       "      <td>0.455578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>1.042630</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.519007</td>\n",
       "      <td>0.484311</td>\n",
       "      <td>0.484605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>1.059707</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.503335</td>\n",
       "      <td>0.497284</td>\n",
       "      <td>0.491072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.552970</td>\n",
       "      <td>0.513020</td>\n",
       "      <td>0.521348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>1.005179</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.593131</td>\n",
       "      <td>0.551022</td>\n",
       "      <td>0.559157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>1.026449</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.610375</td>\n",
       "      <td>0.564873</td>\n",
       "      <td>0.571867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>1.010615</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.633023</td>\n",
       "      <td>0.574407</td>\n",
       "      <td>0.590713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>1.015029</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.613936</td>\n",
       "      <td>0.575334</td>\n",
       "      <td>0.580825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>1.012548</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.647426</td>\n",
       "      <td>0.594615</td>\n",
       "      <td>0.603174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>1.018058</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.667488</td>\n",
       "      <td>0.606405</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>1.031595</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.685816</td>\n",
       "      <td>0.619796</td>\n",
       "      <td>0.634167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>1.053604</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.674359</td>\n",
       "      <td>0.612755</td>\n",
       "      <td>0.626883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.042253</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.728879</td>\n",
       "      <td>0.661815</td>\n",
       "      <td>0.678716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>1.048596</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.730294</td>\n",
       "      <td>0.671618</td>\n",
       "      <td>0.684961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>1.047349</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.756650</td>\n",
       "      <td>0.679463</td>\n",
       "      <td>0.701311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>1.055622</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.776242</td>\n",
       "      <td>0.690999</td>\n",
       "      <td>0.714919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.059487</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.784927</td>\n",
       "      <td>0.685785</td>\n",
       "      <td>0.712123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.073256</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.750415</td>\n",
       "      <td>0.659628</td>\n",
       "      <td>0.685139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>1.067723</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.779700</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.708482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>1.073472</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.780965</td>\n",
       "      <td>0.681446</td>\n",
       "      <td>0.707983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.077597</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.678046</td>\n",
       "      <td>0.703860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.764709</td>\n",
       "      <td>0.676887</td>\n",
       "      <td>0.699980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>1.073897</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.765659</td>\n",
       "      <td>0.677342</td>\n",
       "      <td>0.700691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:04:09,047] Trial 141 finished with value: 0.7006908983668994 and parameters: {'learning_rate': 0.0004981796332020285, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 142 with params: {'learning_rate': 0.0004235336546447731, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.436600</td>\n",
       "      <td>2.971219</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.080552</td>\n",
       "      <td>0.076551</td>\n",
       "      <td>0.056627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.663600</td>\n",
       "      <td>2.330764</td>\n",
       "      <td>0.523373</td>\n",
       "      <td>0.172005</td>\n",
       "      <td>0.164629</td>\n",
       "      <td>0.144924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.074300</td>\n",
       "      <td>1.843183</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.246261</td>\n",
       "      <td>0.240902</td>\n",
       "      <td>0.221480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.603300</td>\n",
       "      <td>1.548545</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.297665</td>\n",
       "      <td>0.310380</td>\n",
       "      <td>0.284838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.264900</td>\n",
       "      <td>1.363574</td>\n",
       "      <td>0.727773</td>\n",
       "      <td>0.364356</td>\n",
       "      <td>0.364495</td>\n",
       "      <td>0.339055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.005300</td>\n",
       "      <td>1.212561</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.445554</td>\n",
       "      <td>0.401595</td>\n",
       "      <td>0.390659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>1.152853</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.464502</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>0.423897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>1.103796</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.459739</td>\n",
       "      <td>0.452342</td>\n",
       "      <td>0.442251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.573800</td>\n",
       "      <td>1.072501</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.513855</td>\n",
       "      <td>0.489304</td>\n",
       "      <td>0.488059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>1.068956</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.456846</td>\n",
       "      <td>0.477856</td>\n",
       "      <td>0.462810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>1.044591</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.503772</td>\n",
       "      <td>0.501476</td>\n",
       "      <td>0.494361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>1.017885</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.534467</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>0.505778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>1.013550</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.545021</td>\n",
       "      <td>0.529750</td>\n",
       "      <td>0.528096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.579419</td>\n",
       "      <td>0.531723</td>\n",
       "      <td>0.539488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>1.019591</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.627153</td>\n",
       "      <td>0.567084</td>\n",
       "      <td>0.578683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.611611</td>\n",
       "      <td>0.575971</td>\n",
       "      <td>0.579001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.167900</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.629088</td>\n",
       "      <td>0.587180</td>\n",
       "      <td>0.594282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>1.002715</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.642742</td>\n",
       "      <td>0.597066</td>\n",
       "      <td>0.604609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>1.021331</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.663590</td>\n",
       "      <td>0.602131</td>\n",
       "      <td>0.613756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>1.016571</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.692491</td>\n",
       "      <td>0.619348</td>\n",
       "      <td>0.636955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>1.022641</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.705502</td>\n",
       "      <td>0.641679</td>\n",
       "      <td>0.655616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>1.018376</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.714297</td>\n",
       "      <td>0.642796</td>\n",
       "      <td>0.658655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>1.023450</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.668706</td>\n",
       "      <td>0.684665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>1.021715</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.731235</td>\n",
       "      <td>0.659890</td>\n",
       "      <td>0.675518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.030219</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.743690</td>\n",
       "      <td>0.668644</td>\n",
       "      <td>0.687701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>1.036486</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.738343</td>\n",
       "      <td>0.664968</td>\n",
       "      <td>0.682387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>1.040898</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.737981</td>\n",
       "      <td>0.658209</td>\n",
       "      <td>0.677412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>1.035077</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.735270</td>\n",
       "      <td>0.662783</td>\n",
       "      <td>0.679664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>1.037408</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.734867</td>\n",
       "      <td>0.662631</td>\n",
       "      <td>0.679670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>1.038477</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.734299</td>\n",
       "      <td>0.660305</td>\n",
       "      <td>0.677375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:06:54,926] Trial 142 finished with value: 0.6773747428897684 and parameters: {'learning_rate': 0.0004235336546447731, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 3}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 143 with params: {'learning_rate': 0.0004823107653084038, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.397400</td>\n",
       "      <td>2.861158</td>\n",
       "      <td>0.419798</td>\n",
       "      <td>0.070006</td>\n",
       "      <td>0.097244</td>\n",
       "      <td>0.074938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.516900</td>\n",
       "      <td>2.150416</td>\n",
       "      <td>0.558203</td>\n",
       "      <td>0.202552</td>\n",
       "      <td>0.200948</td>\n",
       "      <td>0.188174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.867900</td>\n",
       "      <td>1.664317</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.314355</td>\n",
       "      <td>0.306268</td>\n",
       "      <td>0.288934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.402300</td>\n",
       "      <td>1.401335</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.327172</td>\n",
       "      <td>0.328221</td>\n",
       "      <td>0.310687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.089900</td>\n",
       "      <td>1.267919</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.408233</td>\n",
       "      <td>0.389305</td>\n",
       "      <td>0.369395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>1.170490</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.448486</td>\n",
       "      <td>0.423720</td>\n",
       "      <td>0.418097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>1.125839</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.512382</td>\n",
       "      <td>0.455442</td>\n",
       "      <td>0.455777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>1.088361</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.501956</td>\n",
       "      <td>0.476137</td>\n",
       "      <td>0.472821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>1.029726</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.507702</td>\n",
       "      <td>0.482770</td>\n",
       "      <td>0.484231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>1.026881</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.512441</td>\n",
       "      <td>0.489676</td>\n",
       "      <td>0.490714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.019548</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.504361</td>\n",
       "      <td>0.494999</td>\n",
       "      <td>0.488802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>1.009444</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.564477</td>\n",
       "      <td>0.529969</td>\n",
       "      <td>0.534713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>1.011619</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.620512</td>\n",
       "      <td>0.568531</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>1.015744</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.592652</td>\n",
       "      <td>0.548851</td>\n",
       "      <td>0.556150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>1.010417</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.622811</td>\n",
       "      <td>0.578516</td>\n",
       "      <td>0.586390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>1.013827</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.658596</td>\n",
       "      <td>0.600875</td>\n",
       "      <td>0.614537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>1.020781</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.699057</td>\n",
       "      <td>0.630432</td>\n",
       "      <td>0.644753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>1.016827</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.679826</td>\n",
       "      <td>0.612330</td>\n",
       "      <td>0.628148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>1.058798</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.699053</td>\n",
       "      <td>0.630805</td>\n",
       "      <td>0.647675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>1.050384</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.713397</td>\n",
       "      <td>0.632283</td>\n",
       "      <td>0.653835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>1.061244</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.709809</td>\n",
       "      <td>0.639628</td>\n",
       "      <td>0.657133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>1.064988</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.701941</td>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.653325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>1.075570</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.702342</td>\n",
       "      <td>0.633985</td>\n",
       "      <td>0.651188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>1.081094</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.732748</td>\n",
       "      <td>0.652629</td>\n",
       "      <td>0.672362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>1.082236</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.739928</td>\n",
       "      <td>0.646602</td>\n",
       "      <td>0.672127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.083110</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.689022</td>\n",
       "      <td>0.713566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.083086</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.736381</td>\n",
       "      <td>0.662003</td>\n",
       "      <td>0.682145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>1.083084</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.780778</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.715028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>1.079343</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.767104</td>\n",
       "      <td>0.686307</td>\n",
       "      <td>0.708462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>1.080570</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.766081</td>\n",
       "      <td>0.684022</td>\n",
       "      <td>0.707045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:09:35,922] Trial 143 finished with value: 0.7070454037090963 and parameters: {'learning_rate': 0.0004823107653084038, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 4}. Best is trial 118 with value: 0.7156145737360871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144 with params: {'learning_rate': 5.195169526885676e-05, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.52 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.791300</td>\n",
       "      <td>3.676977</td>\n",
       "      <td>0.191567</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>0.012601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.618600</td>\n",
       "      <td>3.539440</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.497000</td>\n",
       "      <td>3.417212</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.373600</td>\n",
       "      <td>3.302738</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.008028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.288700</td>\n",
       "      <td>3.200919</td>\n",
       "      <td>0.304308</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.057674</td>\n",
       "      <td>0.048962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.181400</td>\n",
       "      <td>3.108760</td>\n",
       "      <td>0.368469</td>\n",
       "      <td>0.064227</td>\n",
       "      <td>0.076488</td>\n",
       "      <td>0.060652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.085700</td>\n",
       "      <td>3.021619</td>\n",
       "      <td>0.389551</td>\n",
       "      <td>0.057822</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.062848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.012600</td>\n",
       "      <td>2.941980</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.073941</td>\n",
       "      <td>0.089219</td>\n",
       "      <td>0.066681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.931300</td>\n",
       "      <td>2.867162</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.096871</td>\n",
       "      <td>0.073963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.862600</td>\n",
       "      <td>2.796312</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.065031</td>\n",
       "      <td>0.102032</td>\n",
       "      <td>0.076713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.799300</td>\n",
       "      <td>2.729162</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>0.106075</td>\n",
       "      <td>0.076948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.718500</td>\n",
       "      <td>2.666201</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.081682</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.079324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.675100</td>\n",
       "      <td>2.607622</td>\n",
       "      <td>0.454629</td>\n",
       "      <td>0.081377</td>\n",
       "      <td>0.112991</td>\n",
       "      <td>0.079978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.606800</td>\n",
       "      <td>2.551912</td>\n",
       "      <td>0.459212</td>\n",
       "      <td>0.104060</td>\n",
       "      <td>0.115072</td>\n",
       "      <td>0.082815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.555800</td>\n",
       "      <td>2.501316</td>\n",
       "      <td>0.465628</td>\n",
       "      <td>0.106359</td>\n",
       "      <td>0.120805</td>\n",
       "      <td>0.092463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.498700</td>\n",
       "      <td>2.454534</td>\n",
       "      <td>0.471127</td>\n",
       "      <td>0.099779</td>\n",
       "      <td>0.124635</td>\n",
       "      <td>0.097618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.446100</td>\n",
       "      <td>2.412216</td>\n",
       "      <td>0.481210</td>\n",
       "      <td>0.139454</td>\n",
       "      <td>0.135027</td>\n",
       "      <td>0.111594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.420300</td>\n",
       "      <td>2.375242</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.147165</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>0.121191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.385100</td>\n",
       "      <td>2.340272</td>\n",
       "      <td>0.510541</td>\n",
       "      <td>0.209839</td>\n",
       "      <td>0.158283</td>\n",
       "      <td>0.140429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.351000</td>\n",
       "      <td>2.308554</td>\n",
       "      <td>0.521540</td>\n",
       "      <td>0.202050</td>\n",
       "      <td>0.167111</td>\n",
       "      <td>0.149760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:11:23,867] Trial 144 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 145 with params: {'learning_rate': 0.00015248248744042186, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:59 < 00:59, 5.86 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.635100</td>\n",
       "      <td>3.384617</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.232200</td>\n",
       "      <td>3.034082</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.057374</td>\n",
       "      <td>0.084226</td>\n",
       "      <td>0.063720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.917300</td>\n",
       "      <td>2.723018</td>\n",
       "      <td>0.441797</td>\n",
       "      <td>0.080298</td>\n",
       "      <td>0.105827</td>\n",
       "      <td>0.077396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.609700</td>\n",
       "      <td>2.452161</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>0.144264</td>\n",
       "      <td>0.140912</td>\n",
       "      <td>0.118963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.370700</td>\n",
       "      <td>2.217189</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.225760</td>\n",
       "      <td>0.187475</td>\n",
       "      <td>0.173412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.120400</td>\n",
       "      <td>2.029992</td>\n",
       "      <td>0.593951</td>\n",
       "      <td>0.245687</td>\n",
       "      <td>0.214212</td>\n",
       "      <td>0.196347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.917700</td>\n",
       "      <td>1.878856</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.319001</td>\n",
       "      <td>0.265761</td>\n",
       "      <td>0.256790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.763400</td>\n",
       "      <td>1.751952</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.336211</td>\n",
       "      <td>0.296826</td>\n",
       "      <td>0.279860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>1.636251</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.363841</td>\n",
       "      <td>0.316075</td>\n",
       "      <td>0.306535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.464200</td>\n",
       "      <td>1.549035</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.375413</td>\n",
       "      <td>0.340823</td>\n",
       "      <td>0.325391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.368200</td>\n",
       "      <td>1.475123</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.372680</td>\n",
       "      <td>0.364095</td>\n",
       "      <td>0.346411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.251400</td>\n",
       "      <td>1.423180</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.364382</td>\n",
       "      <td>0.357473</td>\n",
       "      <td>0.339367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.187000</td>\n",
       "      <td>1.362949</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.353294</td>\n",
       "      <td>0.366996</td>\n",
       "      <td>0.347765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.097000</td>\n",
       "      <td>1.320196</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.381939</td>\n",
       "      <td>0.393537</td>\n",
       "      <td>0.372515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.039900</td>\n",
       "      <td>1.296770</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.367938</td>\n",
       "      <td>0.394228</td>\n",
       "      <td>0.368942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.983300</td>\n",
       "      <td>1.271521</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.389099</td>\n",
       "      <td>0.396533</td>\n",
       "      <td>0.375158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.920100</td>\n",
       "      <td>1.249846</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.431454</td>\n",
       "      <td>0.409659</td>\n",
       "      <td>0.391767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.879100</td>\n",
       "      <td>1.220170</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.394191</td>\n",
       "      <td>0.395483</td>\n",
       "      <td>0.377605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>1.210979</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.432526</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.402257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.817400</td>\n",
       "      <td>1.191459</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.413481</td>\n",
       "      <td>0.410351</td>\n",
       "      <td>0.396103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:13:24,029] Trial 145 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 146 with params: {'learning_rate': 0.00013897742449221987, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:41 < 00:50, 6.87 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.650400</td>\n",
       "      <td>3.412247</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.267400</td>\n",
       "      <td>3.074276</td>\n",
       "      <td>0.390467</td>\n",
       "      <td>0.058628</td>\n",
       "      <td>0.083284</td>\n",
       "      <td>0.063973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.966500</td>\n",
       "      <td>2.778881</td>\n",
       "      <td>0.436297</td>\n",
       "      <td>0.062849</td>\n",
       "      <td>0.102679</td>\n",
       "      <td>0.074858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.674000</td>\n",
       "      <td>2.519199</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.119665</td>\n",
       "      <td>0.127897</td>\n",
       "      <td>0.102453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.447000</td>\n",
       "      <td>2.290409</td>\n",
       "      <td>0.542621</td>\n",
       "      <td>0.207118</td>\n",
       "      <td>0.175492</td>\n",
       "      <td>0.160765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.206200</td>\n",
       "      <td>2.107826</td>\n",
       "      <td>0.586618</td>\n",
       "      <td>0.229013</td>\n",
       "      <td>0.206009</td>\n",
       "      <td>0.187763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.010500</td>\n",
       "      <td>1.958677</td>\n",
       "      <td>0.623281</td>\n",
       "      <td>0.279979</td>\n",
       "      <td>0.237544</td>\n",
       "      <td>0.228771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.860900</td>\n",
       "      <td>1.832364</td>\n",
       "      <td>0.668194</td>\n",
       "      <td>0.328966</td>\n",
       "      <td>0.284279</td>\n",
       "      <td>0.271350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.703500</td>\n",
       "      <td>1.716218</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.340903</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>0.286775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.567300</td>\n",
       "      <td>1.625075</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.361551</td>\n",
       "      <td>0.323359</td>\n",
       "      <td>0.309028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.469800</td>\n",
       "      <td>1.545912</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.360416</td>\n",
       "      <td>0.343034</td>\n",
       "      <td>0.330912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.353000</td>\n",
       "      <td>1.489498</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.362228</td>\n",
       "      <td>0.344295</td>\n",
       "      <td>0.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.285600</td>\n",
       "      <td>1.424870</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.360643</td>\n",
       "      <td>0.354764</td>\n",
       "      <td>0.337185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.196600</td>\n",
       "      <td>1.383016</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.375267</td>\n",
       "      <td>0.385190</td>\n",
       "      <td>0.364012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.137000</td>\n",
       "      <td>1.349866</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.366495</td>\n",
       "      <td>0.389092</td>\n",
       "      <td>0.365842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.079300</td>\n",
       "      <td>1.323663</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.357343</td>\n",
       "      <td>0.386882</td>\n",
       "      <td>0.362362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.014000</td>\n",
       "      <td>1.296784</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.372106</td>\n",
       "      <td>0.397610</td>\n",
       "      <td>0.374752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.971900</td>\n",
       "      <td>1.268376</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.404035</td>\n",
       "      <td>0.389503</td>\n",
       "      <td>0.372627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.950500</td>\n",
       "      <td>1.256083</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.412438</td>\n",
       "      <td>0.408015</td>\n",
       "      <td>0.386002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>1.234731</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.405092</td>\n",
       "      <td>0.405531</td>\n",
       "      <td>0.386569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:15:06,620] Trial 146 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 147 with params: {'learning_rate': 1.4477251200913327e-05, 'weight_decay': 0.004, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:05, 6.95 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.858800</td>\n",
       "      <td>3.806967</td>\n",
       "      <td>0.096242</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.031298</td>\n",
       "      <td>0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.781400</td>\n",
       "      <td>3.740043</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>0.009656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.729700</td>\n",
       "      <td>3.687729</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.012843</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>0.010963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.678300</td>\n",
       "      <td>3.639993</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.643900</td>\n",
       "      <td>3.595624</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:15:32,498] Trial 147 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 148 with params: {'learning_rate': 0.00030333774133548387, 'weight_decay': 0.004, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:43 < 00:52, 6.72 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.476600</td>\n",
       "      <td>3.102490</td>\n",
       "      <td>0.334555</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>0.066953</td>\n",
       "      <td>0.051488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.859100</td>\n",
       "      <td>2.569742</td>\n",
       "      <td>0.448213</td>\n",
       "      <td>0.062024</td>\n",
       "      <td>0.110548</td>\n",
       "      <td>0.078073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.380400</td>\n",
       "      <td>2.152195</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.186185</td>\n",
       "      <td>0.171495</td>\n",
       "      <td>0.151384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.967400</td>\n",
       "      <td>1.841762</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.271059</td>\n",
       "      <td>0.248863</td>\n",
       "      <td>0.231577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.650900</td>\n",
       "      <td>1.599640</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.331076</td>\n",
       "      <td>0.327210</td>\n",
       "      <td>0.308022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.369800</td>\n",
       "      <td>1.411225</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.333157</td>\n",
       "      <td>0.347052</td>\n",
       "      <td>0.326936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.140400</td>\n",
       "      <td>1.310322</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.365686</td>\n",
       "      <td>0.352327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>1.246827</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.413852</td>\n",
       "      <td>0.408940</td>\n",
       "      <td>0.387986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.857700</td>\n",
       "      <td>1.183771</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.427460</td>\n",
       "      <td>0.415337</td>\n",
       "      <td>0.404059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>1.145553</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.470048</td>\n",
       "      <td>0.426457</td>\n",
       "      <td>0.425042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>1.114713</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.498907</td>\n",
       "      <td>0.457391</td>\n",
       "      <td>0.459633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.571600</td>\n",
       "      <td>1.077640</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.500489</td>\n",
       "      <td>0.471297</td>\n",
       "      <td>0.471219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>1.069362</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.482343</td>\n",
       "      <td>0.480062</td>\n",
       "      <td>0.473310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>1.017354</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.499448</td>\n",
       "      <td>0.482433</td>\n",
       "      <td>0.481394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>1.031177</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.488551</td>\n",
       "      <td>0.489921</td>\n",
       "      <td>0.483291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>1.007898</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.512265</td>\n",
       "      <td>0.499284</td>\n",
       "      <td>0.496226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>1.000473</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.514772</td>\n",
       "      <td>0.501402</td>\n",
       "      <td>0.498660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.997762</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.518542</td>\n",
       "      <td>0.514661</td>\n",
       "      <td>0.509247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>1.005669</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.572027</td>\n",
       "      <td>0.535155</td>\n",
       "      <td>0.541218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.986511</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.562411</td>\n",
       "      <td>0.518550</td>\n",
       "      <td>0.525530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:17:17,413] Trial 148 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 149 with params: {'learning_rate': 0.0004499563740490953, 'weight_decay': 0.0, 'adam_beta1': 0.93, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='581' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 581/1050 01:24 < 01:08, 6.89 it/s, Epoch 16.57/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.349000</td>\n",
       "      <td>2.858869</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.072132</td>\n",
       "      <td>0.093801</td>\n",
       "      <td>0.073905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.552300</td>\n",
       "      <td>2.215647</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.168980</td>\n",
       "      <td>0.178683</td>\n",
       "      <td>0.160364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.958800</td>\n",
       "      <td>1.747913</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.273792</td>\n",
       "      <td>0.261621</td>\n",
       "      <td>0.246225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.507600</td>\n",
       "      <td>1.473900</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.306409</td>\n",
       "      <td>0.330714</td>\n",
       "      <td>0.298926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.186200</td>\n",
       "      <td>1.305043</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.315198</td>\n",
       "      <td>0.355448</td>\n",
       "      <td>0.325776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>1.175576</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.464182</td>\n",
       "      <td>0.414157</td>\n",
       "      <td>0.406327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.756600</td>\n",
       "      <td>1.124920</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.454972</td>\n",
       "      <td>0.432995</td>\n",
       "      <td>0.428937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>1.074999</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.481145</td>\n",
       "      <td>0.451322</td>\n",
       "      <td>0.446282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>1.043069</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.486491</td>\n",
       "      <td>0.476648</td>\n",
       "      <td>0.468342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>1.024741</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.459329</td>\n",
       "      <td>0.461784</td>\n",
       "      <td>0.453779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>1.010778</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.532743</td>\n",
       "      <td>0.504284</td>\n",
       "      <td>0.507669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.995985</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.574677</td>\n",
       "      <td>0.518929</td>\n",
       "      <td>0.529689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>1.001593</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.607070</td>\n",
       "      <td>0.564507</td>\n",
       "      <td>0.568748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.980683</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.643661</td>\n",
       "      <td>0.555048</td>\n",
       "      <td>0.577058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>1.002482</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.561901</td>\n",
       "      <td>0.579885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.994648</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.609381</td>\n",
       "      <td>0.572940</td>\n",
       "      <td>0.576032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-15 14:18:42,874] Trial 149 failed with parameters: {'learning_rate': 0.0004499563740490953, 'weight_decay': 0.0, 'adam_beta1': 0.93, 'warmup_steps': 0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 250, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2241, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2553, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-15 14:18:42,878] Trial 149 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptuna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhp_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_objective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpruner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3588\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   3585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m hp_name\n\u001b[1;32m   3586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_objective \u001b[38;5;241m=\u001b[39m default_compute_objective \u001b[38;5;28;01mif\u001b[39;00m compute_objective \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m compute_objective\n\u001b[0;32m-> 3588\u001b[0m best_run \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_run\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/hyperparameter_search.py:72\u001b[0m, in \u001b[0;36mOptunaBackend.run\u001b[0;34m(self, trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, n_trials: \u001b[38;5;28mint\u001b[39m, direction: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_hp_search_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py:268\u001b[0m, in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m directions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m direction\n\u001b[1;32m    267\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39mdirection, directions\u001b[38;5;241m=\u001b[39mdirections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 268\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    270\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py:250\u001b[0m, in \u001b[0;36mrun_hp_search_optuna.<locals>._objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    248\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name=\"Test-base\",\n",
    "    n_trials=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43d41e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='118', objective=0.7156145737360871, hyperparameters={'learning_rate': 0.0004762367229644209, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 0}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff001c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f49758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill_fine_hp-search\", logging_dir=f\"~/logs/{DATASET}/bert-distill_fine_hp-search\", remove_unused_columns=False, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb876364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    params =  {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 1e-2, step=1e-3),\n",
    "        \"adam_beta1\" : trial.suggest_float(\"adam_beta1\", 0.9, 0.99, step=0.01),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, warm_up),\n",
    "        \"lambda_param\": trial.suggest_float(\"lambda_param\",0,1,step=.1),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 2,7, step=.5)\n",
    "    }\n",
    "    print(f\"Trial {trial.number} with params: {params}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6858ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.HyperbandPruner(min_resource=min_r, max_resource=max_r, reduction_factor=2, bootstrap_count=2)\n",
    "sampler = optuna.samplers.TPESampler(seed=42, multivariate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "511a945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    model_init = lambda: get_Bert(),\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7091f8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:20:09,293] A new study created in memory with name: Test-destilace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 with params: {'learning_rate': 1.0253509690168497e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:08, 6.84 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.459400</td>\n",
       "      <td>2.429851</td>\n",
       "      <td>0.041247</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>0.006414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.423900</td>\n",
       "      <td>2.396845</td>\n",
       "      <td>0.160403</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.396100</td>\n",
       "      <td>2.368189</td>\n",
       "      <td>0.189734</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.024276</td>\n",
       "      <td>0.012312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.371700</td>\n",
       "      <td>2.344594</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.022362</td>\n",
       "      <td>0.009811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.349700</td>\n",
       "      <td>2.323391</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:20:35,601] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 with params: {'learning_rate': 1.4347159517201402e-06, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:43 < 00:51, 6.77 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.474900</td>\n",
       "      <td>2.462585</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.468900</td>\n",
       "      <td>2.456624</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.464900</td>\n",
       "      <td>2.451249</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.002535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.460800</td>\n",
       "      <td>2.446311</td>\n",
       "      <td>0.016499</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.003054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.452700</td>\n",
       "      <td>2.441660</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.003783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.448100</td>\n",
       "      <td>2.437341</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.005226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.445400</td>\n",
       "      <td>2.433274</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>0.006695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.440600</td>\n",
       "      <td>2.429358</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.032154</td>\n",
       "      <td>0.027201</td>\n",
       "      <td>0.007719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.437800</td>\n",
       "      <td>2.425663</td>\n",
       "      <td>0.058662</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.028624</td>\n",
       "      <td>0.008521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.434200</td>\n",
       "      <td>2.422149</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>0.008839</td>\n",
       "      <td>0.029577</td>\n",
       "      <td>0.007975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.430100</td>\n",
       "      <td>2.418758</td>\n",
       "      <td>0.087993</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>0.008720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.429500</td>\n",
       "      <td>2.415579</td>\n",
       "      <td>0.100825</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>0.009028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.425100</td>\n",
       "      <td>2.412506</td>\n",
       "      <td>0.109991</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.008830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.409599</td>\n",
       "      <td>0.118240</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>0.034307</td>\n",
       "      <td>0.008890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.418600</td>\n",
       "      <td>2.406846</td>\n",
       "      <td>0.126489</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.008813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.416700</td>\n",
       "      <td>2.404221</td>\n",
       "      <td>0.138405</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.036757</td>\n",
       "      <td>0.009318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.415900</td>\n",
       "      <td>2.401794</td>\n",
       "      <td>0.143905</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.037379</td>\n",
       "      <td>0.009136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.412400</td>\n",
       "      <td>2.399507</td>\n",
       "      <td>0.153071</td>\n",
       "      <td>0.009483</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>0.009412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.410000</td>\n",
       "      <td>2.397449</td>\n",
       "      <td>0.161320</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.009848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.411100</td>\n",
       "      <td>2.395539</td>\n",
       "      <td>0.163153</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>0.009806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:22:19,764] Trial 1 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 with params: {'learning_rate': 0.0001764971584817573, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.68 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.291700</td>\n",
       "      <td>2.118388</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.006952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.020500</td>\n",
       "      <td>1.875414</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.066579</td>\n",
       "      <td>0.092705</td>\n",
       "      <td>0.068026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.793500</td>\n",
       "      <td>1.648747</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.142685</td>\n",
       "      <td>0.135698</td>\n",
       "      <td>0.114761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.574000</td>\n",
       "      <td>1.461999</td>\n",
       "      <td>0.547204</td>\n",
       "      <td>0.172001</td>\n",
       "      <td>0.174684</td>\n",
       "      <td>0.153992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.405200</td>\n",
       "      <td>1.315813</td>\n",
       "      <td>0.599450</td>\n",
       "      <td>0.240876</td>\n",
       "      <td>0.211836</td>\n",
       "      <td>0.194970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.244800</td>\n",
       "      <td>1.198362</td>\n",
       "      <td>0.657195</td>\n",
       "      <td>0.260667</td>\n",
       "      <td>0.257932</td>\n",
       "      <td>0.240893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.116200</td>\n",
       "      <td>1.104186</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.262459</td>\n",
       "      <td>0.276230</td>\n",
       "      <td>0.255129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.019700</td>\n",
       "      <td>1.031983</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.265929</td>\n",
       "      <td>0.292570</td>\n",
       "      <td>0.266980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.927900</td>\n",
       "      <td>0.975501</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.286390</td>\n",
       "      <td>0.301362</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.926788</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.310206</td>\n",
       "      <td>0.318341</td>\n",
       "      <td>0.294888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.789600</td>\n",
       "      <td>0.890053</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.301233</td>\n",
       "      <td>0.319771</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.861878</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.333304</td>\n",
       "      <td>0.328929</td>\n",
       "      <td>0.306939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.831936</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.335830</td>\n",
       "      <td>0.333692</td>\n",
       "      <td>0.315313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.812474</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.358620</td>\n",
       "      <td>0.361942</td>\n",
       "      <td>0.342128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.800679</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.380202</td>\n",
       "      <td>0.368384</td>\n",
       "      <td>0.352424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>0.793277</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.364473</td>\n",
       "      <td>0.365422</td>\n",
       "      <td>0.344749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>0.781076</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.362034</td>\n",
       "      <td>0.375978</td>\n",
       "      <td>0.356469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.765635</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.369313</td>\n",
       "      <td>0.383027</td>\n",
       "      <td>0.366138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.761564</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.367489</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.752660</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.403720</td>\n",
       "      <td>0.390351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:24:05,306] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 with params: {'learning_rate': 1.4648955132800731e-05, 'weight_decay': 0.003, 'adam_beta1': 0.96, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:56 < 00:58, 6.01 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.450600</td>\n",
       "      <td>2.413191</td>\n",
       "      <td>0.106324</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.007847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.402700</td>\n",
       "      <td>2.369193</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>0.010798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.367400</td>\n",
       "      <td>2.335685</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.010754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.337300</td>\n",
       "      <td>2.304905</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.309700</td>\n",
       "      <td>2.276983</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.281200</td>\n",
       "      <td>2.250201</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.257600</td>\n",
       "      <td>2.225717</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.235800</td>\n",
       "      <td>2.202315</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.215000</td>\n",
       "      <td>2.181003</td>\n",
       "      <td>0.192484</td>\n",
       "      <td>0.043594</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.013753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.197300</td>\n",
       "      <td>2.160498</td>\n",
       "      <td>0.215399</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.023140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.177600</td>\n",
       "      <td>2.141517</td>\n",
       "      <td>0.259395</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>0.043499</td>\n",
       "      <td>0.039481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.158600</td>\n",
       "      <td>2.123315</td>\n",
       "      <td>0.294225</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.049788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.146200</td>\n",
       "      <td>2.106174</td>\n",
       "      <td>0.329973</td>\n",
       "      <td>0.071542</td>\n",
       "      <td>0.064259</td>\n",
       "      <td>0.057964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.125100</td>\n",
       "      <td>2.090612</td>\n",
       "      <td>0.343721</td>\n",
       "      <td>0.069180</td>\n",
       "      <td>0.068574</td>\n",
       "      <td>0.060497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.113700</td>\n",
       "      <td>2.076347</td>\n",
       "      <td>0.361137</td>\n",
       "      <td>0.063813</td>\n",
       "      <td>0.073577</td>\n",
       "      <td>0.062116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.097300</td>\n",
       "      <td>2.062974</td>\n",
       "      <td>0.374885</td>\n",
       "      <td>0.081986</td>\n",
       "      <td>0.078049</td>\n",
       "      <td>0.064557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.084700</td>\n",
       "      <td>2.050773</td>\n",
       "      <td>0.381302</td>\n",
       "      <td>0.081012</td>\n",
       "      <td>0.079921</td>\n",
       "      <td>0.065116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.077600</td>\n",
       "      <td>2.039548</td>\n",
       "      <td>0.385885</td>\n",
       "      <td>0.079470</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.065000</td>\n",
       "      <td>2.029138</td>\n",
       "      <td>0.391384</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.082608</td>\n",
       "      <td>0.064013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.057900</td>\n",
       "      <td>2.019842</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>0.083209</td>\n",
       "      <td>0.063613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:26:02,817] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 with params: {'learning_rate': 1.7018418817029176e-05, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 2, 'lambda_param': 0.6000000000000001, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:04, 7.01 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.449200</td>\n",
       "      <td>2.407241</td>\n",
       "      <td>0.128323</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>0.008741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.393700</td>\n",
       "      <td>2.355601</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.010541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.353000</td>\n",
       "      <td>2.315982</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.316800</td>\n",
       "      <td>2.281678</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.284700</td>\n",
       "      <td>2.245561</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:26:28,463] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 with params: {'learning_rate': 4.3625993625605605e-05, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 4, 'lambda_param': 1.0, 'temperature': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:42, 6.81 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.418700</td>\n",
       "      <td>2.340030</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.310300</td>\n",
       "      <td>2.248027</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.230500</td>\n",
       "      <td>2.164900</td>\n",
       "      <td>0.201650</td>\n",
       "      <td>0.059635</td>\n",
       "      <td>0.026992</td>\n",
       "      <td>0.017953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.152900</td>\n",
       "      <td>2.088111</td>\n",
       "      <td>0.349221</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>0.070230</td>\n",
       "      <td>0.060808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>2.018613</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.077292</td>\n",
       "      <td>0.084844</td>\n",
       "      <td>0.065279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.014900</td>\n",
       "      <td>1.953556</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.089932</td>\n",
       "      <td>0.068166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.951800</td>\n",
       "      <td>1.893720</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>0.087108</td>\n",
       "      <td>0.099525</td>\n",
       "      <td>0.077771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.897200</td>\n",
       "      <td>1.837176</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>0.104949</td>\n",
       "      <td>0.113664</td>\n",
       "      <td>0.092499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.841700</td>\n",
       "      <td>1.785568</td>\n",
       "      <td>0.469294</td>\n",
       "      <td>0.102984</td>\n",
       "      <td>0.122360</td>\n",
       "      <td>0.099440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.794200</td>\n",
       "      <td>1.739937</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.101493</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>0.105799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:27:21,391] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 with params: {'learning_rate': 6.639623079859462e-06, 'weight_decay': 0.001, 'adam_beta1': 0.96, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:45, 6.67 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.465300</td>\n",
       "      <td>2.442222</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.002785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.440800</td>\n",
       "      <td>2.419843</td>\n",
       "      <td>0.073327</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.006799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.398395</td>\n",
       "      <td>0.155820</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.009355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.403400</td>\n",
       "      <td>2.378804</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>0.022973</td>\n",
       "      <td>0.011063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.383800</td>\n",
       "      <td>2.361715</td>\n",
       "      <td>0.190651</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>0.024550</td>\n",
       "      <td>0.012650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.367800</td>\n",
       "      <td>2.346921</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.011169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.355400</td>\n",
       "      <td>2.333164</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.342700</td>\n",
       "      <td>2.320621</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.331700</td>\n",
       "      <td>2.309198</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.321600</td>\n",
       "      <td>2.298605</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.016907</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:28:14,558] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 with params: {'learning_rate': 1.2382649697023546e-06, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.475300</td>\n",
       "      <td>2.463449</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.470200</td>\n",
       "      <td>2.458217</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.466900</td>\n",
       "      <td>2.453497</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.463300</td>\n",
       "      <td>2.449136</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.455700</td>\n",
       "      <td>2.445041</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.003380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.451800</td>\n",
       "      <td>2.441245</td>\n",
       "      <td>0.025665</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.004772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.449400</td>\n",
       "      <td>2.437656</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.005108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.445200</td>\n",
       "      <td>2.434200</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.027944</td>\n",
       "      <td>0.025580</td>\n",
       "      <td>0.006260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.442700</td>\n",
       "      <td>2.430972</td>\n",
       "      <td>0.047663</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>0.008022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.439500</td>\n",
       "      <td>2.427918</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.008144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.435800</td>\n",
       "      <td>2.424966</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.028350</td>\n",
       "      <td>0.007925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.435700</td>\n",
       "      <td>2.422181</td>\n",
       "      <td>0.076077</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.029721</td>\n",
       "      <td>0.008426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.431400</td>\n",
       "      <td>2.419533</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.030964</td>\n",
       "      <td>0.008818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.428600</td>\n",
       "      <td>2.417060</td>\n",
       "      <td>0.097159</td>\n",
       "      <td>0.010332</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>0.009433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.425800</td>\n",
       "      <td>2.414693</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.009395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.424200</td>\n",
       "      <td>2.412468</td>\n",
       "      <td>0.109991</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.033374</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.424100</td>\n",
       "      <td>2.410380</td>\n",
       "      <td>0.115490</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.008810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.420400</td>\n",
       "      <td>2.408451</td>\n",
       "      <td>0.123740</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.418600</td>\n",
       "      <td>2.406665</td>\n",
       "      <td>0.126489</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.008788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.419800</td>\n",
       "      <td>2.404987</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.009161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:30:03,034] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 with params: {'learning_rate': 2.9891977384599008e-05, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 3, 'lambda_param': 1.0, 'temperature': 6.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:08, 6.81 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.433800</td>\n",
       "      <td>2.375420</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>0.011432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.357200</td>\n",
       "      <td>2.314352</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.305800</td>\n",
       "      <td>2.264966</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.258700</td>\n",
       "      <td>2.219901</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.219600</td>\n",
       "      <td>2.178287</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:30:29,564] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 with params: {'learning_rate': 4.108791545324077e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.413000</td>\n",
       "      <td>2.337925</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.310900</td>\n",
       "      <td>2.251229</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.235900</td>\n",
       "      <td>2.172647</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.063597</td>\n",
       "      <td>0.024742</td>\n",
       "      <td>0.014307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.162600</td>\n",
       "      <td>2.100256</td>\n",
       "      <td>0.341888</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>0.068039</td>\n",
       "      <td>0.060738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.100900</td>\n",
       "      <td>2.034513</td>\n",
       "      <td>0.392301</td>\n",
       "      <td>0.078309</td>\n",
       "      <td>0.083211</td>\n",
       "      <td>0.065104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.032700</td>\n",
       "      <td>1.973692</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.073564</td>\n",
       "      <td>0.087633</td>\n",
       "      <td>0.064854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.973200</td>\n",
       "      <td>1.917075</td>\n",
       "      <td>0.417965</td>\n",
       "      <td>0.091995</td>\n",
       "      <td>0.093116</td>\n",
       "      <td>0.071774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.921300</td>\n",
       "      <td>1.863005</td>\n",
       "      <td>0.443630</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.105860</td>\n",
       "      <td>0.082665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.868700</td>\n",
       "      <td>1.813754</td>\n",
       "      <td>0.457379</td>\n",
       "      <td>0.102647</td>\n",
       "      <td>0.115298</td>\n",
       "      <td>0.092977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.823800</td>\n",
       "      <td>1.770011</td>\n",
       "      <td>0.476627</td>\n",
       "      <td>0.102889</td>\n",
       "      <td>0.127239</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.782400</td>\n",
       "      <td>1.727752</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.102957</td>\n",
       "      <td>0.130653</td>\n",
       "      <td>0.106633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.735100</td>\n",
       "      <td>1.689984</td>\n",
       "      <td>0.491292</td>\n",
       "      <td>0.102449</td>\n",
       "      <td>0.135201</td>\n",
       "      <td>0.109740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.706500</td>\n",
       "      <td>1.656049</td>\n",
       "      <td>0.498625</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.139441</td>\n",
       "      <td>0.112308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.669500</td>\n",
       "      <td>1.624133</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.116920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.636300</td>\n",
       "      <td>1.595170</td>\n",
       "      <td>0.505958</td>\n",
       "      <td>0.140810</td>\n",
       "      <td>0.144285</td>\n",
       "      <td>0.117612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.606200</td>\n",
       "      <td>1.570157</td>\n",
       "      <td>0.517874</td>\n",
       "      <td>0.194806</td>\n",
       "      <td>0.149969</td>\n",
       "      <td>0.125634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.576200</td>\n",
       "      <td>1.546714</td>\n",
       "      <td>0.532539</td>\n",
       "      <td>0.198811</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>0.144275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.559400</td>\n",
       "      <td>1.525394</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.205311</td>\n",
       "      <td>0.168534</td>\n",
       "      <td>0.152556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.540700</td>\n",
       "      <td>1.505867</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.211569</td>\n",
       "      <td>0.175409</td>\n",
       "      <td>0.161016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.522500</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>0.552704</td>\n",
       "      <td>0.210804</td>\n",
       "      <td>0.181888</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.494300</td>\n",
       "      <td>1.473918</td>\n",
       "      <td>0.556370</td>\n",
       "      <td>0.214079</td>\n",
       "      <td>0.184026</td>\n",
       "      <td>0.168476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.487000</td>\n",
       "      <td>1.460518</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.211462</td>\n",
       "      <td>0.186013</td>\n",
       "      <td>0.169985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.465700</td>\n",
       "      <td>1.449284</td>\n",
       "      <td>0.563703</td>\n",
       "      <td>0.229173</td>\n",
       "      <td>0.188149</td>\n",
       "      <td>0.172739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.459200</td>\n",
       "      <td>1.439721</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.249339</td>\n",
       "      <td>0.196191</td>\n",
       "      <td>0.181428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.446200</td>\n",
       "      <td>1.431714</td>\n",
       "      <td>0.571952</td>\n",
       "      <td>0.255889</td>\n",
       "      <td>0.196499</td>\n",
       "      <td>0.182236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.432800</td>\n",
       "      <td>1.424926</td>\n",
       "      <td>0.576535</td>\n",
       "      <td>0.252703</td>\n",
       "      <td>0.198814</td>\n",
       "      <td>0.183810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.435100</td>\n",
       "      <td>1.419721</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.248949</td>\n",
       "      <td>0.197933</td>\n",
       "      <td>0.182695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.428500</td>\n",
       "      <td>1.415962</td>\n",
       "      <td>0.580202</td>\n",
       "      <td>0.252119</td>\n",
       "      <td>0.200837</td>\n",
       "      <td>0.186855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>1.413707</td>\n",
       "      <td>0.580202</td>\n",
       "      <td>0.252184</td>\n",
       "      <td>0.200837</td>\n",
       "      <td>0.186887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.409600</td>\n",
       "      <td>1.412883</td>\n",
       "      <td>0.581118</td>\n",
       "      <td>0.252793</td>\n",
       "      <td>0.201363</td>\n",
       "      <td>0.187788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:33:15,378] Trial 9 finished with value: 0.18778763747230062 and parameters: {'learning_rate': 4.108791545324077e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 3.5}. Best is trial 9 with value: 0.18778763747230062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 with params: {'learning_rate': 6.182305620915354e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:43, 6.75 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.389200</td>\n",
       "      <td>2.297538</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.258200</td>\n",
       "      <td>2.182814</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.158600</td>\n",
       "      <td>2.079293</td>\n",
       "      <td>0.344638</td>\n",
       "      <td>0.066101</td>\n",
       "      <td>0.068732</td>\n",
       "      <td>0.058711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.059100</td>\n",
       "      <td>1.983368</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>0.086837</td>\n",
       "      <td>0.063775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.974100</td>\n",
       "      <td>1.892501</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>0.086049</td>\n",
       "      <td>0.098232</td>\n",
       "      <td>0.075189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.879100</td>\n",
       "      <td>1.808123</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>0.104354</td>\n",
       "      <td>0.114868</td>\n",
       "      <td>0.092778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.798400</td>\n",
       "      <td>1.735307</td>\n",
       "      <td>0.475710</td>\n",
       "      <td>0.103409</td>\n",
       "      <td>0.126702</td>\n",
       "      <td>0.102962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.732300</td>\n",
       "      <td>1.667390</td>\n",
       "      <td>0.494959</td>\n",
       "      <td>0.121061</td>\n",
       "      <td>0.138796</td>\n",
       "      <td>0.112338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.661700</td>\n",
       "      <td>1.604500</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.119884</td>\n",
       "      <td>0.143682</td>\n",
       "      <td>0.116026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.600600</td>\n",
       "      <td>1.551492</td>\n",
       "      <td>0.524290</td>\n",
       "      <td>0.186036</td>\n",
       "      <td>0.158349</td>\n",
       "      <td>0.137228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:34:07,818] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 with params: {'learning_rate': 0.00015915550792002775, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.50 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.303200</td>\n",
       "      <td>2.137296</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.046500</td>\n",
       "      <td>1.908542</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.070244</td>\n",
       "      <td>0.089909</td>\n",
       "      <td>0.064655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.834100</td>\n",
       "      <td>1.693793</td>\n",
       "      <td>0.471127</td>\n",
       "      <td>0.123545</td>\n",
       "      <td>0.128563</td>\n",
       "      <td>0.106314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.626700</td>\n",
       "      <td>1.513946</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.171507</td>\n",
       "      <td>0.154161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.465700</td>\n",
       "      <td>1.370520</td>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.243919</td>\n",
       "      <td>0.200412</td>\n",
       "      <td>0.184173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.310300</td>\n",
       "      <td>1.254927</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>0.281314</td>\n",
       "      <td>0.244787</td>\n",
       "      <td>0.233591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.183900</td>\n",
       "      <td>1.158519</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.261774</td>\n",
       "      <td>0.268938</td>\n",
       "      <td>0.250222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.088500</td>\n",
       "      <td>1.084924</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.262502</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.260973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.995900</td>\n",
       "      <td>1.023059</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.303643</td>\n",
       "      <td>0.297096</td>\n",
       "      <td>0.274102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>0.970873</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.301225</td>\n",
       "      <td>0.306129</td>\n",
       "      <td>0.282891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:35:02,715] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 with params: {'learning_rate': 0.000419426490802605, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.1, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.157400</td>\n",
       "      <td>1.862028</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.065617</td>\n",
       "      <td>0.100432</td>\n",
       "      <td>0.075538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.671100</td>\n",
       "      <td>1.452709</td>\n",
       "      <td>0.549038</td>\n",
       "      <td>0.168522</td>\n",
       "      <td>0.181672</td>\n",
       "      <td>0.161647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.300300</td>\n",
       "      <td>1.165089</td>\n",
       "      <td>0.636114</td>\n",
       "      <td>0.229216</td>\n",
       "      <td>0.241084</td>\n",
       "      <td>0.224323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.030300</td>\n",
       "      <td>0.998735</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.275819</td>\n",
       "      <td>0.288456</td>\n",
       "      <td>0.264110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.895196</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.322039</td>\n",
       "      <td>0.326894</td>\n",
       "      <td>0.303367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.815031</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.385046</td>\n",
       "      <td>0.357287</td>\n",
       "      <td>0.342217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>0.778704</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.401253</td>\n",
       "      <td>0.380203</td>\n",
       "      <td>0.366018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.746761</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.404369</td>\n",
       "      <td>0.403072</td>\n",
       "      <td>0.390389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.721888</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.449973</td>\n",
       "      <td>0.430582</td>\n",
       "      <td>0.423110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.386000</td>\n",
       "      <td>0.705953</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.471796</td>\n",
       "      <td>0.438014</td>\n",
       "      <td>0.438794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.701292</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.476429</td>\n",
       "      <td>0.460649</td>\n",
       "      <td>0.453724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.674663</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.530752</td>\n",
       "      <td>0.487819</td>\n",
       "      <td>0.494666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.543128</td>\n",
       "      <td>0.498749</td>\n",
       "      <td>0.503687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.656191</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.562242</td>\n",
       "      <td>0.529826</td>\n",
       "      <td>0.533931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.661314</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.617333</td>\n",
       "      <td>0.541102</td>\n",
       "      <td>0.560013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.642897</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.598838</td>\n",
       "      <td>0.554378</td>\n",
       "      <td>0.563117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.645871</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.598653</td>\n",
       "      <td>0.542718</td>\n",
       "      <td>0.553928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.618432</td>\n",
       "      <td>0.561317</td>\n",
       "      <td>0.572107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.644818</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.617160</td>\n",
       "      <td>0.574487</td>\n",
       "      <td>0.582489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.637716</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.640701</td>\n",
       "      <td>0.575786</td>\n",
       "      <td>0.590877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:36:51,194] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 with params: {'learning_rate': 7.100205390479966e-06, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 02:38 < 05:18, 2.20 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.463300</td>\n",
       "      <td>2.439474</td>\n",
       "      <td>0.021998</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.023085</td>\n",
       "      <td>0.003645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.415150</td>\n",
       "      <td>0.095325</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.031897</td>\n",
       "      <td>0.008723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>2.391193</td>\n",
       "      <td>0.172319</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.021285</td>\n",
       "      <td>0.010364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.396200</td>\n",
       "      <td>2.369962</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>0.012163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.375500</td>\n",
       "      <td>2.351939</td>\n",
       "      <td>0.190651</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.024370</td>\n",
       "      <td>0.011993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>2.336093</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.345200</td>\n",
       "      <td>2.321528</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.331900</td>\n",
       "      <td>2.308127</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.319500</td>\n",
       "      <td>2.295583</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.308600</td>\n",
       "      <td>2.283989</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sat Oct 12 13:56:14 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 14:39:30,372] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 with params: {'learning_rate': 0.00040386930502167296, 'weight_decay': 0.003, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.39 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.189500</td>\n",
       "      <td>1.912197</td>\n",
       "      <td>0.402383</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.065080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.722900</td>\n",
       "      <td>1.504570</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.175663</td>\n",
       "      <td>0.178224</td>\n",
       "      <td>0.160821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.354200</td>\n",
       "      <td>1.212899</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.215680</td>\n",
       "      <td>0.211411</td>\n",
       "      <td>0.192431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>1.032739</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.258796</td>\n",
       "      <td>0.283526</td>\n",
       "      <td>0.261650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.279132</td>\n",
       "      <td>0.310125</td>\n",
       "      <td>0.282158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.829004</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.336457</td>\n",
       "      <td>0.336827</td>\n",
       "      <td>0.319486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.802715</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.372123</td>\n",
       "      <td>0.357585</td>\n",
       "      <td>0.343167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.757456</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.403321</td>\n",
       "      <td>0.396305</td>\n",
       "      <td>0.381835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.734136</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.449472</td>\n",
       "      <td>0.416486</td>\n",
       "      <td>0.409643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.720903</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.487970</td>\n",
       "      <td>0.436526</td>\n",
       "      <td>0.439056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0.700452</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.473843</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>0.444550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.674308</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.550570</td>\n",
       "      <td>0.481117</td>\n",
       "      <td>0.494407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.520528</td>\n",
       "      <td>0.500817</td>\n",
       "      <td>0.499020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.652336</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.552275</td>\n",
       "      <td>0.530280</td>\n",
       "      <td>0.531026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.652543</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.552444</td>\n",
       "      <td>0.526091</td>\n",
       "      <td>0.527042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.641740</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.546268</td>\n",
       "      <td>0.549477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.645547</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.564928</td>\n",
       "      <td>0.541411</td>\n",
       "      <td>0.543097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.644201</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.592764</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>0.559953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.644240</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.614034</td>\n",
       "      <td>0.565681</td>\n",
       "      <td>0.573238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.634055</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.616276</td>\n",
       "      <td>0.567825</td>\n",
       "      <td>0.575938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:41:20,666] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 with params: {'learning_rate': 0.00044802688590348845, 'weight_decay': 0.006, 'adam_beta1': 0.96, 'warmup_steps': 0, 'lambda_param': 0.2, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.158100</td>\n",
       "      <td>1.878799</td>\n",
       "      <td>0.406966</td>\n",
       "      <td>0.069862</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>0.071852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.698700</td>\n",
       "      <td>1.496223</td>\n",
       "      <td>0.527039</td>\n",
       "      <td>0.172609</td>\n",
       "      <td>0.166609</td>\n",
       "      <td>0.147219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.347100</td>\n",
       "      <td>1.217801</td>\n",
       "      <td>0.593951</td>\n",
       "      <td>0.225662</td>\n",
       "      <td>0.209073</td>\n",
       "      <td>0.182473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.067500</td>\n",
       "      <td>1.037081</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.245897</td>\n",
       "      <td>0.281054</td>\n",
       "      <td>0.252933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.874700</td>\n",
       "      <td>0.935401</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.277727</td>\n",
       "      <td>0.291308</td>\n",
       "      <td>0.264985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.731500</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.360447</td>\n",
       "      <td>0.341210</td>\n",
       "      <td>0.319355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.798779</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.366833</td>\n",
       "      <td>0.353885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>0.771288</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.417405</td>\n",
       "      <td>0.402997</td>\n",
       "      <td>0.390405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.415441</td>\n",
       "      <td>0.414974</td>\n",
       "      <td>0.399630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.736656</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.471663</td>\n",
       "      <td>0.438548</td>\n",
       "      <td>0.430987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.725567</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.479996</td>\n",
       "      <td>0.444948</td>\n",
       "      <td>0.440781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.696769</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.539817</td>\n",
       "      <td>0.484671</td>\n",
       "      <td>0.495318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.685428</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.547080</td>\n",
       "      <td>0.506135</td>\n",
       "      <td>0.512007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.671080</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.577476</td>\n",
       "      <td>0.526717</td>\n",
       "      <td>0.535271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.668864</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.555537</td>\n",
       "      <td>0.523997</td>\n",
       "      <td>0.526072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.663943</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.581112</td>\n",
       "      <td>0.536618</td>\n",
       "      <td>0.543737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.653266</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.579637</td>\n",
       "      <td>0.539041</td>\n",
       "      <td>0.544292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.610440</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.573033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.657547</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.622132</td>\n",
       "      <td>0.572167</td>\n",
       "      <td>0.579576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.655458</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>0.582088</td>\n",
       "      <td>0.588348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:43:09,215] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 with params: {'learning_rate': 0.00044745114736637206, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.176600</td>\n",
       "      <td>1.872992</td>\n",
       "      <td>0.414299</td>\n",
       "      <td>0.066177</td>\n",
       "      <td>0.093867</td>\n",
       "      <td>0.071857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.669000</td>\n",
       "      <td>1.443257</td>\n",
       "      <td>0.549038</td>\n",
       "      <td>0.177410</td>\n",
       "      <td>0.184497</td>\n",
       "      <td>0.164025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.279400</td>\n",
       "      <td>1.145677</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.252864</td>\n",
       "      <td>0.243475</td>\n",
       "      <td>0.226989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>0.976686</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.289110</td>\n",
       "      <td>0.298811</td>\n",
       "      <td>0.277762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.870708</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.321154</td>\n",
       "      <td>0.322077</td>\n",
       "      <td>0.300886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.801028</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.390824</td>\n",
       "      <td>0.359514</td>\n",
       "      <td>0.348164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.779485</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.418907</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>0.382116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.745497</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.427285</td>\n",
       "      <td>0.412069</td>\n",
       "      <td>0.404461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.719476</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.497513</td>\n",
       "      <td>0.437105</td>\n",
       "      <td>0.440581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.703978</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.490838</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.455830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.695665</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.503540</td>\n",
       "      <td>0.468083</td>\n",
       "      <td>0.466031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.289200</td>\n",
       "      <td>0.671441</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.520950</td>\n",
       "      <td>0.486990</td>\n",
       "      <td>0.494846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.570298</td>\n",
       "      <td>0.520950</td>\n",
       "      <td>0.528553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.651805</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.570265</td>\n",
       "      <td>0.538701</td>\n",
       "      <td>0.544191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.656616</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.634534</td>\n",
       "      <td>0.563339</td>\n",
       "      <td>0.579863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.650043</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.609151</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.569596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.654863</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.593170</td>\n",
       "      <td>0.558653</td>\n",
       "      <td>0.562239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.653857</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.614632</td>\n",
       "      <td>0.568796</td>\n",
       "      <td>0.578405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.656437</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.611509</td>\n",
       "      <td>0.587917</td>\n",
       "      <td>0.587423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.654174</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.621369</td>\n",
       "      <td>0.584234</td>\n",
       "      <td>0.588547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.655544</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>0.583617</td>\n",
       "      <td>0.589691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.653644</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.633404</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>0.596909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.653191</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.659506</td>\n",
       "      <td>0.600641</td>\n",
       "      <td>0.612329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.647212</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>0.590658</td>\n",
       "      <td>0.600240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.646799</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.624784</td>\n",
       "      <td>0.582727</td>\n",
       "      <td>0.589148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.646790</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.641103</td>\n",
       "      <td>0.602427</td>\n",
       "      <td>0.605931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.647124</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.646742</td>\n",
       "      <td>0.587219</td>\n",
       "      <td>0.598840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.648179</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.645771</td>\n",
       "      <td>0.588521</td>\n",
       "      <td>0.599950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.645643</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666377</td>\n",
       "      <td>0.601581</td>\n",
       "      <td>0.614576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.646327</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.665984</td>\n",
       "      <td>0.605581</td>\n",
       "      <td>0.617150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:45:55,559] Trial 16 finished with value: 0.6171503690738473 and parameters: {'learning_rate': 0.00044745114736637206, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 3.0}. Best is trial 16 with value: 0.6171503690738473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 with params: {'learning_rate': 0.00014978859143602985, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.91, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.328800</td>\n",
       "      <td>2.167349</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.080400</td>\n",
       "      <td>1.947633</td>\n",
       "      <td>0.406966</td>\n",
       "      <td>0.073501</td>\n",
       "      <td>0.087928</td>\n",
       "      <td>0.064695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.876600</td>\n",
       "      <td>1.738654</td>\n",
       "      <td>0.461045</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>0.121116</td>\n",
       "      <td>0.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.675200</td>\n",
       "      <td>1.561933</td>\n",
       "      <td>0.520623</td>\n",
       "      <td>0.198432</td>\n",
       "      <td>0.158285</td>\n",
       "      <td>0.137817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.516000</td>\n",
       "      <td>1.417834</td>\n",
       "      <td>0.564620</td>\n",
       "      <td>0.223026</td>\n",
       "      <td>0.190275</td>\n",
       "      <td>0.172578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>1.299616</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.241472</td>\n",
       "      <td>0.220615</td>\n",
       "      <td>0.206211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.235200</td>\n",
       "      <td>1.201646</td>\n",
       "      <td>0.649863</td>\n",
       "      <td>0.267617</td>\n",
       "      <td>0.257830</td>\n",
       "      <td>0.245458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.137900</td>\n",
       "      <td>1.123200</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.267683</td>\n",
       "      <td>0.283691</td>\n",
       "      <td>0.261954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.042100</td>\n",
       "      <td>1.058048</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.298357</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.275310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.958500</td>\n",
       "      <td>1.004148</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.304811</td>\n",
       "      <td>0.306632</td>\n",
       "      <td>0.286811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>0.958050</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.297448</td>\n",
       "      <td>0.306247</td>\n",
       "      <td>0.282578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.829900</td>\n",
       "      <td>0.926554</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.289046</td>\n",
       "      <td>0.313514</td>\n",
       "      <td>0.290315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.892077</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.297272</td>\n",
       "      <td>0.321121</td>\n",
       "      <td>0.298706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.869366</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.313048</td>\n",
       "      <td>0.327808</td>\n",
       "      <td>0.304389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.853305</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.313834</td>\n",
       "      <td>0.328338</td>\n",
       "      <td>0.306847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.839651</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.330199</td>\n",
       "      <td>0.329146</td>\n",
       "      <td>0.308453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.642100</td>\n",
       "      <td>0.826491</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.335336</td>\n",
       "      <td>0.355136</td>\n",
       "      <td>0.333379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.620200</td>\n",
       "      <td>0.812527</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>0.358265</td>\n",
       "      <td>0.338217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>0.805297</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.357540</td>\n",
       "      <td>0.337176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>0.796188</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.393876</td>\n",
       "      <td>0.372565</td>\n",
       "      <td>0.356129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.787635</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.361968</td>\n",
       "      <td>0.372617</td>\n",
       "      <td>0.355872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.778575</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.375058</td>\n",
       "      <td>0.357391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.771790</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.370805</td>\n",
       "      <td>0.379198</td>\n",
       "      <td>0.362522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.767025</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.368437</td>\n",
       "      <td>0.380310</td>\n",
       "      <td>0.363547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>0.764568</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.412884</td>\n",
       "      <td>0.402995</td>\n",
       "      <td>0.390607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.760092</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.419016</td>\n",
       "      <td>0.402911</td>\n",
       "      <td>0.391686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.410234</td>\n",
       "      <td>0.403353</td>\n",
       "      <td>0.390542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.754775</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.409788</td>\n",
       "      <td>0.402767</td>\n",
       "      <td>0.390047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.754591</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.426236</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.395610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.753723</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.405198</td>\n",
       "      <td>0.404371</td>\n",
       "      <td>0.389409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:48:38,874] Trial 17 finished with value: 0.38940911186640337 and parameters: {'learning_rate': 0.00014978859143602985, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.91, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.0}. Best is trial 16 with value: 0.6171503690738473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 with params: {'learning_rate': 0.0003070981811342446, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.248100</td>\n",
       "      <td>2.004475</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.065855</td>\n",
       "      <td>0.074486</td>\n",
       "      <td>0.060334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.841300</td>\n",
       "      <td>1.631227</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.139391</td>\n",
       "      <td>0.138703</td>\n",
       "      <td>0.114312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.501200</td>\n",
       "      <td>1.338429</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.219485</td>\n",
       "      <td>0.194688</td>\n",
       "      <td>0.177663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.226900</td>\n",
       "      <td>1.144347</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.264504</td>\n",
       "      <td>0.266697</td>\n",
       "      <td>0.251001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.028000</td>\n",
       "      <td>1.009114</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.282385</td>\n",
       "      <td>0.291071</td>\n",
       "      <td>0.266003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>0.904463</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.298279</td>\n",
       "      <td>0.313708</td>\n",
       "      <td>0.292764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>0.847441</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.321434</td>\n",
       "      <td>0.318486</td>\n",
       "      <td>0.300308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.814791</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.376385</td>\n",
       "      <td>0.362543</td>\n",
       "      <td>0.345304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>0.785707</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.387290</td>\n",
       "      <td>0.372418</td>\n",
       "      <td>0.359680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.766615</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.443191</td>\n",
       "      <td>0.393747</td>\n",
       "      <td>0.389141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>0.737787</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.449007</td>\n",
       "      <td>0.422952</td>\n",
       "      <td>0.415887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.724087</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.492876</td>\n",
       "      <td>0.415731</td>\n",
       "      <td>0.419540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.704853</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.452544</td>\n",
       "      <td>0.441342</td>\n",
       "      <td>0.435791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.693690</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.498227</td>\n",
       "      <td>0.459784</td>\n",
       "      <td>0.459377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.686776</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.489555</td>\n",
       "      <td>0.449060</td>\n",
       "      <td>0.448275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.678499</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.494470</td>\n",
       "      <td>0.483755</td>\n",
       "      <td>0.480577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.675896</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.546317</td>\n",
       "      <td>0.491674</td>\n",
       "      <td>0.496945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.667299</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>0.508183</td>\n",
       "      <td>0.513517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.663218</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.541276</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.511917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.653438</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.546661</td>\n",
       "      <td>0.522586</td>\n",
       "      <td>0.523610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.659795</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.555752</td>\n",
       "      <td>0.531887</td>\n",
       "      <td>0.534105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.656466</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.593378</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.555144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.650721</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.582790</td>\n",
       "      <td>0.543905</td>\n",
       "      <td>0.551931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>0.649920</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.598631</td>\n",
       "      <td>0.550575</td>\n",
       "      <td>0.559063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.649910</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>0.564437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.646808</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.621605</td>\n",
       "      <td>0.569666</td>\n",
       "      <td>0.581175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.645333</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.626749</td>\n",
       "      <td>0.566454</td>\n",
       "      <td>0.582787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.646387</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.620932</td>\n",
       "      <td>0.560602</td>\n",
       "      <td>0.575551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.646362</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.619021</td>\n",
       "      <td>0.569945</td>\n",
       "      <td>0.582352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.646783</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.618786</td>\n",
       "      <td>0.571392</td>\n",
       "      <td>0.583070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:51:27,636] Trial 18 finished with value: 0.5830696296591694 and parameters: {'learning_rate': 0.0003070981811342446, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 16 with value: 0.6171503690738473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 with params: {'learning_rate': 0.000264337305031989, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 4, 'lambda_param': 0.4, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:53, 6.60 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.268500</td>\n",
       "      <td>2.047209</td>\n",
       "      <td>0.303391</td>\n",
       "      <td>0.071965</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>0.045728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.901200</td>\n",
       "      <td>1.705089</td>\n",
       "      <td>0.463795</td>\n",
       "      <td>0.124434</td>\n",
       "      <td>0.128626</td>\n",
       "      <td>0.101793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>1.427365</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.173135</td>\n",
       "      <td>0.153217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.326400</td>\n",
       "      <td>1.225659</td>\n",
       "      <td>0.638863</td>\n",
       "      <td>0.248223</td>\n",
       "      <td>0.243166</td>\n",
       "      <td>0.225761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.127900</td>\n",
       "      <td>1.075925</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.272318</td>\n",
       "      <td>0.286331</td>\n",
       "      <td>0.264542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.966228</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.285276</td>\n",
       "      <td>0.296804</td>\n",
       "      <td>0.275630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.895058</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.304983</td>\n",
       "      <td>0.303086</td>\n",
       "      <td>0.282441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.736500</td>\n",
       "      <td>0.849831</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.341724</td>\n",
       "      <td>0.338228</td>\n",
       "      <td>0.316463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.817992</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.357685</td>\n",
       "      <td>0.346013</td>\n",
       "      <td>0.333335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.797321</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.377273</td>\n",
       "      <td>0.371353</td>\n",
       "      <td>0.355064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.769861</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.399668</td>\n",
       "      <td>0.399061</td>\n",
       "      <td>0.387979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.747905</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.442019</td>\n",
       "      <td>0.397078</td>\n",
       "      <td>0.391595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.729071</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.445742</td>\n",
       "      <td>0.418851</td>\n",
       "      <td>0.412517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.716317</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.478041</td>\n",
       "      <td>0.436194</td>\n",
       "      <td>0.431777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.707958</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.446169</td>\n",
       "      <td>0.434996</td>\n",
       "      <td>0.428833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.701652</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.441289</td>\n",
       "      <td>0.447828</td>\n",
       "      <td>0.436694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.694301</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.452872</td>\n",
       "      <td>0.452463</td>\n",
       "      <td>0.443995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.690384</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.497480</td>\n",
       "      <td>0.470859</td>\n",
       "      <td>0.468615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.684336</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.506589</td>\n",
       "      <td>0.473704</td>\n",
       "      <td>0.472578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.669531</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.507724</td>\n",
       "      <td>0.484857</td>\n",
       "      <td>0.480787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:53:14,378] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 with params: {'learning_rate': 0.0004703512078685806, 'weight_decay': 0.003, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:33, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.182900</td>\n",
       "      <td>1.881323</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.071544</td>\n",
       "      <td>0.093461</td>\n",
       "      <td>0.072734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.673900</td>\n",
       "      <td>1.449646</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.168236</td>\n",
       "      <td>0.190546</td>\n",
       "      <td>0.166774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.279100</td>\n",
       "      <td>1.136615</td>\n",
       "      <td>0.641613</td>\n",
       "      <td>0.241738</td>\n",
       "      <td>0.247125</td>\n",
       "      <td>0.229290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>0.961518</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.272529</td>\n",
       "      <td>0.292728</td>\n",
       "      <td>0.267577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.790900</td>\n",
       "      <td>0.869410</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.308807</td>\n",
       "      <td>0.327055</td>\n",
       "      <td>0.302417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.792474</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.401374</td>\n",
       "      <td>0.372816</td>\n",
       "      <td>0.361861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.767816</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.384548</td>\n",
       "      <td>0.380427</td>\n",
       "      <td>0.364745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>0.734059</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.421694</td>\n",
       "      <td>0.416171</td>\n",
       "      <td>0.404513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.716736</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.453611</td>\n",
       "      <td>0.442486</td>\n",
       "      <td>0.435964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.700629</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.533683</td>\n",
       "      <td>0.456711</td>\n",
       "      <td>0.461454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.683447</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.502982</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.475982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.664628</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.583784</td>\n",
       "      <td>0.519421</td>\n",
       "      <td>0.532457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.667173</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.562482</td>\n",
       "      <td>0.531183</td>\n",
       "      <td>0.535349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.658876</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.599929</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.565210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.656532</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.554258</td>\n",
       "      <td>0.564429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.647623</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.618798</td>\n",
       "      <td>0.558310</td>\n",
       "      <td>0.572604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.646488</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.623478</td>\n",
       "      <td>0.572557</td>\n",
       "      <td>0.586048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.642726</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.654581</td>\n",
       "      <td>0.579626</td>\n",
       "      <td>0.599926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.637777</td>\n",
       "      <td>0.580665</td>\n",
       "      <td>0.594518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.646735</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.633583</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.602455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.648616</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.632262</td>\n",
       "      <td>0.592643</td>\n",
       "      <td>0.600425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.649742</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>0.594273</td>\n",
       "      <td>0.601139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.651626</td>\n",
       "      <td>0.598773</td>\n",
       "      <td>0.610718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.640216</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.649383</td>\n",
       "      <td>0.595949</td>\n",
       "      <td>0.606056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.639223</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.656687</td>\n",
       "      <td>0.599887</td>\n",
       "      <td>0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.634305</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.677464</td>\n",
       "      <td>0.622391</td>\n",
       "      <td>0.633713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.634393</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.679845</td>\n",
       "      <td>0.619483</td>\n",
       "      <td>0.630979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.638929</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.660761</td>\n",
       "      <td>0.597516</td>\n",
       "      <td>0.609050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>0.635964</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.680222</td>\n",
       "      <td>0.620214</td>\n",
       "      <td>0.631288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.635932</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.678777</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>0.629487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:55:49,121] Trial 20 finished with value: 0.6294873476522568 and parameters: {'learning_rate': 0.0004703512078685806, 'weight_decay': 0.003, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 3.0}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 with params: {'learning_rate': 0.00014626977079125038, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.2, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:31 < 02:38, 5.52 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.332200</td>\n",
       "      <td>2.175040</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.091400</td>\n",
       "      <td>1.963308</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>0.055296</td>\n",
       "      <td>0.085993</td>\n",
       "      <td>0.063656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.895400</td>\n",
       "      <td>1.760436</td>\n",
       "      <td>0.460128</td>\n",
       "      <td>0.099223</td>\n",
       "      <td>0.120120</td>\n",
       "      <td>0.092065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.698100</td>\n",
       "      <td>1.586834</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.160261</td>\n",
       "      <td>0.152523</td>\n",
       "      <td>0.132463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.441257</td>\n",
       "      <td>0.558203</td>\n",
       "      <td>0.223298</td>\n",
       "      <td>0.187559</td>\n",
       "      <td>0.169601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:56:21,347] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 with params: {'learning_rate': 0.00022538974253837323, 'weight_decay': 0.005, 'adam_beta1': 0.96, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:59 < 01:59, 5.86 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.295600</td>\n",
       "      <td>2.106008</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.003900</td>\n",
       "      <td>1.855937</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.065532</td>\n",
       "      <td>0.096099</td>\n",
       "      <td>0.071466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.763400</td>\n",
       "      <td>1.616804</td>\n",
       "      <td>0.481210</td>\n",
       "      <td>0.120112</td>\n",
       "      <td>0.139375</td>\n",
       "      <td>0.111927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.521400</td>\n",
       "      <td>1.411612</td>\n",
       "      <td>0.548121</td>\n",
       "      <td>0.167689</td>\n",
       "      <td>0.179463</td>\n",
       "      <td>0.154941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.331300</td>\n",
       "      <td>1.255288</td>\n",
       "      <td>0.614115</td>\n",
       "      <td>0.234816</td>\n",
       "      <td>0.224380</td>\n",
       "      <td>0.200843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.156100</td>\n",
       "      <td>1.127644</td>\n",
       "      <td>0.661778</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>0.258756</td>\n",
       "      <td>0.239554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.017100</td>\n",
       "      <td>1.036859</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.255745</td>\n",
       "      <td>0.284787</td>\n",
       "      <td>0.259815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>0.961436</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.300546</td>\n",
       "      <td>0.302714</td>\n",
       "      <td>0.280644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>0.317782</td>\n",
       "      <td>0.290440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.875065</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.348062</td>\n",
       "      <td>0.339291</td>\n",
       "      <td>0.320624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:57:21,683] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 with params: {'learning_rate': 0.00025168387465058866, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 2, 'lambda_param': 0.4, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:43 < 00:51, 6.76 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.266500</td>\n",
       "      <td>2.059134</td>\n",
       "      <td>0.256645</td>\n",
       "      <td>0.075323</td>\n",
       "      <td>0.041947</td>\n",
       "      <td>0.034353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.928700</td>\n",
       "      <td>1.747852</td>\n",
       "      <td>0.454629</td>\n",
       "      <td>0.105419</td>\n",
       "      <td>0.122605</td>\n",
       "      <td>0.097452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.640400</td>\n",
       "      <td>1.485167</td>\n",
       "      <td>0.528873</td>\n",
       "      <td>0.160437</td>\n",
       "      <td>0.160313</td>\n",
       "      <td>0.139530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.383200</td>\n",
       "      <td>1.281728</td>\n",
       "      <td>0.593034</td>\n",
       "      <td>0.223821</td>\n",
       "      <td>0.207782</td>\n",
       "      <td>0.185318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>1.129622</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.259954</td>\n",
       "      <td>0.258891</td>\n",
       "      <td>0.240187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.017200</td>\n",
       "      <td>1.013289</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.264813</td>\n",
       "      <td>0.288376</td>\n",
       "      <td>0.265713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>0.933952</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.284320</td>\n",
       "      <td>0.294518</td>\n",
       "      <td>0.271468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.879860</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.340896</td>\n",
       "      <td>0.330525</td>\n",
       "      <td>0.308038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.844064</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.332698</td>\n",
       "      <td>0.333653</td>\n",
       "      <td>0.313950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.820824</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.359444</td>\n",
       "      <td>0.363415</td>\n",
       "      <td>0.341492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.789470</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.418270</td>\n",
       "      <td>0.378877</td>\n",
       "      <td>0.368798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.406573</td>\n",
       "      <td>0.387839</td>\n",
       "      <td>0.378493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.746094</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.429421</td>\n",
       "      <td>0.410577</td>\n",
       "      <td>0.400122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.729554</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.440268</td>\n",
       "      <td>0.423033</td>\n",
       "      <td>0.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.723759</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.458675</td>\n",
       "      <td>0.431938</td>\n",
       "      <td>0.423910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.714421</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.454151</td>\n",
       "      <td>0.437054</td>\n",
       "      <td>0.428411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.707330</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.454286</td>\n",
       "      <td>0.443866</td>\n",
       "      <td>0.436182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.702212</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.497522</td>\n",
       "      <td>0.454643</td>\n",
       "      <td>0.453740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.689962</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.534555</td>\n",
       "      <td>0.470326</td>\n",
       "      <td>0.470067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.682909</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.509605</td>\n",
       "      <td>0.473527</td>\n",
       "      <td>0.469541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:59:06,077] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 with params: {'learning_rate': 0.00012991083381890363, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:49 < 01:40, 6.99 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.346800</td>\n",
       "      <td>2.201475</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.124400</td>\n",
       "      <td>2.004223</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.082081</td>\n",
       "      <td>0.064312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.946000</td>\n",
       "      <td>1.819910</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.077669</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>0.079687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.763900</td>\n",
       "      <td>1.655985</td>\n",
       "      <td>0.491292</td>\n",
       "      <td>0.118141</td>\n",
       "      <td>0.137178</td>\n",
       "      <td>0.110440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.615100</td>\n",
       "      <td>1.513076</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.190479</td>\n",
       "      <td>0.173359</td>\n",
       "      <td>0.155666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.467500</td>\n",
       "      <td>1.397444</td>\n",
       "      <td>0.571036</td>\n",
       "      <td>0.238430</td>\n",
       "      <td>0.195605</td>\n",
       "      <td>0.179421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.346800</td>\n",
       "      <td>1.301489</td>\n",
       "      <td>0.615032</td>\n",
       "      <td>0.247603</td>\n",
       "      <td>0.226896</td>\n",
       "      <td>0.214189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.251300</td>\n",
       "      <td>1.220818</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.262886</td>\n",
       "      <td>0.264101</td>\n",
       "      <td>0.248227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.157800</td>\n",
       "      <td>1.150230</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.263999</td>\n",
       "      <td>0.269793</td>\n",
       "      <td>0.252103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.074700</td>\n",
       "      <td>1.093551</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.285424</td>\n",
       "      <td>0.289981</td>\n",
       "      <td>0.267063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 14:59:56,915] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 with params: {'learning_rate': 5.945016510735224e-05, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:53, 6.16 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.402700</td>\n",
       "      <td>2.309586</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.013545</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.269700</td>\n",
       "      <td>2.193073</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.168900</td>\n",
       "      <td>2.088624</td>\n",
       "      <td>0.341888</td>\n",
       "      <td>0.068436</td>\n",
       "      <td>0.067685</td>\n",
       "      <td>0.059023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.069200</td>\n",
       "      <td>1.993056</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.054288</td>\n",
       "      <td>0.086837</td>\n",
       "      <td>0.064371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.984500</td>\n",
       "      <td>1.902678</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>0.087118</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>0.075095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.890800</td>\n",
       "      <td>1.819814</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.113884</td>\n",
       "      <td>0.092054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.811400</td>\n",
       "      <td>1.748590</td>\n",
       "      <td>0.470211</td>\n",
       "      <td>0.102621</td>\n",
       "      <td>0.123013</td>\n",
       "      <td>0.100457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.746300</td>\n",
       "      <td>1.680580</td>\n",
       "      <td>0.493126</td>\n",
       "      <td>0.100981</td>\n",
       "      <td>0.136901</td>\n",
       "      <td>0.109767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.676600</td>\n",
       "      <td>1.618527</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.120765</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.116148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.617100</td>\n",
       "      <td>1.566772</td>\n",
       "      <td>0.519707</td>\n",
       "      <td>0.179309</td>\n",
       "      <td>0.155580</td>\n",
       "      <td>0.134637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:00:54,446] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 with params: {'learning_rate': 7.27361096702708e-05, 'weight_decay': 0.01, 'adam_beta1': 0.93, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:03, 7.10 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.386500</td>\n",
       "      <td>2.286181</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.239800</td>\n",
       "      <td>2.157298</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.021060</td>\n",
       "      <td>0.008081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.127900</td>\n",
       "      <td>2.042118</td>\n",
       "      <td>0.377635</td>\n",
       "      <td>0.059867</td>\n",
       "      <td>0.078444</td>\n",
       "      <td>0.062276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.015900</td>\n",
       "      <td>1.933366</td>\n",
       "      <td>0.414299</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.090509</td>\n",
       "      <td>0.066806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.918400</td>\n",
       "      <td>1.830871</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.103792</td>\n",
       "      <td>0.106779</td>\n",
       "      <td>0.081774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:01:19,799] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 with params: {'learning_rate': 0.00037773988301362265, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:59, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.201500</td>\n",
       "      <td>1.919576</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.074290</td>\n",
       "      <td>0.087447</td>\n",
       "      <td>0.065358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.732100</td>\n",
       "      <td>1.509305</td>\n",
       "      <td>0.525206</td>\n",
       "      <td>0.161807</td>\n",
       "      <td>0.160611</td>\n",
       "      <td>0.140585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.359600</td>\n",
       "      <td>1.208508</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.227563</td>\n",
       "      <td>0.238963</td>\n",
       "      <td>0.218203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.081300</td>\n",
       "      <td>1.033295</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.267526</td>\n",
       "      <td>0.287314</td>\n",
       "      <td>0.265539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>0.919691</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.324485</td>\n",
       "      <td>0.324443</td>\n",
       "      <td>0.302739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.835215</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.343960</td>\n",
       "      <td>0.341180</td>\n",
       "      <td>0.324203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.801648</td>\n",
       "      <td>0.727773</td>\n",
       "      <td>0.361268</td>\n",
       "      <td>0.353974</td>\n",
       "      <td>0.338887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.768894</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.377848</td>\n",
       "      <td>0.391309</td>\n",
       "      <td>0.370807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.738066</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.456230</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.719961</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.463213</td>\n",
       "      <td>0.426965</td>\n",
       "      <td>0.424769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.695382</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.453664</td>\n",
       "      <td>0.439674</td>\n",
       "      <td>0.429284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>0.691017</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>0.455345</td>\n",
       "      <td>0.462234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.679815</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.529883</td>\n",
       "      <td>0.474386</td>\n",
       "      <td>0.482760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.665090</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.528067</td>\n",
       "      <td>0.496257</td>\n",
       "      <td>0.498069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.668039</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.536871</td>\n",
       "      <td>0.502087</td>\n",
       "      <td>0.507773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.652465</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.571598</td>\n",
       "      <td>0.521328</td>\n",
       "      <td>0.529226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.654756</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.586324</td>\n",
       "      <td>0.533145</td>\n",
       "      <td>0.546072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.645252</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.610958</td>\n",
       "      <td>0.544925</td>\n",
       "      <td>0.559324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.649271</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.605671</td>\n",
       "      <td>0.554223</td>\n",
       "      <td>0.564323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.643742</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.590991</td>\n",
       "      <td>0.546546</td>\n",
       "      <td>0.553289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.650819</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.595507</td>\n",
       "      <td>0.552102</td>\n",
       "      <td>0.559344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>0.555658</td>\n",
       "      <td>0.563219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.639626</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.606237</td>\n",
       "      <td>0.561553</td>\n",
       "      <td>0.570975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.636115</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.595634</td>\n",
       "      <td>0.561606</td>\n",
       "      <td>0.564821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.637283</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.620726</td>\n",
       "      <td>0.575154</td>\n",
       "      <td>0.582586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.632844</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.620655</td>\n",
       "      <td>0.579326</td>\n",
       "      <td>0.586095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.635969</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.631114</td>\n",
       "      <td>0.570499</td>\n",
       "      <td>0.585511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.637762</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.629015</td>\n",
       "      <td>0.572691</td>\n",
       "      <td>0.585044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.635632</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.624017</td>\n",
       "      <td>0.572341</td>\n",
       "      <td>0.583151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.636179</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.623141</td>\n",
       "      <td>0.572192</td>\n",
       "      <td>0.582471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:04:21,321] Trial 27 finished with value: 0.582470702602652 and parameters: {'learning_rate': 0.00037773988301362265, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 3.0}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 with params: {'learning_rate': 0.0004811283747798316, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.158200</td>\n",
       "      <td>1.839428</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.060537</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>0.073987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.632000</td>\n",
       "      <td>1.409088</td>\n",
       "      <td>0.567369</td>\n",
       "      <td>0.195582</td>\n",
       "      <td>0.197484</td>\n",
       "      <td>0.175816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.235000</td>\n",
       "      <td>1.111772</td>\n",
       "      <td>0.659028</td>\n",
       "      <td>0.236423</td>\n",
       "      <td>0.264373</td>\n",
       "      <td>0.242422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.957900</td>\n",
       "      <td>0.944991</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>0.297635</td>\n",
       "      <td>0.273314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.765400</td>\n",
       "      <td>0.856692</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.337835</td>\n",
       "      <td>0.331078</td>\n",
       "      <td>0.307004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>0.792356</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.413014</td>\n",
       "      <td>0.381887</td>\n",
       "      <td>0.369676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.755292</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.405232</td>\n",
       "      <td>0.391939</td>\n",
       "      <td>0.382172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.730963</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.433994</td>\n",
       "      <td>0.417328</td>\n",
       "      <td>0.410832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.709576</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.476916</td>\n",
       "      <td>0.444204</td>\n",
       "      <td>0.442372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.689811</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.513569</td>\n",
       "      <td>0.475085</td>\n",
       "      <td>0.482068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.683361</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.493355</td>\n",
       "      <td>0.472773</td>\n",
       "      <td>0.468276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.657770</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.620461</td>\n",
       "      <td>0.526825</td>\n",
       "      <td>0.549920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.652199</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.602911</td>\n",
       "      <td>0.546134</td>\n",
       "      <td>0.555980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.647235</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.623636</td>\n",
       "      <td>0.582676</td>\n",
       "      <td>0.590531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.642081</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.617417</td>\n",
       "      <td>0.570980</td>\n",
       "      <td>0.583697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.648647</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.640529</td>\n",
       "      <td>0.583009</td>\n",
       "      <td>0.596412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.644868</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.623565</td>\n",
       "      <td>0.575381</td>\n",
       "      <td>0.585502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.648186</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.599949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.645751</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.665517</td>\n",
       "      <td>0.593714</td>\n",
       "      <td>0.612417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.641513</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.663327</td>\n",
       "      <td>0.596409</td>\n",
       "      <td>0.612583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.646372</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.659499</td>\n",
       "      <td>0.604865</td>\n",
       "      <td>0.616987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.647056</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.655082</td>\n",
       "      <td>0.610373</td>\n",
       "      <td>0.617266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.645927</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.658067</td>\n",
       "      <td>0.606768</td>\n",
       "      <td>0.616882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.637948</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.651498</td>\n",
       "      <td>0.597665</td>\n",
       "      <td>0.607542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.637053</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.665290</td>\n",
       "      <td>0.607198</td>\n",
       "      <td>0.617386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.638090</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.654674</td>\n",
       "      <td>0.608354</td>\n",
       "      <td>0.615579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.638520</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.665115</td>\n",
       "      <td>0.606229</td>\n",
       "      <td>0.617799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.640934</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.657096</td>\n",
       "      <td>0.603692</td>\n",
       "      <td>0.612964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.638457</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.658418</td>\n",
       "      <td>0.603714</td>\n",
       "      <td>0.613635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.638313</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.658418</td>\n",
       "      <td>0.603714</td>\n",
       "      <td>0.613635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:06:58,612] Trial 28 finished with value: 0.6136352532720691 and parameters: {'learning_rate': 0.0004811283747798316, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 with params: {'learning_rate': 0.0003574785066376213, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:31, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.208900</td>\n",
       "      <td>1.955327</td>\n",
       "      <td>0.387718</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.081311</td>\n",
       "      <td>0.063889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.780100</td>\n",
       "      <td>1.573041</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.137813</td>\n",
       "      <td>0.148034</td>\n",
       "      <td>0.125431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.430700</td>\n",
       "      <td>1.280717</td>\n",
       "      <td>0.575619</td>\n",
       "      <td>0.217459</td>\n",
       "      <td>0.194735</td>\n",
       "      <td>0.177747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.154900</td>\n",
       "      <td>1.095101</td>\n",
       "      <td>0.665445</td>\n",
       "      <td>0.254524</td>\n",
       "      <td>0.270203</td>\n",
       "      <td>0.251006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.959300</td>\n",
       "      <td>0.968116</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.279773</td>\n",
       "      <td>0.296856</td>\n",
       "      <td>0.272060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.863995</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.313398</td>\n",
       "      <td>0.321879</td>\n",
       "      <td>0.300526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.824540</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.326612</td>\n",
       "      <td>0.313352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.789045</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.381785</td>\n",
       "      <td>0.386977</td>\n",
       "      <td>0.368287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.761002</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.409488</td>\n",
       "      <td>0.400422</td>\n",
       "      <td>0.388129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.746376</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.450909</td>\n",
       "      <td>0.411853</td>\n",
       "      <td>0.407101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.712864</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.452973</td>\n",
       "      <td>0.435389</td>\n",
       "      <td>0.428733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.694724</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.531630</td>\n",
       "      <td>0.456079</td>\n",
       "      <td>0.462558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.694348</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.530298</td>\n",
       "      <td>0.478294</td>\n",
       "      <td>0.480234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.674971</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>0.502504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.660889</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.538007</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.506422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.657218</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.546016</td>\n",
       "      <td>0.511336</td>\n",
       "      <td>0.511575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.574403</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.525030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.234300</td>\n",
       "      <td>0.649344</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.565328</td>\n",
       "      <td>0.540548</td>\n",
       "      <td>0.543740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.651012</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.595834</td>\n",
       "      <td>0.550477</td>\n",
       "      <td>0.559275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.640044</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.596639</td>\n",
       "      <td>0.563139</td>\n",
       "      <td>0.569547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.643132</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.666155</td>\n",
       "      <td>0.581573</td>\n",
       "      <td>0.604672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.637898</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.640277</td>\n",
       "      <td>0.581899</td>\n",
       "      <td>0.597007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.638440</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.640226</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.598185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.638627</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.622190</td>\n",
       "      <td>0.577089</td>\n",
       "      <td>0.587482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.637727</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.596934</td>\n",
       "      <td>0.614423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.633531</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.671754</td>\n",
       "      <td>0.604041</td>\n",
       "      <td>0.619922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.668657</td>\n",
       "      <td>0.591904</td>\n",
       "      <td>0.611469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.636473</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.669379</td>\n",
       "      <td>0.594395</td>\n",
       "      <td>0.612669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.635768</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.666881</td>\n",
       "      <td>0.599522</td>\n",
       "      <td>0.616153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.668450</td>\n",
       "      <td>0.599522</td>\n",
       "      <td>0.616638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:09:31,479] Trial 29 finished with value: 0.616637828866353 and parameters: {'learning_rate': 0.0003574785066376213, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 with params: {'learning_rate': 0.000495742016232896, 'weight_decay': 0.01, 'adam_beta1': 0.98, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:54, 6.12 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.175700</td>\n",
       "      <td>1.907694</td>\n",
       "      <td>0.351971</td>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.068161</td>\n",
       "      <td>0.049588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.731200</td>\n",
       "      <td>1.545450</td>\n",
       "      <td>0.483960</td>\n",
       "      <td>0.153409</td>\n",
       "      <td>0.138574</td>\n",
       "      <td>0.118088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.400500</td>\n",
       "      <td>1.274576</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.174506</td>\n",
       "      <td>0.199053</td>\n",
       "      <td>0.170220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>1.112073</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.251127</td>\n",
       "      <td>0.260151</td>\n",
       "      <td>0.233190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.951900</td>\n",
       "      <td>0.992843</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.264221</td>\n",
       "      <td>0.285219</td>\n",
       "      <td>0.256607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.912143</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.299377</td>\n",
       "      <td>0.317229</td>\n",
       "      <td>0.289412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.867322</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.322779</td>\n",
       "      <td>0.338258</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.820669</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.398692</td>\n",
       "      <td>0.375550</td>\n",
       "      <td>0.361871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.799606</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.387691</td>\n",
       "      <td>0.393879</td>\n",
       "      <td>0.377626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.780482</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.426309</td>\n",
       "      <td>0.394684</td>\n",
       "      <td>0.387242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:10:29,259] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 with params: {'learning_rate': 0.0002391890808390922, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:43 < 00:51, 6.77 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>2.059330</td>\n",
       "      <td>0.277727</td>\n",
       "      <td>0.073152</td>\n",
       "      <td>0.048640</td>\n",
       "      <td>0.039475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.927900</td>\n",
       "      <td>1.746854</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>0.101974</td>\n",
       "      <td>0.123063</td>\n",
       "      <td>0.097628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.641300</td>\n",
       "      <td>1.485061</td>\n",
       "      <td>0.525206</td>\n",
       "      <td>0.194521</td>\n",
       "      <td>0.156092</td>\n",
       "      <td>0.135101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.389800</td>\n",
       "      <td>1.284471</td>\n",
       "      <td>0.604033</td>\n",
       "      <td>0.245894</td>\n",
       "      <td>0.215469</td>\n",
       "      <td>0.195235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.197200</td>\n",
       "      <td>1.133675</td>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.259603</td>\n",
       "      <td>0.269763</td>\n",
       "      <td>0.249835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.026600</td>\n",
       "      <td>1.019129</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.259750</td>\n",
       "      <td>0.290295</td>\n",
       "      <td>0.264877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.893700</td>\n",
       "      <td>0.939716</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.276167</td>\n",
       "      <td>0.295479</td>\n",
       "      <td>0.272935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>0.884816</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.337012</td>\n",
       "      <td>0.326242</td>\n",
       "      <td>0.304704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.848401</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.313881</td>\n",
       "      <td>0.326383</td>\n",
       "      <td>0.306577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.330932</td>\n",
       "      <td>0.352375</td>\n",
       "      <td>0.330558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>0.796123</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.383468</td>\n",
       "      <td>0.371933</td>\n",
       "      <td>0.357799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>0.771245</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.403555</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.369283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.750068</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.450578</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>0.408406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.737173</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.466455</td>\n",
       "      <td>0.428484</td>\n",
       "      <td>0.419575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.725617</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.437817</td>\n",
       "      <td>0.418903</td>\n",
       "      <td>0.410448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.719702</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.445437</td>\n",
       "      <td>0.435416</td>\n",
       "      <td>0.424004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.455362</td>\n",
       "      <td>0.429747</td>\n",
       "      <td>0.420957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.702003</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.509875</td>\n",
       "      <td>0.449854</td>\n",
       "      <td>0.448989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>0.694624</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.513168</td>\n",
       "      <td>0.468108</td>\n",
       "      <td>0.466344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.686145</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.469679</td>\n",
       "      <td>0.466897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:12:13,376] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 with params: {'learning_rate': 0.00048589250169561336, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:37, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.163400</td>\n",
       "      <td>1.855191</td>\n",
       "      <td>0.419798</td>\n",
       "      <td>0.063847</td>\n",
       "      <td>0.097807</td>\n",
       "      <td>0.073259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.656100</td>\n",
       "      <td>1.434741</td>\n",
       "      <td>0.554537</td>\n",
       "      <td>0.181122</td>\n",
       "      <td>0.188179</td>\n",
       "      <td>0.167339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.272600</td>\n",
       "      <td>1.149577</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.231441</td>\n",
       "      <td>0.246418</td>\n",
       "      <td>0.227026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.995900</td>\n",
       "      <td>0.979448</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.286872</td>\n",
       "      <td>0.259573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.287740</td>\n",
       "      <td>0.317747</td>\n",
       "      <td>0.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.650700</td>\n",
       "      <td>0.811477</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.399406</td>\n",
       "      <td>0.367319</td>\n",
       "      <td>0.352754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>0.773827</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.395374</td>\n",
       "      <td>0.390797</td>\n",
       "      <td>0.375991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.756645</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.431694</td>\n",
       "      <td>0.420218</td>\n",
       "      <td>0.409411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.722642</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.467032</td>\n",
       "      <td>0.443916</td>\n",
       "      <td>0.436320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.356700</td>\n",
       "      <td>0.712446</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.471444</td>\n",
       "      <td>0.461243</td>\n",
       "      <td>0.456663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.693883</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.503431</td>\n",
       "      <td>0.483778</td>\n",
       "      <td>0.479067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.670740</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>0.517338</td>\n",
       "      <td>0.535695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>0.661441</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.603673</td>\n",
       "      <td>0.544301</td>\n",
       "      <td>0.557905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.658564</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.617787</td>\n",
       "      <td>0.572852</td>\n",
       "      <td>0.580443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>0.660056</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.608331</td>\n",
       "      <td>0.574358</td>\n",
       "      <td>0.577156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.189900</td>\n",
       "      <td>0.656013</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.644262</td>\n",
       "      <td>0.586218</td>\n",
       "      <td>0.600481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.646686</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.627371</td>\n",
       "      <td>0.567937</td>\n",
       "      <td>0.579618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.640537</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.628962</td>\n",
       "      <td>0.582531</td>\n",
       "      <td>0.592178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.646539</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.649313</td>\n",
       "      <td>0.590110</td>\n",
       "      <td>0.605081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.647975</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.638266</td>\n",
       "      <td>0.586270</td>\n",
       "      <td>0.596494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.649691</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.658767</td>\n",
       "      <td>0.596363</td>\n",
       "      <td>0.610598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.645650</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.653853</td>\n",
       "      <td>0.609643</td>\n",
       "      <td>0.615943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.646282</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.656596</td>\n",
       "      <td>0.591382</td>\n",
       "      <td>0.605812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.638972</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.640617</td>\n",
       "      <td>0.591067</td>\n",
       "      <td>0.598513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.642209</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.647395</td>\n",
       "      <td>0.601782</td>\n",
       "      <td>0.607772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.638331</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.648856</td>\n",
       "      <td>0.599834</td>\n",
       "      <td>0.607970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.638931</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.593879</td>\n",
       "      <td>0.603025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.641761</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.652275</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.605485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.639960</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.665421</td>\n",
       "      <td>0.606545</td>\n",
       "      <td>0.616092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.639428</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.663811</td>\n",
       "      <td>0.605493</td>\n",
       "      <td>0.614016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:14:52,861] Trial 32 finished with value: 0.6140158009879676 and parameters: {'learning_rate': 0.00048589250169561336, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 with params: {'learning_rate': 0.00025292455107428236, 'weight_decay': 0.007, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:52, 6.64 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.263400</td>\n",
       "      <td>2.064291</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.037224</td>\n",
       "      <td>0.024263</td>\n",
       "      <td>0.012801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.945600</td>\n",
       "      <td>1.779341</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.106888</td>\n",
       "      <td>0.076997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.675300</td>\n",
       "      <td>1.524691</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.179187</td>\n",
       "      <td>0.157130</td>\n",
       "      <td>0.135383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.419200</td>\n",
       "      <td>1.315652</td>\n",
       "      <td>0.586618</td>\n",
       "      <td>0.225173</td>\n",
       "      <td>0.206215</td>\n",
       "      <td>0.181498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.221100</td>\n",
       "      <td>1.159945</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.231770</td>\n",
       "      <td>0.245056</td>\n",
       "      <td>0.221553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.046200</td>\n",
       "      <td>1.041318</td>\n",
       "      <td>0.687443</td>\n",
       "      <td>0.263557</td>\n",
       "      <td>0.287299</td>\n",
       "      <td>0.263381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>0.957401</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.278207</td>\n",
       "      <td>0.295103</td>\n",
       "      <td>0.269880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.812600</td>\n",
       "      <td>0.896533</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.283238</td>\n",
       "      <td>0.317622</td>\n",
       "      <td>0.289687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.729800</td>\n",
       "      <td>0.863033</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.350042</td>\n",
       "      <td>0.338733</td>\n",
       "      <td>0.318488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.832800</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.379407</td>\n",
       "      <td>0.362388</td>\n",
       "      <td>0.345518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.806028</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.415089</td>\n",
       "      <td>0.377550</td>\n",
       "      <td>0.364388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.542400</td>\n",
       "      <td>0.772015</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.374264</td>\n",
       "      <td>0.385144</td>\n",
       "      <td>0.369220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.754124</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.408245</td>\n",
       "      <td>0.397169</td>\n",
       "      <td>0.384349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.739224</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.425437</td>\n",
       "      <td>0.416239</td>\n",
       "      <td>0.404782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.444300</td>\n",
       "      <td>0.734369</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.445829</td>\n",
       "      <td>0.425606</td>\n",
       "      <td>0.416682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.724608</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.436092</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.414309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.383200</td>\n",
       "      <td>0.714011</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.428668</td>\n",
       "      <td>0.420737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.707291</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.454576</td>\n",
       "      <td>0.444257</td>\n",
       "      <td>0.432947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.696669</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.459369</td>\n",
       "      <td>0.449677</td>\n",
       "      <td>0.439591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.690334</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.496239</td>\n",
       "      <td>0.464784</td>\n",
       "      <td>0.458602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:16:38,933] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34 with params: {'learning_rate': 0.00024599196666744047, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:42 < 00:51, 6.79 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.269100</td>\n",
       "      <td>2.063890</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.056733</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.031769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.936600</td>\n",
       "      <td>1.758566</td>\n",
       "      <td>0.450046</td>\n",
       "      <td>0.081218</td>\n",
       "      <td>0.115771</td>\n",
       "      <td>0.087482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.653200</td>\n",
       "      <td>1.498960</td>\n",
       "      <td>0.526123</td>\n",
       "      <td>0.156592</td>\n",
       "      <td>0.158442</td>\n",
       "      <td>0.137096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.398100</td>\n",
       "      <td>1.295734</td>\n",
       "      <td>0.590284</td>\n",
       "      <td>0.202403</td>\n",
       "      <td>0.204067</td>\n",
       "      <td>0.178824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.205500</td>\n",
       "      <td>1.143494</td>\n",
       "      <td>0.655362</td>\n",
       "      <td>0.259221</td>\n",
       "      <td>0.256322</td>\n",
       "      <td>0.237824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.034100</td>\n",
       "      <td>1.026767</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.266544</td>\n",
       "      <td>0.288587</td>\n",
       "      <td>0.266374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.945801</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.275525</td>\n",
       "      <td>0.294479</td>\n",
       "      <td>0.271731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.889650</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.303043</td>\n",
       "      <td>0.324984</td>\n",
       "      <td>0.300132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>0.852781</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.308083</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.302814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.643700</td>\n",
       "      <td>0.827243</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.339380</td>\n",
       "      <td>0.362577</td>\n",
       "      <td>0.340222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.594300</td>\n",
       "      <td>0.795448</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.388696</td>\n",
       "      <td>0.372537</td>\n",
       "      <td>0.359145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.398965</td>\n",
       "      <td>0.387577</td>\n",
       "      <td>0.376851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.750607</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.430222</td>\n",
       "      <td>0.410067</td>\n",
       "      <td>0.400911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.734820</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.420529</td>\n",
       "      <td>0.416940</td>\n",
       "      <td>0.402199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.728324</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.432994</td>\n",
       "      <td>0.426108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.719364</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.476463</td>\n",
       "      <td>0.439591</td>\n",
       "      <td>0.436626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.710406</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.455624</td>\n",
       "      <td>0.438393</td>\n",
       "      <td>0.432086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.704804</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.505652</td>\n",
       "      <td>0.452688</td>\n",
       "      <td>0.451718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.693477</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.502678</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.461817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.508942</td>\n",
       "      <td>0.471565</td>\n",
       "      <td>0.467910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:18:22,765] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 with params: {'learning_rate': 0.0003375040445074918, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:57 < 01:55, 6.04 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.236000</td>\n",
       "      <td>1.999726</td>\n",
       "      <td>0.357470</td>\n",
       "      <td>0.067291</td>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.058504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.833000</td>\n",
       "      <td>1.628258</td>\n",
       "      <td>0.478460</td>\n",
       "      <td>0.115849</td>\n",
       "      <td>0.133283</td>\n",
       "      <td>0.104348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.495400</td>\n",
       "      <td>1.338022</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.219255</td>\n",
       "      <td>0.187629</td>\n",
       "      <td>0.165999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.214300</td>\n",
       "      <td>1.140303</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.263529</td>\n",
       "      <td>0.256596</td>\n",
       "      <td>0.239215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.013700</td>\n",
       "      <td>1.000111</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>0.289563</td>\n",
       "      <td>0.265560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.845900</td>\n",
       "      <td>0.895659</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.310118</td>\n",
       "      <td>0.288335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.841564</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.325829</td>\n",
       "      <td>0.318577</td>\n",
       "      <td>0.300290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>0.805309</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.383480</td>\n",
       "      <td>0.379704</td>\n",
       "      <td>0.360813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.778364</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.411405</td>\n",
       "      <td>0.388386</td>\n",
       "      <td>0.379581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.764173</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.449363</td>\n",
       "      <td>0.403306</td>\n",
       "      <td>0.394372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:19:21,591] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 with params: {'learning_rate': 0.00013004523237627598, 'weight_decay': 0.003, 'adam_beta1': 0.96, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:41 < 00:50, 6.86 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.343000</td>\n",
       "      <td>2.206457</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.137400</td>\n",
       "      <td>2.031454</td>\n",
       "      <td>0.346471</td>\n",
       "      <td>0.065020</td>\n",
       "      <td>0.069839</td>\n",
       "      <td>0.055058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.980900</td>\n",
       "      <td>1.871273</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.069282</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.071876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.816900</td>\n",
       "      <td>1.713564</td>\n",
       "      <td>0.467461</td>\n",
       "      <td>0.077353</td>\n",
       "      <td>0.123330</td>\n",
       "      <td>0.092049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.676000</td>\n",
       "      <td>1.576768</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.123451</td>\n",
       "      <td>0.144192</td>\n",
       "      <td>0.119692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.530700</td>\n",
       "      <td>1.458258</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.199402</td>\n",
       "      <td>0.181774</td>\n",
       "      <td>0.165211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.408100</td>\n",
       "      <td>1.357096</td>\n",
       "      <td>0.571036</td>\n",
       "      <td>0.240247</td>\n",
       "      <td>0.200050</td>\n",
       "      <td>0.185059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.308900</td>\n",
       "      <td>1.272723</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>0.240087</td>\n",
       "      <td>0.232754</td>\n",
       "      <td>0.214697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.214500</td>\n",
       "      <td>1.199609</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.262656</td>\n",
       "      <td>0.263618</td>\n",
       "      <td>0.242465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.127200</td>\n",
       "      <td>1.139411</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.276522</td>\n",
       "      <td>0.278479</td>\n",
       "      <td>0.257773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>1.084008</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.292404</td>\n",
       "      <td>0.285336</td>\n",
       "      <td>0.264631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.991300</td>\n",
       "      <td>1.038063</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.292907</td>\n",
       "      <td>0.293392</td>\n",
       "      <td>0.271075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.290798</td>\n",
       "      <td>0.300525</td>\n",
       "      <td>0.275974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.891800</td>\n",
       "      <td>0.961876</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.305509</td>\n",
       "      <td>0.297060</td>\n",
       "      <td>0.275657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.937354</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.307706</td>\n",
       "      <td>0.311545</td>\n",
       "      <td>0.288138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.815300</td>\n",
       "      <td>0.918558</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.301049</td>\n",
       "      <td>0.313892</td>\n",
       "      <td>0.292564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.771900</td>\n",
       "      <td>0.897785</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.300020</td>\n",
       "      <td>0.318444</td>\n",
       "      <td>0.293065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.747100</td>\n",
       "      <td>0.878336</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.290725</td>\n",
       "      <td>0.316473</td>\n",
       "      <td>0.291806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.870401</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.320448</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.301749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.857251</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.337967</td>\n",
       "      <td>0.330683</td>\n",
       "      <td>0.312219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:21:04,252] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 with params: {'learning_rate': 0.0004100230031354973, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:42, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.187100</td>\n",
       "      <td>1.906527</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.075577</td>\n",
       "      <td>0.087031</td>\n",
       "      <td>0.065124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.716000</td>\n",
       "      <td>1.496705</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>0.178961</td>\n",
       "      <td>0.161194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.345200</td>\n",
       "      <td>1.205435</td>\n",
       "      <td>0.612282</td>\n",
       "      <td>0.220383</td>\n",
       "      <td>0.217041</td>\n",
       "      <td>0.198785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.069100</td>\n",
       "      <td>1.025558</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.260177</td>\n",
       "      <td>0.285105</td>\n",
       "      <td>0.263289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>0.915346</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.280391</td>\n",
       "      <td>0.311792</td>\n",
       "      <td>0.283709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.719900</td>\n",
       "      <td>0.825270</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.337256</td>\n",
       "      <td>0.336827</td>\n",
       "      <td>0.319984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.606600</td>\n",
       "      <td>0.800136</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.372492</td>\n",
       "      <td>0.356310</td>\n",
       "      <td>0.341318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.754335</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.404954</td>\n",
       "      <td>0.398124</td>\n",
       "      <td>0.383931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.731384</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.417465</td>\n",
       "      <td>0.412077</td>\n",
       "      <td>0.400650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.717924</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.481821</td>\n",
       "      <td>0.433113</td>\n",
       "      <td>0.434770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.697102</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.452438</td>\n",
       "      <td>0.451419</td>\n",
       "      <td>0.440659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.673244</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.516209</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.484378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.676448</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.521514</td>\n",
       "      <td>0.500579</td>\n",
       "      <td>0.498921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.648871</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.549439</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.529255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.652246</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.528502</td>\n",
       "      <td>0.528467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.641365</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.576895</td>\n",
       "      <td>0.539938</td>\n",
       "      <td>0.544790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.643613</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.571151</td>\n",
       "      <td>0.544249</td>\n",
       "      <td>0.547257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.642066</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.584178</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>0.556135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.642867</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.614454</td>\n",
       "      <td>0.565193</td>\n",
       "      <td>0.573524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.634062</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.576342</td>\n",
       "      <td>0.583747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.634812</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.672688</td>\n",
       "      <td>0.606203</td>\n",
       "      <td>0.622592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.631580</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.656378</td>\n",
       "      <td>0.594876</td>\n",
       "      <td>0.607875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.631902</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.664505</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.614531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.628893</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.663862</td>\n",
       "      <td>0.593504</td>\n",
       "      <td>0.609986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.629671</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.672451</td>\n",
       "      <td>0.605581</td>\n",
       "      <td>0.621006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.629422</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.671246</td>\n",
       "      <td>0.609186</td>\n",
       "      <td>0.622084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.631007</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.672401</td>\n",
       "      <td>0.600481</td>\n",
       "      <td>0.617740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.633241</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.664766</td>\n",
       "      <td>0.593699</td>\n",
       "      <td>0.610429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.631883</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.666377</td>\n",
       "      <td>0.599309</td>\n",
       "      <td>0.613574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.631815</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.667068</td>\n",
       "      <td>0.600847</td>\n",
       "      <td>0.614623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:23:48,787] Trial 37 finished with value: 0.614622963082204 and parameters: {'learning_rate': 0.0004100230031354973, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 4.0}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38 with params: {'learning_rate': 0.00017511093165679915, 'weight_decay': 0.01, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.7000000000000001, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:20, 6.24 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.293600</td>\n",
       "      <td>2.122743</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.028100</td>\n",
       "      <td>1.887286</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>0.092274</td>\n",
       "      <td>0.067571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.806500</td>\n",
       "      <td>1.662534</td>\n",
       "      <td>0.475710</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>0.131922</td>\n",
       "      <td>0.108269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>1.475497</td>\n",
       "      <td>0.546288</td>\n",
       "      <td>0.190618</td>\n",
       "      <td>0.177814</td>\n",
       "      <td>0.159821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.419600</td>\n",
       "      <td>1.330065</td>\n",
       "      <td>0.599450</td>\n",
       "      <td>0.227214</td>\n",
       "      <td>0.209995</td>\n",
       "      <td>0.191845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:24:19,920] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39 with params: {'learning_rate': 0.0003339258256805627, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 2, 'lambda_param': 0.5, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:43, 6.79 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.233000</td>\n",
       "      <td>2.006461</td>\n",
       "      <td>0.318057</td>\n",
       "      <td>0.049174</td>\n",
       "      <td>0.058382</td>\n",
       "      <td>0.041795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.848100</td>\n",
       "      <td>1.652338</td>\n",
       "      <td>0.468378</td>\n",
       "      <td>0.143888</td>\n",
       "      <td>0.129072</td>\n",
       "      <td>0.103184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.525900</td>\n",
       "      <td>1.371607</td>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.198594</td>\n",
       "      <td>0.188365</td>\n",
       "      <td>0.163136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.245100</td>\n",
       "      <td>1.163530</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.250230</td>\n",
       "      <td>0.249512</td>\n",
       "      <td>0.229460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>1.021990</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.278879</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.263745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>0.912595</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.308355</td>\n",
       "      <td>0.285469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.733700</td>\n",
       "      <td>0.852972</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.306612</td>\n",
       "      <td>0.322769</td>\n",
       "      <td>0.300807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.647800</td>\n",
       "      <td>0.821198</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.374292</td>\n",
       "      <td>0.370735</td>\n",
       "      <td>0.348446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>0.787645</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.442449</td>\n",
       "      <td>0.389476</td>\n",
       "      <td>0.382596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.775953</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>0.407028</td>\n",
       "      <td>0.391353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:25:12,112] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 with params: {'learning_rate': 0.0004385331610554008, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:34, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.177500</td>\n",
       "      <td>1.871985</td>\n",
       "      <td>0.417965</td>\n",
       "      <td>0.066174</td>\n",
       "      <td>0.095759</td>\n",
       "      <td>0.072947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>1.438529</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.194671</td>\n",
       "      <td>0.183635</td>\n",
       "      <td>0.163847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.277600</td>\n",
       "      <td>1.143253</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.256289</td>\n",
       "      <td>0.251903</td>\n",
       "      <td>0.234548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.971853</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.269556</td>\n",
       "      <td>0.294972</td>\n",
       "      <td>0.271959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.804400</td>\n",
       "      <td>0.872184</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.327006</td>\n",
       "      <td>0.328359</td>\n",
       "      <td>0.307094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.801763</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.386129</td>\n",
       "      <td>0.363543</td>\n",
       "      <td>0.352921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.558600</td>\n",
       "      <td>0.778043</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>0.389391</td>\n",
       "      <td>0.380513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.747417</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.424519</td>\n",
       "      <td>0.415043</td>\n",
       "      <td>0.402080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.426200</td>\n",
       "      <td>0.726341</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>0.430744</td>\n",
       "      <td>0.427158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.701032</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.486353</td>\n",
       "      <td>0.434041</td>\n",
       "      <td>0.437083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.689696</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.481127</td>\n",
       "      <td>0.454769</td>\n",
       "      <td>0.448356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.669757</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.504805</td>\n",
       "      <td>0.480730</td>\n",
       "      <td>0.484288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.663656</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.571902</td>\n",
       "      <td>0.526635</td>\n",
       "      <td>0.533788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.652213</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.553284</td>\n",
       "      <td>0.541733</td>\n",
       "      <td>0.538855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.655929</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.602627</td>\n",
       "      <td>0.550230</td>\n",
       "      <td>0.559943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.650093</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.614778</td>\n",
       "      <td>0.569142</td>\n",
       "      <td>0.579502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.657007</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.615205</td>\n",
       "      <td>0.567434</td>\n",
       "      <td>0.574706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.654138</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.596844</td>\n",
       "      <td>0.577595</td>\n",
       "      <td>0.575074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.658216</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.611481</td>\n",
       "      <td>0.584558</td>\n",
       "      <td>0.583335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.650585</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.648149</td>\n",
       "      <td>0.597202</td>\n",
       "      <td>0.606334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.651022</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.617726</td>\n",
       "      <td>0.585278</td>\n",
       "      <td>0.588246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.648357</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.619012</td>\n",
       "      <td>0.586434</td>\n",
       "      <td>0.589032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.649289</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.622615</td>\n",
       "      <td>0.588474</td>\n",
       "      <td>0.591597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.647603</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.622746</td>\n",
       "      <td>0.583055</td>\n",
       "      <td>0.587727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.644709</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.625238</td>\n",
       "      <td>0.591680</td>\n",
       "      <td>0.593579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.645074</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.638596</td>\n",
       "      <td>0.601191</td>\n",
       "      <td>0.604685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.646304</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.592006</td>\n",
       "      <td>0.595374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.646951</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.625713</td>\n",
       "      <td>0.590266</td>\n",
       "      <td>0.593017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.645145</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.639579</td>\n",
       "      <td>0.600135</td>\n",
       "      <td>0.604627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.645544</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.639579</td>\n",
       "      <td>0.600135</td>\n",
       "      <td>0.604627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:27:48,178] Trial 40 finished with value: 0.6046267697590215 and parameters: {'learning_rate': 0.0004385331610554008, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 4.0}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 with params: {'learning_rate': 0.00021370315190289845, 'weight_decay': 0.008, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:34, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.270500</td>\n",
       "      <td>2.087190</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.038554</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.007825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.981000</td>\n",
       "      <td>1.829105</td>\n",
       "      <td>0.426214</td>\n",
       "      <td>0.064610</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>0.073489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.735800</td>\n",
       "      <td>1.585401</td>\n",
       "      <td>0.494042</td>\n",
       "      <td>0.147455</td>\n",
       "      <td>0.144573</td>\n",
       "      <td>0.122912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.496400</td>\n",
       "      <td>1.387401</td>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.233312</td>\n",
       "      <td>0.194909</td>\n",
       "      <td>0.175747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.312300</td>\n",
       "      <td>1.236307</td>\n",
       "      <td>0.620532</td>\n",
       "      <td>0.236118</td>\n",
       "      <td>0.227115</td>\n",
       "      <td>0.206051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.144300</td>\n",
       "      <td>1.115849</td>\n",
       "      <td>0.668194</td>\n",
       "      <td>0.256630</td>\n",
       "      <td>0.269102</td>\n",
       "      <td>0.246839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.010300</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.261488</td>\n",
       "      <td>0.284105</td>\n",
       "      <td>0.261235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>0.956376</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.282360</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.274262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.911962</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.310407</td>\n",
       "      <td>0.318774</td>\n",
       "      <td>0.295619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.741600</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.339632</td>\n",
       "      <td>0.333960</td>\n",
       "      <td>0.311798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.840902</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.347956</td>\n",
       "      <td>0.343318</td>\n",
       "      <td>0.325092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>0.811653</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.349904</td>\n",
       "      <td>0.347970</td>\n",
       "      <td>0.330156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.787958</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.388290</td>\n",
       "      <td>0.373843</td>\n",
       "      <td>0.359318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.766103</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.393710</td>\n",
       "      <td>0.379308</td>\n",
       "      <td>0.363584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.759784</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.388386</td>\n",
       "      <td>0.383077</td>\n",
       "      <td>0.368187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.757103</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.391627</td>\n",
       "      <td>0.397314</td>\n",
       "      <td>0.380054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.444454</td>\n",
       "      <td>0.411959</td>\n",
       "      <td>0.405844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.730486</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.442797</td>\n",
       "      <td>0.420931</td>\n",
       "      <td>0.413156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.724495</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.457754</td>\n",
       "      <td>0.428250</td>\n",
       "      <td>0.420614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.716027</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.448215</td>\n",
       "      <td>0.439240</td>\n",
       "      <td>0.427260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.712994</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.452896</td>\n",
       "      <td>0.434437</td>\n",
       "      <td>0.426092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.704006</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.462003</td>\n",
       "      <td>0.439507</td>\n",
       "      <td>0.432684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.699722</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.476824</td>\n",
       "      <td>0.441965</td>\n",
       "      <td>0.437706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.694692</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.458271</td>\n",
       "      <td>0.448302</td>\n",
       "      <td>0.439371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.695504</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.459280</td>\n",
       "      <td>0.454605</td>\n",
       "      <td>0.444733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.334700</td>\n",
       "      <td>0.693557</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.484854</td>\n",
       "      <td>0.464647</td>\n",
       "      <td>0.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.688776</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.477132</td>\n",
       "      <td>0.461775</td>\n",
       "      <td>0.456867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.687409</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.475671</td>\n",
       "      <td>0.461876</td>\n",
       "      <td>0.456480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.686782</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>0.461681</td>\n",
       "      <td>0.453412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.686476</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.486272</td>\n",
       "      <td>0.467088</td>\n",
       "      <td>0.461472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:30:23,986] Trial 41 finished with value: 0.4614717654188404 and parameters: {'learning_rate': 0.00021370315190289845, 'weight_decay': 0.008, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 with params: {'learning_rate': 0.00018989107351359612, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.2, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.295600</td>\n",
       "      <td>2.116091</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.016800</td>\n",
       "      <td>1.869662</td>\n",
       "      <td>0.423465</td>\n",
       "      <td>0.068031</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.782400</td>\n",
       "      <td>1.634584</td>\n",
       "      <td>0.481210</td>\n",
       "      <td>0.141559</td>\n",
       "      <td>0.136674</td>\n",
       "      <td>0.112967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.553800</td>\n",
       "      <td>1.441209</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>0.178594</td>\n",
       "      <td>0.179579</td>\n",
       "      <td>0.157470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.377300</td>\n",
       "      <td>1.292598</td>\n",
       "      <td>0.608616</td>\n",
       "      <td>0.224470</td>\n",
       "      <td>0.217350</td>\n",
       "      <td>0.196706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.213500</td>\n",
       "      <td>1.171465</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.262558</td>\n",
       "      <td>0.246507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.080900</td>\n",
       "      <td>1.079290</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.260468</td>\n",
       "      <td>0.274684</td>\n",
       "      <td>0.253222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>1.007445</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.285056</td>\n",
       "      <td>0.297488</td>\n",
       "      <td>0.274191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.890200</td>\n",
       "      <td>0.954723</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.293226</td>\n",
       "      <td>0.309011</td>\n",
       "      <td>0.287955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.909315</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.326708</td>\n",
       "      <td>0.322590</td>\n",
       "      <td>0.300801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.870744</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.311611</td>\n",
       "      <td>0.326722</td>\n",
       "      <td>0.305625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.845987</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.331109</td>\n",
       "      <td>0.334188</td>\n",
       "      <td>0.311118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.817963</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.345305</td>\n",
       "      <td>0.357134</td>\n",
       "      <td>0.337547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.798247</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.398690</td>\n",
       "      <td>0.375369</td>\n",
       "      <td>0.358199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>0.788172</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.387303</td>\n",
       "      <td>0.371923</td>\n",
       "      <td>0.355533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.780214</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.363784</td>\n",
       "      <td>0.372459</td>\n",
       "      <td>0.356308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.770457</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.386565</td>\n",
       "      <td>0.389460</td>\n",
       "      <td>0.371308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.755723</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.429529</td>\n",
       "      <td>0.396810</td>\n",
       "      <td>0.386513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.749038</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.430465</td>\n",
       "      <td>0.411433</td>\n",
       "      <td>0.401965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.741185</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.440501</td>\n",
       "      <td>0.421923</td>\n",
       "      <td>0.413834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.737275</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.459372</td>\n",
       "      <td>0.431978</td>\n",
       "      <td>0.425041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.728580</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.463976</td>\n",
       "      <td>0.432598</td>\n",
       "      <td>0.427378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>0.721931</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.461174</td>\n",
       "      <td>0.429656</td>\n",
       "      <td>0.424142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.405400</td>\n",
       "      <td>0.718187</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.453422</td>\n",
       "      <td>0.437506</td>\n",
       "      <td>0.426874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.398300</td>\n",
       "      <td>0.718134</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.458649</td>\n",
       "      <td>0.443014</td>\n",
       "      <td>0.434486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>0.716335</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.482531</td>\n",
       "      <td>0.439283</td>\n",
       "      <td>0.434326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.711818</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.476980</td>\n",
       "      <td>0.433830</td>\n",
       "      <td>0.428120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.709520</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.485543</td>\n",
       "      <td>0.437566</td>\n",
       "      <td>0.435466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.708863</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.456247</td>\n",
       "      <td>0.443395</td>\n",
       "      <td>0.434428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.708612</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.454103</td>\n",
       "      <td>0.443849</td>\n",
       "      <td>0.434140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:33:03,475] Trial 42 finished with value: 0.43413979138227565 and parameters: {'learning_rate': 0.00018989107351359612, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.2, 'temperature': 4.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 with params: {'learning_rate': 3.777515239636729e-05, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 0, 'lambda_param': 0.8, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:16, 6.40 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.418500</td>\n",
       "      <td>2.352690</td>\n",
       "      <td>0.196150</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.026013</td>\n",
       "      <td>0.013345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.330600</td>\n",
       "      <td>2.282899</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.271100</td>\n",
       "      <td>2.223492</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.214200</td>\n",
       "      <td>2.171516</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.170200</td>\n",
       "      <td>2.122084</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:33:31,505] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 with params: {'learning_rate': 0.00027505500686707344, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 1.0, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:44, 6.67 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.254100</td>\n",
       "      <td>2.034586</td>\n",
       "      <td>0.314390</td>\n",
       "      <td>0.072694</td>\n",
       "      <td>0.059835</td>\n",
       "      <td>0.048772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.887200</td>\n",
       "      <td>1.691828</td>\n",
       "      <td>0.466544</td>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.129160</td>\n",
       "      <td>0.101924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.574000</td>\n",
       "      <td>1.415198</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.170391</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.308800</td>\n",
       "      <td>1.212181</td>\n",
       "      <td>0.636114</td>\n",
       "      <td>0.245126</td>\n",
       "      <td>0.243704</td>\n",
       "      <td>0.225913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.109100</td>\n",
       "      <td>1.064486</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>0.286263</td>\n",
       "      <td>0.262830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.951684</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.311953</td>\n",
       "      <td>0.301694</td>\n",
       "      <td>0.280997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.805200</td>\n",
       "      <td>0.884142</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.304414</td>\n",
       "      <td>0.282542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.716900</td>\n",
       "      <td>0.838504</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.325698</td>\n",
       "      <td>0.343555</td>\n",
       "      <td>0.320239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641400</td>\n",
       "      <td>0.809253</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.347187</td>\n",
       "      <td>0.352324</td>\n",
       "      <td>0.337599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.793392</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.410154</td>\n",
       "      <td>0.380904</td>\n",
       "      <td>0.369901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:34:24,629] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 with params: {'learning_rate': 0.0004565536432351292, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.169900</td>\n",
       "      <td>1.867165</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>0.097592</td>\n",
       "      <td>0.074328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.668400</td>\n",
       "      <td>1.444768</td>\n",
       "      <td>0.557287</td>\n",
       "      <td>0.176562</td>\n",
       "      <td>0.190920</td>\n",
       "      <td>0.170830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.284700</td>\n",
       "      <td>1.155585</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.238682</td>\n",
       "      <td>0.247617</td>\n",
       "      <td>0.231305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.010100</td>\n",
       "      <td>0.980705</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.269716</td>\n",
       "      <td>0.291111</td>\n",
       "      <td>0.268110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.305578</td>\n",
       "      <td>0.324748</td>\n",
       "      <td>0.298472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.808517</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.415871</td>\n",
       "      <td>0.370628</td>\n",
       "      <td>0.360101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.775568</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.398092</td>\n",
       "      <td>0.385829</td>\n",
       "      <td>0.373701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.490400</td>\n",
       "      <td>0.744774</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.435448</td>\n",
       "      <td>0.409328</td>\n",
       "      <td>0.400466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.717157</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.471436</td>\n",
       "      <td>0.437779</td>\n",
       "      <td>0.433814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.704129</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.491621</td>\n",
       "      <td>0.459093</td>\n",
       "      <td>0.461709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.501109</td>\n",
       "      <td>0.473822</td>\n",
       "      <td>0.474229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.669004</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.563156</td>\n",
       "      <td>0.508056</td>\n",
       "      <td>0.523569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.663572</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.599105</td>\n",
       "      <td>0.543107</td>\n",
       "      <td>0.551773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.651209</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.606377</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.570823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.651130</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.617711</td>\n",
       "      <td>0.562938</td>\n",
       "      <td>0.574207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.643006</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.622795</td>\n",
       "      <td>0.570124</td>\n",
       "      <td>0.583003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.643857</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>0.578883</td>\n",
       "      <td>0.590657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.642259</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.647098</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.606799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.640886</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.668615</td>\n",
       "      <td>0.600529</td>\n",
       "      <td>0.617647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.633046</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.670834</td>\n",
       "      <td>0.607770</td>\n",
       "      <td>0.623277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.152100</td>\n",
       "      <td>0.641499</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.673666</td>\n",
       "      <td>0.604231</td>\n",
       "      <td>0.622082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.639154</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.659698</td>\n",
       "      <td>0.607673</td>\n",
       "      <td>0.618287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.639542</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.665473</td>\n",
       "      <td>0.600971</td>\n",
       "      <td>0.616637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.631512</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.654302</td>\n",
       "      <td>0.597662</td>\n",
       "      <td>0.607617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.635005</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.653799</td>\n",
       "      <td>0.610938</td>\n",
       "      <td>0.617164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.631809</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.656542</td>\n",
       "      <td>0.604377</td>\n",
       "      <td>0.613474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.632747</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.659369</td>\n",
       "      <td>0.604627</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.635232</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.659295</td>\n",
       "      <td>0.602055</td>\n",
       "      <td>0.612328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.633211</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>0.603263</td>\n",
       "      <td>0.614114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.633106</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.660670</td>\n",
       "      <td>0.603758</td>\n",
       "      <td>0.614362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:37:10,283] Trial 45 finished with value: 0.614362197646446 and parameters: {'learning_rate': 0.0004565536432351292, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.0}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 with params: {'learning_rate': 2.1109501932833057e-06, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0, 'lambda_param': 0.9, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:53, 6.60 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.473100</td>\n",
       "      <td>2.459247</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.002109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.464400</td>\n",
       "      <td>2.450910</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.458200</td>\n",
       "      <td>2.443428</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.002821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.452300</td>\n",
       "      <td>2.436514</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>0.004388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.442600</td>\n",
       "      <td>2.429923</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.034086</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.008170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.436000</td>\n",
       "      <td>2.423765</td>\n",
       "      <td>0.065995</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.008697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.431700</td>\n",
       "      <td>2.417798</td>\n",
       "      <td>0.088909</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.031171</td>\n",
       "      <td>0.008609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.425100</td>\n",
       "      <td>2.412005</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.008831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.420300</td>\n",
       "      <td>2.406370</td>\n",
       "      <td>0.129239</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.008865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.415400</td>\n",
       "      <td>2.400930</td>\n",
       "      <td>0.148488</td>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.009585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.409800</td>\n",
       "      <td>2.395719</td>\n",
       "      <td>0.162236</td>\n",
       "      <td>0.010382</td>\n",
       "      <td>0.019896</td>\n",
       "      <td>0.009817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.407200</td>\n",
       "      <td>2.390838</td>\n",
       "      <td>0.172319</td>\n",
       "      <td>0.012971</td>\n",
       "      <td>0.021285</td>\n",
       "      <td>0.010323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.401800</td>\n",
       "      <td>2.386329</td>\n",
       "      <td>0.175069</td>\n",
       "      <td>0.013462</td>\n",
       "      <td>0.021596</td>\n",
       "      <td>0.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.397700</td>\n",
       "      <td>2.382293</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>0.010851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.393200</td>\n",
       "      <td>2.378597</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.023521</td>\n",
       "      <td>0.011567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.390400</td>\n",
       "      <td>2.375176</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.018262</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.388300</td>\n",
       "      <td>2.372134</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.385100</td>\n",
       "      <td>2.369397</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.018227</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.381600</td>\n",
       "      <td>2.366877</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.382600</td>\n",
       "      <td>2.364612</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:38:57,142] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 with params: {'learning_rate': 0.00016690349007326933, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:03, 7.11 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.310100</td>\n",
       "      <td>2.142100</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.052900</td>\n",
       "      <td>1.918375</td>\n",
       "      <td>0.408799</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.088335</td>\n",
       "      <td>0.063921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.841800</td>\n",
       "      <td>1.701337</td>\n",
       "      <td>0.469294</td>\n",
       "      <td>0.098033</td>\n",
       "      <td>0.125154</td>\n",
       "      <td>0.093805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.629800</td>\n",
       "      <td>1.517638</td>\n",
       "      <td>0.531622</td>\n",
       "      <td>0.183059</td>\n",
       "      <td>0.165629</td>\n",
       "      <td>0.146771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.462800</td>\n",
       "      <td>1.370743</td>\n",
       "      <td>0.581118</td>\n",
       "      <td>0.225619</td>\n",
       "      <td>0.200489</td>\n",
       "      <td>0.179636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:39:22,540] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 with params: {'learning_rate': 2.1576923653516515e-06, 'weight_decay': 0.004, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.65 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.473700</td>\n",
       "      <td>2.459658</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.002109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.464700</td>\n",
       "      <td>2.451114</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.002422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.458300</td>\n",
       "      <td>2.443419</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.002813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.452200</td>\n",
       "      <td>2.436375</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.442400</td>\n",
       "      <td>2.429635</td>\n",
       "      <td>0.050412</td>\n",
       "      <td>0.034716</td>\n",
       "      <td>0.027475</td>\n",
       "      <td>0.008298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.435600</td>\n",
       "      <td>2.423330</td>\n",
       "      <td>0.067828</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>0.008795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.431200</td>\n",
       "      <td>2.417241</td>\n",
       "      <td>0.090742</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.031379</td>\n",
       "      <td>0.008602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.424400</td>\n",
       "      <td>2.411304</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>0.008781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.419600</td>\n",
       "      <td>2.405530</td>\n",
       "      <td>0.130156</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.008719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.414600</td>\n",
       "      <td>2.399975</td>\n",
       "      <td>0.151237</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.009423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.408800</td>\n",
       "      <td>2.394668</td>\n",
       "      <td>0.165903</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.009851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.406100</td>\n",
       "      <td>2.389708</td>\n",
       "      <td>0.173236</td>\n",
       "      <td>0.013286</td>\n",
       "      <td>0.021389</td>\n",
       "      <td>0.010351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.400700</td>\n",
       "      <td>2.385151</td>\n",
       "      <td>0.175985</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.010212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.396600</td>\n",
       "      <td>2.381071</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.010891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.391900</td>\n",
       "      <td>2.377325</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.023521</td>\n",
       "      <td>0.011591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.389100</td>\n",
       "      <td>2.373871</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.018010</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.387000</td>\n",
       "      <td>2.370774</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.018365</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.011611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.383700</td>\n",
       "      <td>2.368004</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.380200</td>\n",
       "      <td>2.365431</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.020808</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.381200</td>\n",
       "      <td>2.363138</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.021191</td>\n",
       "      <td>0.024002</td>\n",
       "      <td>0.012156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:41:08,500] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 with params: {'learning_rate': 0.00031126881960135896, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3, 'lambda_param': 0.4, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:46, 6.59 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.246900</td>\n",
       "      <td>2.020109</td>\n",
       "      <td>0.329056</td>\n",
       "      <td>0.070691</td>\n",
       "      <td>0.063332</td>\n",
       "      <td>0.052605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>1.664914</td>\n",
       "      <td>0.471127</td>\n",
       "      <td>0.123820</td>\n",
       "      <td>0.127455</td>\n",
       "      <td>0.098047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.540900</td>\n",
       "      <td>1.382903</td>\n",
       "      <td>0.555454</td>\n",
       "      <td>0.180588</td>\n",
       "      <td>0.180405</td>\n",
       "      <td>0.157432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.265700</td>\n",
       "      <td>1.181349</td>\n",
       "      <td>0.637947</td>\n",
       "      <td>0.244477</td>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.228481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.065800</td>\n",
       "      <td>1.036951</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.268126</td>\n",
       "      <td>0.285260</td>\n",
       "      <td>0.259928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.927195</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.312884</td>\n",
       "      <td>0.307803</td>\n",
       "      <td>0.287784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.761400</td>\n",
       "      <td>0.863462</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.326830</td>\n",
       "      <td>0.309580</td>\n",
       "      <td>0.292069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.824738</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.398202</td>\n",
       "      <td>0.370281</td>\n",
       "      <td>0.350913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.796219</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.411693</td>\n",
       "      <td>0.378125</td>\n",
       "      <td>0.369428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.778105</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.449440</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.386429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:42:02,417] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 with params: {'learning_rate': 2.356716916016174e-06, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:05, 6.98 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.472600</td>\n",
       "      <td>2.458180</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.462900</td>\n",
       "      <td>2.449006</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.002718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.456000</td>\n",
       "      <td>2.440757</td>\n",
       "      <td>0.021998</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.022734</td>\n",
       "      <td>0.003339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.449400</td>\n",
       "      <td>2.433104</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>0.006709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.439100</td>\n",
       "      <td>2.425756</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.029352</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:42:28,552] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 with params: {'learning_rate': 0.00047011879549512886, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:58, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.165100</td>\n",
       "      <td>1.856589</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>0.099312</td>\n",
       "      <td>0.074205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655600</td>\n",
       "      <td>1.432337</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.176664</td>\n",
       "      <td>0.195604</td>\n",
       "      <td>0.174050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.268800</td>\n",
       "      <td>1.142843</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.234884</td>\n",
       "      <td>0.250922</td>\n",
       "      <td>0.233238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>0.970445</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>0.292758</td>\n",
       "      <td>0.267552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>0.875214</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.305860</td>\n",
       "      <td>0.324284</td>\n",
       "      <td>0.297850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.652600</td>\n",
       "      <td>0.805339</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.402821</td>\n",
       "      <td>0.373567</td>\n",
       "      <td>0.360351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.771551</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.404028</td>\n",
       "      <td>0.389743</td>\n",
       "      <td>0.377952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.743918</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.415796</td>\n",
       "      <td>0.408416</td>\n",
       "      <td>0.395986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.471030</td>\n",
       "      <td>0.441066</td>\n",
       "      <td>0.436705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.699920</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.504526</td>\n",
       "      <td>0.467606</td>\n",
       "      <td>0.474213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.690906</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.498692</td>\n",
       "      <td>0.475070</td>\n",
       "      <td>0.473374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.668141</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.583717</td>\n",
       "      <td>0.516471</td>\n",
       "      <td>0.532520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.660496</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.603643</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.557014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.646606</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.634402</td>\n",
       "      <td>0.578466</td>\n",
       "      <td>0.591960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.650855</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.613042</td>\n",
       "      <td>0.559485</td>\n",
       "      <td>0.570790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.648333</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.646163</td>\n",
       "      <td>0.577317</td>\n",
       "      <td>0.595084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.628124</td>\n",
       "      <td>0.579686</td>\n",
       "      <td>0.591888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.643125</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.625418</td>\n",
       "      <td>0.578469</td>\n",
       "      <td>0.589061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.644563</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.673335</td>\n",
       "      <td>0.601791</td>\n",
       "      <td>0.619833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.643262</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.671454</td>\n",
       "      <td>0.604739</td>\n",
       "      <td>0.620585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.645365</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.659629</td>\n",
       "      <td>0.598565</td>\n",
       "      <td>0.613406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.639186</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.653766</td>\n",
       "      <td>0.610979</td>\n",
       "      <td>0.617073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.641757</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.658675</td>\n",
       "      <td>0.606961</td>\n",
       "      <td>0.617904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.632313</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.644534</td>\n",
       "      <td>0.602295</td>\n",
       "      <td>0.606103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.634188</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.653903</td>\n",
       "      <td>0.613113</td>\n",
       "      <td>0.616510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.633345</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.649774</td>\n",
       "      <td>0.612516</td>\n",
       "      <td>0.615666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.633930</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.654891</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>0.619494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.636715</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.650160</td>\n",
       "      <td>0.606037</td>\n",
       "      <td>0.611414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.654279</td>\n",
       "      <td>0.610087</td>\n",
       "      <td>0.614322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.635086</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.610087</td>\n",
       "      <td>0.613286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:45:28,903] Trial 51 finished with value: 0.613286271525658 and parameters: {'learning_rate': 0.00047011879549512886, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 20 with value: 0.6294873476522568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52 with params: {'learning_rate': 0.00046394057610124693, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:34, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.196700</td>\n",
       "      <td>1.906708</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.075928</td>\n",
       "      <td>0.085866</td>\n",
       "      <td>0.064499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.698400</td>\n",
       "      <td>1.473129</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.168929</td>\n",
       "      <td>0.175749</td>\n",
       "      <td>0.156523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.307700</td>\n",
       "      <td>1.164904</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.233220</td>\n",
       "      <td>0.241597</td>\n",
       "      <td>0.223325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.017500</td>\n",
       "      <td>0.986699</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.253274</td>\n",
       "      <td>0.283158</td>\n",
       "      <td>0.258737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.822100</td>\n",
       "      <td>0.880598</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.298727</td>\n",
       "      <td>0.320447</td>\n",
       "      <td>0.296374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.810475</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.420348</td>\n",
       "      <td>0.366082</td>\n",
       "      <td>0.358567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.562100</td>\n",
       "      <td>0.780860</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.389634</td>\n",
       "      <td>0.386285</td>\n",
       "      <td>0.373631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>0.756375</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.420553</td>\n",
       "      <td>0.406935</td>\n",
       "      <td>0.392873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.740054</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.482516</td>\n",
       "      <td>0.450377</td>\n",
       "      <td>0.446060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.713023</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.515950</td>\n",
       "      <td>0.459226</td>\n",
       "      <td>0.465308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.687425</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.505706</td>\n",
       "      <td>0.483050</td>\n",
       "      <td>0.480975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.667816</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.588518</td>\n",
       "      <td>0.509464</td>\n",
       "      <td>0.529667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.673547</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.573779</td>\n",
       "      <td>0.525066</td>\n",
       "      <td>0.534245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.657686</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.608041</td>\n",
       "      <td>0.560746</td>\n",
       "      <td>0.572437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.217800</td>\n",
       "      <td>0.661358</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.596239</td>\n",
       "      <td>0.551651</td>\n",
       "      <td>0.560622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.651368</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.640109</td>\n",
       "      <td>0.575562</td>\n",
       "      <td>0.592159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.644164</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.651771</td>\n",
       "      <td>0.580131</td>\n",
       "      <td>0.599360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.646111</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.668322</td>\n",
       "      <td>0.591486</td>\n",
       "      <td>0.613937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.654099</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.629543</td>\n",
       "      <td>0.577292</td>\n",
       "      <td>0.589428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.648328</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.659924</td>\n",
       "      <td>0.596320</td>\n",
       "      <td>0.611296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.649441</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.654723</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.608583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.650183</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.652025</td>\n",
       "      <td>0.594009</td>\n",
       "      <td>0.606449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.647309</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.664505</td>\n",
       "      <td>0.604541</td>\n",
       "      <td>0.621791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.646317</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.653457</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>0.603213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.641805</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.597845</td>\n",
       "      <td>0.609226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.639207</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.655774</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>0.607939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.642540</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.655037</td>\n",
       "      <td>0.600048</td>\n",
       "      <td>0.612414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.642774</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.682221</td>\n",
       "      <td>0.620486</td>\n",
       "      <td>0.635866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.640478</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.680227</td>\n",
       "      <td>0.619113</td>\n",
       "      <td>0.633629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.640560</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.680640</td>\n",
       "      <td>0.619882</td>\n",
       "      <td>0.634185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:48:05,252] Trial 52 finished with value: 0.6341848954231764 and parameters: {'learning_rate': 0.00046394057610124693, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 4.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53 with params: {'learning_rate': 1.2327552056258486e-06, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 4, 'lambda_param': 0.5, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:01, 7.18 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.475400</td>\n",
       "      <td>2.463541</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.470300</td>\n",
       "      <td>2.458344</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.004332</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.002421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.467000</td>\n",
       "      <td>2.453621</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.463400</td>\n",
       "      <td>2.449282</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.455800</td>\n",
       "      <td>2.445180</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.003240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:48:30,443] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54 with params: {'learning_rate': 0.00031862699468485545, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:33, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.247400</td>\n",
       "      <td>2.010891</td>\n",
       "      <td>0.348304</td>\n",
       "      <td>0.068160</td>\n",
       "      <td>0.069516</td>\n",
       "      <td>0.055998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.847400</td>\n",
       "      <td>1.640873</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.123403</td>\n",
       "      <td>0.129130</td>\n",
       "      <td>0.098973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.510400</td>\n",
       "      <td>1.350851</td>\n",
       "      <td>0.556370</td>\n",
       "      <td>0.217963</td>\n",
       "      <td>0.179956</td>\n",
       "      <td>0.161108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>1.152874</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.268385</td>\n",
       "      <td>0.255161</td>\n",
       "      <td>0.240235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.033500</td>\n",
       "      <td>1.014703</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.291912</td>\n",
       "      <td>0.267536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>0.904154</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.301801</td>\n",
       "      <td>0.311936</td>\n",
       "      <td>0.291371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.733800</td>\n",
       "      <td>0.848673</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.316996</td>\n",
       "      <td>0.299098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>0.811279</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.389951</td>\n",
       "      <td>0.378503</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.577100</td>\n",
       "      <td>0.783838</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.414469</td>\n",
       "      <td>0.387865</td>\n",
       "      <td>0.381502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.764023</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.441420</td>\n",
       "      <td>0.397608</td>\n",
       "      <td>0.392078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.732874</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.429160</td>\n",
       "      <td>0.419790</td>\n",
       "      <td>0.411388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>0.714324</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.463317</td>\n",
       "      <td>0.420802</td>\n",
       "      <td>0.420896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.380200</td>\n",
       "      <td>0.705641</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.461598</td>\n",
       "      <td>0.449997</td>\n",
       "      <td>0.444554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.690266</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.501742</td>\n",
       "      <td>0.462971</td>\n",
       "      <td>0.463449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.518041</td>\n",
       "      <td>0.487646</td>\n",
       "      <td>0.488608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.675656</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.515009</td>\n",
       "      <td>0.496170</td>\n",
       "      <td>0.494845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.674389</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.552796</td>\n",
       "      <td>0.505751</td>\n",
       "      <td>0.513057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.666230</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.545212</td>\n",
       "      <td>0.516017</td>\n",
       "      <td>0.519831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.664184</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.545715</td>\n",
       "      <td>0.531884</td>\n",
       "      <td>0.530729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.658852</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.535196</td>\n",
       "      <td>0.536441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.560460</td>\n",
       "      <td>0.538879</td>\n",
       "      <td>0.541369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.654422</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.610462</td>\n",
       "      <td>0.562589</td>\n",
       "      <td>0.570688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.650880</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.606588</td>\n",
       "      <td>0.557540</td>\n",
       "      <td>0.568782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.209500</td>\n",
       "      <td>0.651480</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.590409</td>\n",
       "      <td>0.550099</td>\n",
       "      <td>0.555114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.649671</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.623525</td>\n",
       "      <td>0.574642</td>\n",
       "      <td>0.585949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.647495</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.621145</td>\n",
       "      <td>0.574529</td>\n",
       "      <td>0.585528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.647293</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.619041</td>\n",
       "      <td>0.568606</td>\n",
       "      <td>0.580533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.647986</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.623802</td>\n",
       "      <td>0.570458</td>\n",
       "      <td>0.582969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.648298</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.612044</td>\n",
       "      <td>0.572542</td>\n",
       "      <td>0.580328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.648472</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.612360</td>\n",
       "      <td>0.573495</td>\n",
       "      <td>0.580951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:51:05,901] Trial 54 finished with value: 0.5809509981812864 and parameters: {'learning_rate': 0.00031862699468485545, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 5.5}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 55 with params: {'learning_rate': 0.0002211776590290286, 'weight_decay': 0.001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:49, 6.42 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.290700</td>\n",
       "      <td>2.099802</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.994100</td>\n",
       "      <td>1.841151</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.063402</td>\n",
       "      <td>0.099993</td>\n",
       "      <td>0.073748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.746200</td>\n",
       "      <td>1.596913</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.154116</td>\n",
       "      <td>0.144089</td>\n",
       "      <td>0.118867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.503200</td>\n",
       "      <td>1.393042</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.202989</td>\n",
       "      <td>0.187260</td>\n",
       "      <td>0.163633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.312300</td>\n",
       "      <td>1.234427</td>\n",
       "      <td>0.629698</td>\n",
       "      <td>0.230092</td>\n",
       "      <td>0.232378</td>\n",
       "      <td>0.211885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.138000</td>\n",
       "      <td>1.111088</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.259947</td>\n",
       "      <td>0.270097</td>\n",
       "      <td>0.248181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>1.023552</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.293673</td>\n",
       "      <td>0.287888</td>\n",
       "      <td>0.267190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.953037</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.288321</td>\n",
       "      <td>0.306546</td>\n",
       "      <td>0.283058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.910980</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.309652</td>\n",
       "      <td>0.319866</td>\n",
       "      <td>0.295177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.871638</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.342597</td>\n",
       "      <td>0.338031</td>\n",
       "      <td>0.319751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:52:01,162] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 56 with params: {'learning_rate': 0.00020759116475665268, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.299600</td>\n",
       "      <td>2.106311</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.006952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.994200</td>\n",
       "      <td>1.829888</td>\n",
       "      <td>0.432631</td>\n",
       "      <td>0.081354</td>\n",
       "      <td>0.103602</td>\n",
       "      <td>0.076969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.735000</td>\n",
       "      <td>1.580592</td>\n",
       "      <td>0.503208</td>\n",
       "      <td>0.157422</td>\n",
       "      <td>0.147772</td>\n",
       "      <td>0.127263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.494300</td>\n",
       "      <td>1.382203</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.195469</td>\n",
       "      <td>0.184390</td>\n",
       "      <td>0.163306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1.229351</td>\n",
       "      <td>0.634280</td>\n",
       "      <td>0.223860</td>\n",
       "      <td>0.232771</td>\n",
       "      <td>0.213188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.139100</td>\n",
       "      <td>1.107232</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.268652</td>\n",
       "      <td>0.275086</td>\n",
       "      <td>0.255975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.005500</td>\n",
       "      <td>1.019073</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.265669</td>\n",
       "      <td>0.288760</td>\n",
       "      <td>0.266056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>0.953289</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.303460</td>\n",
       "      <td>0.309704</td>\n",
       "      <td>0.286746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.908501</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.308766</td>\n",
       "      <td>0.311815</td>\n",
       "      <td>0.290491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.868586</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.347752</td>\n",
       "      <td>0.335567</td>\n",
       "      <td>0.316906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.838093</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.351326</td>\n",
       "      <td>0.351276</td>\n",
       "      <td>0.331042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.812729</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.345618</td>\n",
       "      <td>0.349166</td>\n",
       "      <td>0.331036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>0.788538</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.388564</td>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.358239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.771885</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.392261</td>\n",
       "      <td>0.380398</td>\n",
       "      <td>0.363010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.762328</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.415016</td>\n",
       "      <td>0.409658</td>\n",
       "      <td>0.398985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.757107</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.409512</td>\n",
       "      <td>0.399570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.746559</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.455199</td>\n",
       "      <td>0.421811</td>\n",
       "      <td>0.418404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.733622</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.450275</td>\n",
       "      <td>0.418206</td>\n",
       "      <td>0.412410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.725873</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.456558</td>\n",
       "      <td>0.432068</td>\n",
       "      <td>0.424165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.720631</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.443011</td>\n",
       "      <td>0.436524</td>\n",
       "      <td>0.424848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.717927</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.457010</td>\n",
       "      <td>0.434309</td>\n",
       "      <td>0.425876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.709369</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.457726</td>\n",
       "      <td>0.438120</td>\n",
       "      <td>0.428753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.436894</td>\n",
       "      <td>0.428595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.702688</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.466212</td>\n",
       "      <td>0.445262</td>\n",
       "      <td>0.436284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.701492</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.477242</td>\n",
       "      <td>0.449749</td>\n",
       "      <td>0.444722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.699096</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.467463</td>\n",
       "      <td>0.448985</td>\n",
       "      <td>0.440235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.695398</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.476540</td>\n",
       "      <td>0.444956</td>\n",
       "      <td>0.441058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.693971</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.479022</td>\n",
       "      <td>0.449238</td>\n",
       "      <td>0.444939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.692828</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.475906</td>\n",
       "      <td>0.451951</td>\n",
       "      <td>0.445698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>0.692653</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.480730</td>\n",
       "      <td>0.449996</td>\n",
       "      <td>0.445584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:54:50,757] Trial 56 finished with value: 0.44558377947987 and parameters: {'learning_rate': 0.00020759116475665268, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 4.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 57 with params: {'learning_rate': 0.0004883002811669377, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.172600</td>\n",
       "      <td>1.854656</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.064722</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.074990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.640800</td>\n",
       "      <td>1.412920</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.197475</td>\n",
       "      <td>0.200289</td>\n",
       "      <td>0.177581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.237200</td>\n",
       "      <td>1.106691</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.265893</td>\n",
       "      <td>0.247168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.952600</td>\n",
       "      <td>0.935316</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.274514</td>\n",
       "      <td>0.292760</td>\n",
       "      <td>0.267749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.755500</td>\n",
       "      <td>0.849225</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.341560</td>\n",
       "      <td>0.329349</td>\n",
       "      <td>0.305537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>0.782513</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.424787</td>\n",
       "      <td>0.391140</td>\n",
       "      <td>0.383643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.519500</td>\n",
       "      <td>0.759364</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.380158</td>\n",
       "      <td>0.387268</td>\n",
       "      <td>0.372013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.730024</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.449291</td>\n",
       "      <td>0.430383</td>\n",
       "      <td>0.420984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.713078</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.483829</td>\n",
       "      <td>0.450017</td>\n",
       "      <td>0.449232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.485518</td>\n",
       "      <td>0.449452</td>\n",
       "      <td>0.452243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.684690</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.539966</td>\n",
       "      <td>0.494321</td>\n",
       "      <td>0.494125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.666768</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.578832</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>0.535240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>0.662170</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.593277</td>\n",
       "      <td>0.542068</td>\n",
       "      <td>0.551322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.648752</td>\n",
       "      <td>0.807516</td>\n",
       "      <td>0.617094</td>\n",
       "      <td>0.577479</td>\n",
       "      <td>0.583319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.659362</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.622167</td>\n",
       "      <td>0.568859</td>\n",
       "      <td>0.581517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.650699</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.639084</td>\n",
       "      <td>0.590296</td>\n",
       "      <td>0.600576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.644476</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.627836</td>\n",
       "      <td>0.583202</td>\n",
       "      <td>0.593234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.642364</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.650627</td>\n",
       "      <td>0.592378</td>\n",
       "      <td>0.605173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.652704</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.632393</td>\n",
       "      <td>0.593258</td>\n",
       "      <td>0.598260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.652096</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.634518</td>\n",
       "      <td>0.595036</td>\n",
       "      <td>0.601047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.635723</td>\n",
       "      <td>0.593797</td>\n",
       "      <td>0.600669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.651105</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.643250</td>\n",
       "      <td>0.609850</td>\n",
       "      <td>0.613019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.645580</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.673050</td>\n",
       "      <td>0.629180</td>\n",
       "      <td>0.636479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.642323</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.678787</td>\n",
       "      <td>0.622481</td>\n",
       "      <td>0.633305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.641525</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.649240</td>\n",
       "      <td>0.609939</td>\n",
       "      <td>0.614569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.640142</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.665539</td>\n",
       "      <td>0.626122</td>\n",
       "      <td>0.631430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.639946</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.678579</td>\n",
       "      <td>0.615812</td>\n",
       "      <td>0.629478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.677015</td>\n",
       "      <td>0.617181</td>\n",
       "      <td>0.629450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.640678</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.676192</td>\n",
       "      <td>0.619108</td>\n",
       "      <td>0.630519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.641158</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.676593</td>\n",
       "      <td>0.622130</td>\n",
       "      <td>0.631876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 15:57:37,254] Trial 57 finished with value: 0.6318757340004337 and parameters: {'learning_rate': 0.0004883002811669377, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.2, 'temperature': 2.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 58 with params: {'learning_rate': 0.0004685055056855652, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.173500</td>\n",
       "      <td>1.851200</td>\n",
       "      <td>0.428048</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.102020</td>\n",
       "      <td>0.075680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.637100</td>\n",
       "      <td>1.403444</td>\n",
       "      <td>0.559120</td>\n",
       "      <td>0.200172</td>\n",
       "      <td>0.192342</td>\n",
       "      <td>0.172897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.231700</td>\n",
       "      <td>1.099637</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.268140</td>\n",
       "      <td>0.272504</td>\n",
       "      <td>0.250332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.948400</td>\n",
       "      <td>0.931025</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.284929</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>0.275022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.756400</td>\n",
       "      <td>0.850860</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.348787</td>\n",
       "      <td>0.335068</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.788189</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.414158</td>\n",
       "      <td>0.378695</td>\n",
       "      <td>0.370715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.759042</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.382187</td>\n",
       "      <td>0.381276</td>\n",
       "      <td>0.366510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.732605</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.450021</td>\n",
       "      <td>0.431692</td>\n",
       "      <td>0.419960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.716191</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.456639</td>\n",
       "      <td>0.435629</td>\n",
       "      <td>0.431502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.681467</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.505728</td>\n",
       "      <td>0.445914</td>\n",
       "      <td>0.453548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.677525</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.462668</td>\n",
       "      <td>0.460635</td>\n",
       "      <td>0.447898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.668925</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.551622</td>\n",
       "      <td>0.503914</td>\n",
       "      <td>0.512519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.669139</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.568357</td>\n",
       "      <td>0.518449</td>\n",
       "      <td>0.528930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.223200</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>0.552545</td>\n",
       "      <td>0.563144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.659105</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.609001</td>\n",
       "      <td>0.550615</td>\n",
       "      <td>0.565254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.651228</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.658234</td>\n",
       "      <td>0.569027</td>\n",
       "      <td>0.593758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.644381</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.646863</td>\n",
       "      <td>0.567240</td>\n",
       "      <td>0.588923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.644806</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.664523</td>\n",
       "      <td>0.586284</td>\n",
       "      <td>0.610191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.652513</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.655905</td>\n",
       "      <td>0.587263</td>\n",
       "      <td>0.606150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.646636</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.650920</td>\n",
       "      <td>0.588574</td>\n",
       "      <td>0.603658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.646938</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.645648</td>\n",
       "      <td>0.598721</td>\n",
       "      <td>0.607568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.645388</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.640797</td>\n",
       "      <td>0.595497</td>\n",
       "      <td>0.603809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.638936</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.604603</td>\n",
       "      <td>0.615728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.632554</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.671398</td>\n",
       "      <td>0.606005</td>\n",
       "      <td>0.620522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.633362</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.644739</td>\n",
       "      <td>0.600896</td>\n",
       "      <td>0.607879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.631947</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.640434</td>\n",
       "      <td>0.601502</td>\n",
       "      <td>0.607406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.632526</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.650892</td>\n",
       "      <td>0.601084</td>\n",
       "      <td>0.609938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.634823</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.646763</td>\n",
       "      <td>0.598869</td>\n",
       "      <td>0.606778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>0.634106</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.653895</td>\n",
       "      <td>0.607195</td>\n",
       "      <td>0.615688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.634782</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.652705</td>\n",
       "      <td>0.607308</td>\n",
       "      <td>0.615148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:00:18,337] Trial 58 finished with value: 0.6151483561113101 and parameters: {'learning_rate': 0.0004685055056855652, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 59 with params: {'learning_rate': 0.00024386401983372643, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 4, 'lambda_param': 0.4, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:49, 6.38 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.278000</td>\n",
       "      <td>2.064167</td>\n",
       "      <td>0.275894</td>\n",
       "      <td>0.073106</td>\n",
       "      <td>0.048794</td>\n",
       "      <td>0.040709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.925500</td>\n",
       "      <td>1.735531</td>\n",
       "      <td>0.463795</td>\n",
       "      <td>0.127788</td>\n",
       "      <td>0.127904</td>\n",
       "      <td>0.102373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.626200</td>\n",
       "      <td>1.464084</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.198380</td>\n",
       "      <td>0.165968</td>\n",
       "      <td>0.149563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.369200</td>\n",
       "      <td>1.262064</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>0.249418</td>\n",
       "      <td>0.229032</td>\n",
       "      <td>0.209777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.173700</td>\n",
       "      <td>1.110541</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.272462</td>\n",
       "      <td>0.251941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>0.999433</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.282918</td>\n",
       "      <td>0.294478</td>\n",
       "      <td>0.273294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>0.923552</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.298875</td>\n",
       "      <td>0.300409</td>\n",
       "      <td>0.280148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.873860</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.350710</td>\n",
       "      <td>0.334991</td>\n",
       "      <td>0.314510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.836666</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.320509</td>\n",
       "      <td>0.329683</td>\n",
       "      <td>0.310094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.808448</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.373112</td>\n",
       "      <td>0.364335</td>\n",
       "      <td>0.347438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:01:13,955] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 with params: {'learning_rate': 0.0003853127827969721, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:59 < 01:00, 5.82 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.209100</td>\n",
       "      <td>1.930313</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.075925</td>\n",
       "      <td>0.085512</td>\n",
       "      <td>0.063887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.738500</td>\n",
       "      <td>1.514370</td>\n",
       "      <td>0.525206</td>\n",
       "      <td>0.183462</td>\n",
       "      <td>0.166061</td>\n",
       "      <td>0.148790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.362800</td>\n",
       "      <td>1.206808</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.227444</td>\n",
       "      <td>0.241373</td>\n",
       "      <td>0.221220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.079700</td>\n",
       "      <td>1.033830</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.274586</td>\n",
       "      <td>0.284346</td>\n",
       "      <td>0.262538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.921006</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.316482</td>\n",
       "      <td>0.317240</td>\n",
       "      <td>0.296343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.728200</td>\n",
       "      <td>0.830354</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.376766</td>\n",
       "      <td>0.339757</td>\n",
       "      <td>0.325335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>0.799700</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.376815</td>\n",
       "      <td>0.359792</td>\n",
       "      <td>0.344908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.403220</td>\n",
       "      <td>0.398183</td>\n",
       "      <td>0.381425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>0.742044</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.432248</td>\n",
       "      <td>0.413187</td>\n",
       "      <td>0.406233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.720785</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.484378</td>\n",
       "      <td>0.430247</td>\n",
       "      <td>0.433888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.699062</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.467306</td>\n",
       "      <td>0.445638</td>\n",
       "      <td>0.436708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.692103</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.497161</td>\n",
       "      <td>0.454542</td>\n",
       "      <td>0.460080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.679356</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.485126</td>\n",
       "      <td>0.493952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.676143</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.519104</td>\n",
       "      <td>0.503322</td>\n",
       "      <td>0.500834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.665493</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.537446</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.504224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.238200</td>\n",
       "      <td>0.654679</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.603876</td>\n",
       "      <td>0.545583</td>\n",
       "      <td>0.557471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.658202</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.608646</td>\n",
       "      <td>0.548362</td>\n",
       "      <td>0.562621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.655122</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.644582</td>\n",
       "      <td>0.557737</td>\n",
       "      <td>0.578265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.653654</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.600619</td>\n",
       "      <td>0.554242</td>\n",
       "      <td>0.560625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.645303</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.618058</td>\n",
       "      <td>0.560985</td>\n",
       "      <td>0.573233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:03:15,104] Trial 60 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 61 with params: {'learning_rate': 0.0003439050443277329, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.235200</td>\n",
       "      <td>1.981465</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.064225</td>\n",
       "      <td>0.076756</td>\n",
       "      <td>0.061155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.804500</td>\n",
       "      <td>1.588862</td>\n",
       "      <td>0.493126</td>\n",
       "      <td>0.129584</td>\n",
       "      <td>0.142355</td>\n",
       "      <td>0.116429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.447900</td>\n",
       "      <td>1.287763</td>\n",
       "      <td>0.590284</td>\n",
       "      <td>0.221102</td>\n",
       "      <td>0.208715</td>\n",
       "      <td>0.191299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.166700</td>\n",
       "      <td>1.099639</td>\n",
       "      <td>0.667278</td>\n",
       "      <td>0.257534</td>\n",
       "      <td>0.273390</td>\n",
       "      <td>0.254114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.967400</td>\n",
       "      <td>0.975927</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.286265</td>\n",
       "      <td>0.300772</td>\n",
       "      <td>0.276570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.329637</td>\n",
       "      <td>0.323907</td>\n",
       "      <td>0.303251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.823061</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.353263</td>\n",
       "      <td>0.336626</td>\n",
       "      <td>0.323134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.787173</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.380109</td>\n",
       "      <td>0.362208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.764650</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.409471</td>\n",
       "      <td>0.394183</td>\n",
       "      <td>0.383866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.749562</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>0.402710</td>\n",
       "      <td>0.397210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.722524</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.462729</td>\n",
       "      <td>0.431160</td>\n",
       "      <td>0.423888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.706682</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.487305</td>\n",
       "      <td>0.447404</td>\n",
       "      <td>0.450298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.695387</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.496799</td>\n",
       "      <td>0.472106</td>\n",
       "      <td>0.473367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.686609</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.509392</td>\n",
       "      <td>0.488084</td>\n",
       "      <td>0.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.672975</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.527530</td>\n",
       "      <td>0.489533</td>\n",
       "      <td>0.493671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.666062</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.553376</td>\n",
       "      <td>0.518070</td>\n",
       "      <td>0.522851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.667691</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.558550</td>\n",
       "      <td>0.520820</td>\n",
       "      <td>0.527554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.658561</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.575131</td>\n",
       "      <td>0.537448</td>\n",
       "      <td>0.545329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.655836</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.570582</td>\n",
       "      <td>0.540389</td>\n",
       "      <td>0.541690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.651019</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.565155</td>\n",
       "      <td>0.543541</td>\n",
       "      <td>0.542465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.655252</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.624566</td>\n",
       "      <td>0.574177</td>\n",
       "      <td>0.586576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.649442</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.644192</td>\n",
       "      <td>0.585816</td>\n",
       "      <td>0.600449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.645412</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.627199</td>\n",
       "      <td>0.574135</td>\n",
       "      <td>0.587492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.644582</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.627564</td>\n",
       "      <td>0.569615</td>\n",
       "      <td>0.583657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.643856</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.627421</td>\n",
       "      <td>0.575314</td>\n",
       "      <td>0.588065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.640501</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.646967</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>0.599462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.639629</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.624822</td>\n",
       "      <td>0.575298</td>\n",
       "      <td>0.588348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.176300</td>\n",
       "      <td>0.640954</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.627564</td>\n",
       "      <td>0.574969</td>\n",
       "      <td>0.588144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.640950</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.642911</td>\n",
       "      <td>0.585298</td>\n",
       "      <td>0.601110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.641388</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.642185</td>\n",
       "      <td>0.581083</td>\n",
       "      <td>0.597852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Fri Jan 10 23:13:59 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 16:06:38,759] Trial 61 finished with value: 0.5978521818185435 and parameters: {'learning_rate': 0.0003439050443277329, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 2.5}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 62 with params: {'learning_rate': 0.00010037683401527949, 'weight_decay': 0.0, 'adam_beta1': 0.97, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:14, 6.52 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.371300</td>\n",
       "      <td>2.255709</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.199000</td>\n",
       "      <td>2.110240</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.006952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.074700</td>\n",
       "      <td>1.985780</td>\n",
       "      <td>0.384968</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.081148</td>\n",
       "      <td>0.063165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.949000</td>\n",
       "      <td>1.862002</td>\n",
       "      <td>0.431714</td>\n",
       "      <td>0.067298</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>0.074849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.838100</td>\n",
       "      <td>1.747278</td>\n",
       "      <td>0.456462</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.088499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:07:08,329] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 63 with params: {'learning_rate': 0.0003066027287823963, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:53, 6.16 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.246800</td>\n",
       "      <td>2.016312</td>\n",
       "      <td>0.346471</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.069070</td>\n",
       "      <td>0.056775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.858400</td>\n",
       "      <td>1.654972</td>\n",
       "      <td>0.472044</td>\n",
       "      <td>0.122503</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.097961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.529200</td>\n",
       "      <td>1.370617</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>0.195674</td>\n",
       "      <td>0.177078</td>\n",
       "      <td>0.156106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.255700</td>\n",
       "      <td>1.173112</td>\n",
       "      <td>0.637030</td>\n",
       "      <td>0.262887</td>\n",
       "      <td>0.246173</td>\n",
       "      <td>0.230090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.058400</td>\n",
       "      <td>1.031587</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.272763</td>\n",
       "      <td>0.289812</td>\n",
       "      <td>0.265564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.920408</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.313757</td>\n",
       "      <td>0.308352</td>\n",
       "      <td>0.288647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.756700</td>\n",
       "      <td>0.860582</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.326502</td>\n",
       "      <td>0.312370</td>\n",
       "      <td>0.294713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.820853</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.366885</td>\n",
       "      <td>0.354898</td>\n",
       "      <td>0.334016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.790181</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.365629</td>\n",
       "      <td>0.368613</td>\n",
       "      <td>0.353324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.773641</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.462110</td>\n",
       "      <td>0.395345</td>\n",
       "      <td>0.390250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:08:06,101] Trial 63 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 with params: {'learning_rate': 0.0003021574030153618, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.49 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.256100</td>\n",
       "      <td>2.031007</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>0.071703</td>\n",
       "      <td>0.054093</td>\n",
       "      <td>0.042437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.880600</td>\n",
       "      <td>1.682675</td>\n",
       "      <td>0.467461</td>\n",
       "      <td>0.121728</td>\n",
       "      <td>0.129829</td>\n",
       "      <td>0.102377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.561600</td>\n",
       "      <td>1.403411</td>\n",
       "      <td>0.541705</td>\n",
       "      <td>0.177362</td>\n",
       "      <td>0.171627</td>\n",
       "      <td>0.150187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.288100</td>\n",
       "      <td>1.198638</td>\n",
       "      <td>0.628781</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.235170</td>\n",
       "      <td>0.214453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.086800</td>\n",
       "      <td>1.052824</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.265771</td>\n",
       "      <td>0.282778</td>\n",
       "      <td>0.257258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.307430</td>\n",
       "      <td>0.303770</td>\n",
       "      <td>0.284720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.874045</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.302445</td>\n",
       "      <td>0.304628</td>\n",
       "      <td>0.282008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.833702</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.377562</td>\n",
       "      <td>0.365658</td>\n",
       "      <td>0.345088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>0.803321</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.411080</td>\n",
       "      <td>0.377198</td>\n",
       "      <td>0.368524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.787510</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.430554</td>\n",
       "      <td>0.389171</td>\n",
       "      <td>0.378358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.753988</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.423777</td>\n",
       "      <td>0.412182</td>\n",
       "      <td>0.400915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.730426</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.451709</td>\n",
       "      <td>0.421297</td>\n",
       "      <td>0.417076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.719886</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.439337</td>\n",
       "      <td>0.434048</td>\n",
       "      <td>0.425464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.705345</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.501671</td>\n",
       "      <td>0.452779</td>\n",
       "      <td>0.453159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.699438</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.504817</td>\n",
       "      <td>0.457388</td>\n",
       "      <td>0.456502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.687577</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.472345</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.456239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.684359</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.516021</td>\n",
       "      <td>0.475919</td>\n",
       "      <td>0.479641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.677529</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.513785</td>\n",
       "      <td>0.494809</td>\n",
       "      <td>0.493191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.672308</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.544632</td>\n",
       "      <td>0.520017</td>\n",
       "      <td>0.518421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.667443</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.552546</td>\n",
       "      <td>0.525377</td>\n",
       "      <td>0.522811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:09:54,788] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 65 with params: {'learning_rate': 0.00035835584343043295, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.210800</td>\n",
       "      <td>1.939397</td>\n",
       "      <td>0.395967</td>\n",
       "      <td>0.075777</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>0.063852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.756900</td>\n",
       "      <td>1.537018</td>\n",
       "      <td>0.519707</td>\n",
       "      <td>0.161841</td>\n",
       "      <td>0.159590</td>\n",
       "      <td>0.140356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.392300</td>\n",
       "      <td>1.237861</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.223672</td>\n",
       "      <td>0.234691</td>\n",
       "      <td>0.214548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.115000</td>\n",
       "      <td>1.060860</td>\n",
       "      <td>0.680110</td>\n",
       "      <td>0.260830</td>\n",
       "      <td>0.282796</td>\n",
       "      <td>0.260647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.940448</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.301648</td>\n",
       "      <td>0.316443</td>\n",
       "      <td>0.294104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.762800</td>\n",
       "      <td>0.847240</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.329810</td>\n",
       "      <td>0.331380</td>\n",
       "      <td>0.311538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>0.809931</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.360962</td>\n",
       "      <td>0.340888</td>\n",
       "      <td>0.326356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.779010</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.379756</td>\n",
       "      <td>0.387089</td>\n",
       "      <td>0.368509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.754336</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.432380</td>\n",
       "      <td>0.399907</td>\n",
       "      <td>0.393234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.733425</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.412721</td>\n",
       "      <td>0.410058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.704803</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.454403</td>\n",
       "      <td>0.435432</td>\n",
       "      <td>0.428070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.693857</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.515875</td>\n",
       "      <td>0.457265</td>\n",
       "      <td>0.462286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.684291</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.525582</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.485066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.672944</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>0.478502</td>\n",
       "      <td>0.473525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>0.663073</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.500511</td>\n",
       "      <td>0.482464</td>\n",
       "      <td>0.483589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.558752</td>\n",
       "      <td>0.517398</td>\n",
       "      <td>0.518006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>0.662942</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.568535</td>\n",
       "      <td>0.527153</td>\n",
       "      <td>0.533761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.653673</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.604788</td>\n",
       "      <td>0.541403</td>\n",
       "      <td>0.554221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.656164</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.607929</td>\n",
       "      <td>0.549160</td>\n",
       "      <td>0.558982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.552405</td>\n",
       "      <td>0.560912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.649877</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.616968</td>\n",
       "      <td>0.550123</td>\n",
       "      <td>0.567561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.652737</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.601677</td>\n",
       "      <td>0.549949</td>\n",
       "      <td>0.560515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.643624</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.605790</td>\n",
       "      <td>0.552664</td>\n",
       "      <td>0.563980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.642467</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.608735</td>\n",
       "      <td>0.564501</td>\n",
       "      <td>0.571709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.643823</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.653790</td>\n",
       "      <td>0.580346</td>\n",
       "      <td>0.597978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.639912</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.654954</td>\n",
       "      <td>0.584975</td>\n",
       "      <td>0.600746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.640265</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.653094</td>\n",
       "      <td>0.574321</td>\n",
       "      <td>0.594248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.642002</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.636989</td>\n",
       "      <td>0.572599</td>\n",
       "      <td>0.588997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.638420</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.656008</td>\n",
       "      <td>0.580996</td>\n",
       "      <td>0.600251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.639252</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.657170</td>\n",
       "      <td>0.581450</td>\n",
       "      <td>0.600788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:12:40,154] Trial 65 finished with value: 0.600788124130334 and parameters: {'learning_rate': 0.00035835584343043295, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 66 with params: {'learning_rate': 2.174477018969353e-05, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.9, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:52, 6.21 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.441800</td>\n",
       "      <td>2.391338</td>\n",
       "      <td>0.171402</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>0.008672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.375700</td>\n",
       "      <td>2.334175</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.329500</td>\n",
       "      <td>2.288096</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.286200</td>\n",
       "      <td>2.246684</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.248700</td>\n",
       "      <td>2.204117</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.209300</td>\n",
       "      <td>2.168125</td>\n",
       "      <td>0.199817</td>\n",
       "      <td>0.063631</td>\n",
       "      <td>0.026493</td>\n",
       "      <td>0.017199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.175100</td>\n",
       "      <td>2.132962</td>\n",
       "      <td>0.274060</td>\n",
       "      <td>0.074352</td>\n",
       "      <td>0.047563</td>\n",
       "      <td>0.044091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.143900</td>\n",
       "      <td>2.100139</td>\n",
       "      <td>0.340055</td>\n",
       "      <td>0.070613</td>\n",
       "      <td>0.067335</td>\n",
       "      <td>0.060022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.114700</td>\n",
       "      <td>2.070334</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.063735</td>\n",
       "      <td>0.073941</td>\n",
       "      <td>0.062233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.088200</td>\n",
       "      <td>2.042365</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.079052</td>\n",
       "      <td>0.081905</td>\n",
       "      <td>0.064961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:13:37,159] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 67 with params: {'learning_rate': 0.0001282754863777374, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:16, 6.42 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.340900</td>\n",
       "      <td>2.196843</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.121800</td>\n",
       "      <td>2.002682</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.059936</td>\n",
       "      <td>0.081932</td>\n",
       "      <td>0.063948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.945800</td>\n",
       "      <td>1.821072</td>\n",
       "      <td>0.438130</td>\n",
       "      <td>0.076731</td>\n",
       "      <td>0.104623</td>\n",
       "      <td>0.078558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.766200</td>\n",
       "      <td>1.659499</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.117877</td>\n",
       "      <td>0.136409</td>\n",
       "      <td>0.110041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.619700</td>\n",
       "      <td>1.518095</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.191193</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.155996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:14:05,165] Trial 67 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 68 with params: {'learning_rate': 0.00029808734586146555, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 3, 'lambda_param': 0.4, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.70 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.246400</td>\n",
       "      <td>2.007869</td>\n",
       "      <td>0.367553</td>\n",
       "      <td>0.067049</td>\n",
       "      <td>0.075495</td>\n",
       "      <td>0.061999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.849400</td>\n",
       "      <td>1.642337</td>\n",
       "      <td>0.479377</td>\n",
       "      <td>0.135279</td>\n",
       "      <td>0.136027</td>\n",
       "      <td>0.110772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.515900</td>\n",
       "      <td>1.354553</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.214704</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.164487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.244600</td>\n",
       "      <td>1.159204</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.272127</td>\n",
       "      <td>0.266283</td>\n",
       "      <td>0.251238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.046500</td>\n",
       "      <td>1.021368</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.265658</td>\n",
       "      <td>0.290655</td>\n",
       "      <td>0.265509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.881500</td>\n",
       "      <td>0.917725</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.299864</td>\n",
       "      <td>0.312239</td>\n",
       "      <td>0.291855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.755500</td>\n",
       "      <td>0.857761</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.301081</td>\n",
       "      <td>0.314719</td>\n",
       "      <td>0.294231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.822261</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.373429</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.338278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.600200</td>\n",
       "      <td>0.790746</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.390241</td>\n",
       "      <td>0.372360</td>\n",
       "      <td>0.360196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.529700</td>\n",
       "      <td>0.771244</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.453809</td>\n",
       "      <td>0.391047</td>\n",
       "      <td>0.385217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.484100</td>\n",
       "      <td>0.741205</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.441349</td>\n",
       "      <td>0.413894</td>\n",
       "      <td>0.406452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>0.726721</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.442199</td>\n",
       "      <td>0.407592</td>\n",
       "      <td>0.402997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.707592</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.460246</td>\n",
       "      <td>0.444067</td>\n",
       "      <td>0.439060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.697764</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.489736</td>\n",
       "      <td>0.452992</td>\n",
       "      <td>0.447933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.689071</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.448663</td>\n",
       "      <td>0.445352</td>\n",
       "      <td>0.438149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.681380</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.471732</td>\n",
       "      <td>0.465299</td>\n",
       "      <td>0.456562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.677187</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.518642</td>\n",
       "      <td>0.477215</td>\n",
       "      <td>0.480171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.669650</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.536125</td>\n",
       "      <td>0.494823</td>\n",
       "      <td>0.497747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.663811</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.542232</td>\n",
       "      <td>0.505503</td>\n",
       "      <td>0.505215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.653821</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.551158</td>\n",
       "      <td>0.519427</td>\n",
       "      <td>0.519292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:15:50,498] Trial 68 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 69 with params: {'learning_rate': 8.217756913819263e-06, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 4, 'lambda_param': 0.9, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:45 < 00:52, 6.60 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.463500</td>\n",
       "      <td>2.437492</td>\n",
       "      <td>0.025665</td>\n",
       "      <td>0.024872</td>\n",
       "      <td>0.023985</td>\n",
       "      <td>0.004711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.433900</td>\n",
       "      <td>2.409627</td>\n",
       "      <td>0.115490</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.008832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.409800</td>\n",
       "      <td>2.382557</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.010667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.386900</td>\n",
       "      <td>2.359769</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.021445</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.365300</td>\n",
       "      <td>2.340321</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.019265</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.011932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.346400</td>\n",
       "      <td>2.322715</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.331600</td>\n",
       "      <td>2.306541</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.316800</td>\n",
       "      <td>2.291396</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.302700</td>\n",
       "      <td>2.277120</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.290700</td>\n",
       "      <td>2.264025</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.278500</td>\n",
       "      <td>2.251504</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.266600</td>\n",
       "      <td>2.240252</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.259000</td>\n",
       "      <td>2.229436</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.246000</td>\n",
       "      <td>2.219292</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.238300</td>\n",
       "      <td>2.210186</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.023561</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.227900</td>\n",
       "      <td>2.201463</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.043574</td>\n",
       "      <td>0.022407</td>\n",
       "      <td>0.010441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.220900</td>\n",
       "      <td>2.193490</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.063587</td>\n",
       "      <td>0.023193</td>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.216000</td>\n",
       "      <td>2.186292</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.063611</td>\n",
       "      <td>0.024801</td>\n",
       "      <td>0.014291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.207800</td>\n",
       "      <td>2.179259</td>\n",
       "      <td>0.197984</td>\n",
       "      <td>0.063628</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.016357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.205000</td>\n",
       "      <td>2.173188</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.021510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:17:37,387] Trial 69 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 70 with params: {'learning_rate': 6.0142611090541214e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:06, 6.94 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.396900</td>\n",
       "      <td>2.307804</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.271000</td>\n",
       "      <td>2.200875</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.177100</td>\n",
       "      <td>2.104767</td>\n",
       "      <td>0.263061</td>\n",
       "      <td>0.072242</td>\n",
       "      <td>0.044536</td>\n",
       "      <td>0.039549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.085200</td>\n",
       "      <td>2.015182</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.084390</td>\n",
       "      <td>0.064179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.007400</td>\n",
       "      <td>1.930626</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.091830</td>\n",
       "      <td>0.068494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:18:03,362] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 71 with params: {'learning_rate': 0.0003340918907489455, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:47 < 00:53, 6.50 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.228400</td>\n",
       "      <td>1.967752</td>\n",
       "      <td>0.389551</td>\n",
       "      <td>0.060396</td>\n",
       "      <td>0.083055</td>\n",
       "      <td>0.063793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.791100</td>\n",
       "      <td>1.572350</td>\n",
       "      <td>0.510541</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>0.150697</td>\n",
       "      <td>0.128043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.432700</td>\n",
       "      <td>1.273083</td>\n",
       "      <td>0.615949</td>\n",
       "      <td>0.227215</td>\n",
       "      <td>0.224812</td>\n",
       "      <td>0.207288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.157400</td>\n",
       "      <td>1.091805</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.262771</td>\n",
       "      <td>0.279562</td>\n",
       "      <td>0.259928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.969889</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.300376</td>\n",
       "      <td>0.307377</td>\n",
       "      <td>0.285686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.869708</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.315307</td>\n",
       "      <td>0.323983</td>\n",
       "      <td>0.302345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.822880</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.338964</td>\n",
       "      <td>0.335247</td>\n",
       "      <td>0.316626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>0.793428</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.370614</td>\n",
       "      <td>0.381207</td>\n",
       "      <td>0.361507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>0.762230</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.389338</td>\n",
       "      <td>0.387729</td>\n",
       "      <td>0.372792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.745359</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.455841</td>\n",
       "      <td>0.399108</td>\n",
       "      <td>0.393992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.716518</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>0.432538</td>\n",
       "      <td>0.426203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.705664</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.498488</td>\n",
       "      <td>0.438734</td>\n",
       "      <td>0.443503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.696708</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.496993</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.464907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.681748</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.509073</td>\n",
       "      <td>0.479580</td>\n",
       "      <td>0.479081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.666771</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.496063</td>\n",
       "      <td>0.473932</td>\n",
       "      <td>0.475036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.660950</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.526846</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>0.504852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.663798</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.550718</td>\n",
       "      <td>0.511741</td>\n",
       "      <td>0.514272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.651258</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.563801</td>\n",
       "      <td>0.526715</td>\n",
       "      <td>0.532054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>0.651751</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.548232</td>\n",
       "      <td>0.524073</td>\n",
       "      <td>0.525308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.641986</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.552116</td>\n",
       "      <td>0.531881</td>\n",
       "      <td>0.533996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:19:52,086] Trial 71 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 with params: {'learning_rate': 0.0003523606388342241, 'weight_decay': 0.005, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.229800</td>\n",
       "      <td>1.987539</td>\n",
       "      <td>0.366636</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>0.075338</td>\n",
       "      <td>0.060094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.814700</td>\n",
       "      <td>1.608563</td>\n",
       "      <td>0.478460</td>\n",
       "      <td>0.112474</td>\n",
       "      <td>0.133499</td>\n",
       "      <td>0.104292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.470400</td>\n",
       "      <td>1.314303</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.220220</td>\n",
       "      <td>0.193173</td>\n",
       "      <td>0.172327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.187100</td>\n",
       "      <td>1.119107</td>\n",
       "      <td>0.650779</td>\n",
       "      <td>0.266615</td>\n",
       "      <td>0.261655</td>\n",
       "      <td>0.244390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.982166</td>\n",
       "      <td>0.692942</td>\n",
       "      <td>0.286357</td>\n",
       "      <td>0.293619</td>\n",
       "      <td>0.271124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>0.879256</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.309776</td>\n",
       "      <td>0.322827</td>\n",
       "      <td>0.301386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.831309</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.316877</td>\n",
       "      <td>0.324729</td>\n",
       "      <td>0.306166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.607300</td>\n",
       "      <td>0.795827</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.393479</td>\n",
       "      <td>0.379602</td>\n",
       "      <td>0.361438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.770454</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.428630</td>\n",
       "      <td>0.395404</td>\n",
       "      <td>0.387104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.757778</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.449947</td>\n",
       "      <td>0.403916</td>\n",
       "      <td>0.395855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.722002</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.458609</td>\n",
       "      <td>0.425928</td>\n",
       "      <td>0.420269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.701709</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.485338</td>\n",
       "      <td>0.446203</td>\n",
       "      <td>0.447750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.694839</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.504586</td>\n",
       "      <td>0.481876</td>\n",
       "      <td>0.481859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.681056</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.514392</td>\n",
       "      <td>0.492723</td>\n",
       "      <td>0.492362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.672960</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.536974</td>\n",
       "      <td>0.503781</td>\n",
       "      <td>0.505650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.662723</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.544509</td>\n",
       "      <td>0.522359</td>\n",
       "      <td>0.523785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.662089</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.550053</td>\n",
       "      <td>0.517744</td>\n",
       "      <td>0.519703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.656991</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.582969</td>\n",
       "      <td>0.531624</td>\n",
       "      <td>0.539440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.659235</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.558274</td>\n",
       "      <td>0.532336</td>\n",
       "      <td>0.532665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.657806</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.584988</td>\n",
       "      <td>0.548581</td>\n",
       "      <td>0.552599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.655413</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.591335</td>\n",
       "      <td>0.561679</td>\n",
       "      <td>0.564656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.651311</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.612189</td>\n",
       "      <td>0.570628</td>\n",
       "      <td>0.578054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.646648</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.600643</td>\n",
       "      <td>0.558125</td>\n",
       "      <td>0.566614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.646943</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.619287</td>\n",
       "      <td>0.574952</td>\n",
       "      <td>0.584051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.645860</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.618028</td>\n",
       "      <td>0.579625</td>\n",
       "      <td>0.585687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.641357</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.646256</td>\n",
       "      <td>0.596063</td>\n",
       "      <td>0.607287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.643698</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.619090</td>\n",
       "      <td>0.566248</td>\n",
       "      <td>0.578962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.643585</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.619499</td>\n",
       "      <td>0.573018</td>\n",
       "      <td>0.583024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.642935</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.622404</td>\n",
       "      <td>0.581814</td>\n",
       "      <td>0.589419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.643049</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.622803</td>\n",
       "      <td>0.581814</td>\n",
       "      <td>0.589615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:22:30,415] Trial 72 finished with value: 0.5896153129552318 and parameters: {'learning_rate': 0.0003523606388342241, 'weight_decay': 0.005, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 73 with params: {'learning_rate': 3.292397722386191e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.97, 'warmup_steps': 2, 'lambda_param': 1.0, 'temperature': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:24 < 02:04, 7.01 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.428700</td>\n",
       "      <td>2.365949</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.012699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.344800</td>\n",
       "      <td>2.297573</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.287100</td>\n",
       "      <td>2.239503</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.231100</td>\n",
       "      <td>2.185321</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.185000</td>\n",
       "      <td>2.133239</td>\n",
       "      <td>0.226398</td>\n",
       "      <td>0.079733</td>\n",
       "      <td>0.034367</td>\n",
       "      <td>0.027248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:22:56,140] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 74 with params: {'learning_rate': 0.00041831093045197835, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:58, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.189500</td>\n",
       "      <td>1.900690</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>0.073533</td>\n",
       "      <td>0.088060</td>\n",
       "      <td>0.066566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.702500</td>\n",
       "      <td>1.479269</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.169734</td>\n",
       "      <td>0.180913</td>\n",
       "      <td>0.162054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.322100</td>\n",
       "      <td>1.179222</td>\n",
       "      <td>0.636114</td>\n",
       "      <td>0.229324</td>\n",
       "      <td>0.235711</td>\n",
       "      <td>0.216934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.041700</td>\n",
       "      <td>1.005973</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.272573</td>\n",
       "      <td>0.293127</td>\n",
       "      <td>0.270068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.844100</td>\n",
       "      <td>0.892490</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.329656</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.304476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>0.815980</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.349720</td>\n",
       "      <td>0.345275</td>\n",
       "      <td>0.330264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.583900</td>\n",
       "      <td>0.792363</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.385869</td>\n",
       "      <td>0.372504</td>\n",
       "      <td>0.358952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.511800</td>\n",
       "      <td>0.751540</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.409368</td>\n",
       "      <td>0.414185</td>\n",
       "      <td>0.400030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.734764</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.459566</td>\n",
       "      <td>0.422534</td>\n",
       "      <td>0.418249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.711398</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.479796</td>\n",
       "      <td>0.439253</td>\n",
       "      <td>0.441990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.693686</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.472945</td>\n",
       "      <td>0.451680</td>\n",
       "      <td>0.447057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.671960</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.529226</td>\n",
       "      <td>0.475937</td>\n",
       "      <td>0.485073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.674287</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.533788</td>\n",
       "      <td>0.510149</td>\n",
       "      <td>0.510932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.656005</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.574695</td>\n",
       "      <td>0.530712</td>\n",
       "      <td>0.537044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.656098</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.580559</td>\n",
       "      <td>0.548925</td>\n",
       "      <td>0.553613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.652072</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.596805</td>\n",
       "      <td>0.550282</td>\n",
       "      <td>0.557957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.584273</td>\n",
       "      <td>0.546520</td>\n",
       "      <td>0.554612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.650948</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.591802</td>\n",
       "      <td>0.550031</td>\n",
       "      <td>0.554153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.656136</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.566216</td>\n",
       "      <td>0.572459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.647373</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.607607</td>\n",
       "      <td>0.572261</td>\n",
       "      <td>0.578590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.649883</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.622865</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.591974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.645920</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>0.585357</td>\n",
       "      <td>0.589177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.643491</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.622816</td>\n",
       "      <td>0.579279</td>\n",
       "      <td>0.587491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.616688</td>\n",
       "      <td>0.573332</td>\n",
       "      <td>0.581428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.637751</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.617433</td>\n",
       "      <td>0.587828</td>\n",
       "      <td>0.590319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.636229</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.620913</td>\n",
       "      <td>0.588417</td>\n",
       "      <td>0.591832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.639001</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.623422</td>\n",
       "      <td>0.581389</td>\n",
       "      <td>0.589731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.640834</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>0.579987</td>\n",
       "      <td>0.587158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.638690</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.617577</td>\n",
       "      <td>0.579839</td>\n",
       "      <td>0.585421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.639039</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.615999</td>\n",
       "      <td>0.579738</td>\n",
       "      <td>0.584326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:25:56,336] Trial 74 finished with value: 0.5843257811205536 and parameters: {'learning_rate': 0.00041831093045197835, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 75 with params: {'learning_rate': 0.0004740324993790357, 'weight_decay': 0.002, 'adam_beta1': 0.96, 'warmup_steps': 4, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:37, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.200200</td>\n",
       "      <td>1.927376</td>\n",
       "      <td>0.378552</td>\n",
       "      <td>0.058403</td>\n",
       "      <td>0.079123</td>\n",
       "      <td>0.059895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.730300</td>\n",
       "      <td>1.519622</td>\n",
       "      <td>0.505958</td>\n",
       "      <td>0.146064</td>\n",
       "      <td>0.155597</td>\n",
       "      <td>0.133826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.362800</td>\n",
       "      <td>1.225812</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.223362</td>\n",
       "      <td>0.215836</td>\n",
       "      <td>0.186089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.066700</td>\n",
       "      <td>1.042808</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.264197</td>\n",
       "      <td>0.280790</td>\n",
       "      <td>0.253391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.924120</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.295849</td>\n",
       "      <td>0.307627</td>\n",
       "      <td>0.279883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.843377</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.403121</td>\n",
       "      <td>0.350750</td>\n",
       "      <td>0.339201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.816331</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.377944</td>\n",
       "      <td>0.370147</td>\n",
       "      <td>0.360004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.775887</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.381733</td>\n",
       "      <td>0.398558</td>\n",
       "      <td>0.377344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.759048</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.448642</td>\n",
       "      <td>0.422313</td>\n",
       "      <td>0.418221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.736863</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.482634</td>\n",
       "      <td>0.450914</td>\n",
       "      <td>0.447453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.714638</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.489052</td>\n",
       "      <td>0.469982</td>\n",
       "      <td>0.469978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.699881</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.570583</td>\n",
       "      <td>0.498193</td>\n",
       "      <td>0.516950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.691238</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.570927</td>\n",
       "      <td>0.517356</td>\n",
       "      <td>0.527489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.669907</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.602614</td>\n",
       "      <td>0.550498</td>\n",
       "      <td>0.561282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.668353</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.578882</td>\n",
       "      <td>0.540549</td>\n",
       "      <td>0.548960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.657115</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.616651</td>\n",
       "      <td>0.559526</td>\n",
       "      <td>0.574414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.655323</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.581583</td>\n",
       "      <td>0.540116</td>\n",
       "      <td>0.548015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.654476</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.613183</td>\n",
       "      <td>0.557760</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.649804</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.612739</td>\n",
       "      <td>0.569158</td>\n",
       "      <td>0.577770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.653599</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.626727</td>\n",
       "      <td>0.589836</td>\n",
       "      <td>0.594979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.649005</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.629207</td>\n",
       "      <td>0.580687</td>\n",
       "      <td>0.588374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.652529</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.624519</td>\n",
       "      <td>0.584354</td>\n",
       "      <td>0.589936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.650256</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.608421</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.575641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.648779</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.611864</td>\n",
       "      <td>0.579032</td>\n",
       "      <td>0.582020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.647127</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.615152</td>\n",
       "      <td>0.584642</td>\n",
       "      <td>0.586157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.642706</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.625241</td>\n",
       "      <td>0.597191</td>\n",
       "      <td>0.597472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.647763</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.630282</td>\n",
       "      <td>0.585845</td>\n",
       "      <td>0.593325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.648223</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.649117</td>\n",
       "      <td>0.592452</td>\n",
       "      <td>0.603738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.647747</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.631399</td>\n",
       "      <td>0.582089</td>\n",
       "      <td>0.590275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.647746</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.642304</td>\n",
       "      <td>0.592089</td>\n",
       "      <td>0.600518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:28:35,975] Trial 75 finished with value: 0.6005175043364164 and parameters: {'learning_rate': 0.0004740324993790357, 'weight_decay': 0.002, 'adam_beta1': 0.96, 'warmup_steps': 4, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 76 with params: {'learning_rate': 0.00015001917556377228, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:44 < 00:52, 6.69 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.309800</td>\n",
       "      <td>2.148980</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.063000</td>\n",
       "      <td>1.930170</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.088292</td>\n",
       "      <td>0.062930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.860600</td>\n",
       "      <td>1.723329</td>\n",
       "      <td>0.458295</td>\n",
       "      <td>0.102153</td>\n",
       "      <td>0.118077</td>\n",
       "      <td>0.091868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.660600</td>\n",
       "      <td>1.548330</td>\n",
       "      <td>0.518790</td>\n",
       "      <td>0.201412</td>\n",
       "      <td>0.157206</td>\n",
       "      <td>0.137990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.504100</td>\n",
       "      <td>1.406351</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.246539</td>\n",
       "      <td>0.194695</td>\n",
       "      <td>0.179475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.351900</td>\n",
       "      <td>1.292668</td>\n",
       "      <td>0.608616</td>\n",
       "      <td>0.241814</td>\n",
       "      <td>0.223008</td>\n",
       "      <td>0.206926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.227500</td>\n",
       "      <td>1.195377</td>\n",
       "      <td>0.653529</td>\n",
       "      <td>0.240920</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.240158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.132200</td>\n",
       "      <td>1.119395</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.281989</td>\n",
       "      <td>0.259512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.039300</td>\n",
       "      <td>1.055907</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.280706</td>\n",
       "      <td>0.289161</td>\n",
       "      <td>0.265685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>1.001718</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.283964</td>\n",
       "      <td>0.297960</td>\n",
       "      <td>0.274836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.958389</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.285504</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>0.281328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.925407</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.311416</td>\n",
       "      <td>0.287201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>0.891292</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.301175</td>\n",
       "      <td>0.314618</td>\n",
       "      <td>0.290990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.346597</td>\n",
       "      <td>0.330945</td>\n",
       "      <td>0.309635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.851603</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.318998</td>\n",
       "      <td>0.330549</td>\n",
       "      <td>0.308388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.837999</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.328644</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.308346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.823435</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.332477</td>\n",
       "      <td>0.342829</td>\n",
       "      <td>0.323362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.810189</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.347355</td>\n",
       "      <td>0.354924</td>\n",
       "      <td>0.332926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.803338</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.336736</td>\n",
       "      <td>0.360847</td>\n",
       "      <td>0.338495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.793649</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.376587</td>\n",
       "      <td>0.367803</td>\n",
       "      <td>0.347702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:30:21,394] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 77 with params: {'learning_rate': 0.00041891462370218397, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.175700</td>\n",
       "      <td>1.874340</td>\n",
       "      <td>0.423465</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.099076</td>\n",
       "      <td>0.074827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.674600</td>\n",
       "      <td>1.446445</td>\n",
       "      <td>0.554537</td>\n",
       "      <td>0.219646</td>\n",
       "      <td>0.185696</td>\n",
       "      <td>0.168986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.290200</td>\n",
       "      <td>1.151284</td>\n",
       "      <td>0.657195</td>\n",
       "      <td>0.250084</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.238965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.015400</td>\n",
       "      <td>0.982911</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.263149</td>\n",
       "      <td>0.289699</td>\n",
       "      <td>0.264824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.822100</td>\n",
       "      <td>0.881014</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.317319</td>\n",
       "      <td>0.330080</td>\n",
       "      <td>0.304329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.807987</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.368985</td>\n",
       "      <td>0.358090</td>\n",
       "      <td>0.348224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.779204</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.393099</td>\n",
       "      <td>0.375728</td>\n",
       "      <td>0.362174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.744078</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.401319</td>\n",
       "      <td>0.414079</td>\n",
       "      <td>0.398292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.441700</td>\n",
       "      <td>0.715642</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.451572</td>\n",
       "      <td>0.423079</td>\n",
       "      <td>0.416610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.700259</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>0.442795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.691707</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.474495</td>\n",
       "      <td>0.456737</td>\n",
       "      <td>0.450961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.668049</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.509946</td>\n",
       "      <td>0.484150</td>\n",
       "      <td>0.486612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.544873</td>\n",
       "      <td>0.495618</td>\n",
       "      <td>0.502637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.655497</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.546592</td>\n",
       "      <td>0.551553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.657689</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.606652</td>\n",
       "      <td>0.548676</td>\n",
       "      <td>0.557925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.644473</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.581210</td>\n",
       "      <td>0.545441</td>\n",
       "      <td>0.552201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.643376</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.619751</td>\n",
       "      <td>0.564709</td>\n",
       "      <td>0.576838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.641902</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.600991</td>\n",
       "      <td>0.560078</td>\n",
       "      <td>0.569190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.646367</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.616612</td>\n",
       "      <td>0.567697</td>\n",
       "      <td>0.578101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.640627</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.641821</td>\n",
       "      <td>0.578278</td>\n",
       "      <td>0.594232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.645522</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.646264</td>\n",
       "      <td>0.585979</td>\n",
       "      <td>0.601936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.638577</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.661235</td>\n",
       "      <td>0.593456</td>\n",
       "      <td>0.609417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.639128</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666863</td>\n",
       "      <td>0.601316</td>\n",
       "      <td>0.614087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.638384</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.637311</td>\n",
       "      <td>0.585468</td>\n",
       "      <td>0.594676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.635971</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.664817</td>\n",
       "      <td>0.596867</td>\n",
       "      <td>0.611170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.636529</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.662129</td>\n",
       "      <td>0.602327</td>\n",
       "      <td>0.613187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.636518</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.665379</td>\n",
       "      <td>0.596938</td>\n",
       "      <td>0.612120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.638216</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.644325</td>\n",
       "      <td>0.591518</td>\n",
       "      <td>0.602678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.636516</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.665989</td>\n",
       "      <td>0.599402</td>\n",
       "      <td>0.612813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.636848</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.662720</td>\n",
       "      <td>0.599402</td>\n",
       "      <td>0.611827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:33:03,098] Trial 77 finished with value: 0.6118265793276343 and parameters: {'learning_rate': 0.00041891462370218397, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 5.0}. Best is trial 52 with value: 0.6341848954231764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 78 with params: {'learning_rate': 1.3245726232440134e-06, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:13, 6.56 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.474800</td>\n",
       "      <td>2.462725</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.469300</td>\n",
       "      <td>2.457204</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.465700</td>\n",
       "      <td>2.452202</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.002253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.461900</td>\n",
       "      <td>2.447650</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.454100</td>\n",
       "      <td>2.443338</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.003354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:33:30,689] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 79 with params: {'learning_rate': 0.0002475885664259343, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0, 'lambda_param': 0.5, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 01:00 < 02:01, 5.78 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.252000</td>\n",
       "      <td>2.059629</td>\n",
       "      <td>0.215399</td>\n",
       "      <td>0.057202</td>\n",
       "      <td>0.030015</td>\n",
       "      <td>0.019825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.941500</td>\n",
       "      <td>1.776989</td>\n",
       "      <td>0.434464</td>\n",
       "      <td>0.061822</td>\n",
       "      <td>0.104251</td>\n",
       "      <td>0.075379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.677300</td>\n",
       "      <td>1.526794</td>\n",
       "      <td>0.520623</td>\n",
       "      <td>0.181450</td>\n",
       "      <td>0.160388</td>\n",
       "      <td>0.139922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.424500</td>\n",
       "      <td>1.321145</td>\n",
       "      <td>0.582035</td>\n",
       "      <td>0.224563</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.178076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.231100</td>\n",
       "      <td>1.167420</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.212640</td>\n",
       "      <td>0.241038</td>\n",
       "      <td>0.215045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.059100</td>\n",
       "      <td>1.049191</td>\n",
       "      <td>0.684693</td>\n",
       "      <td>0.261860</td>\n",
       "      <td>0.284809</td>\n",
       "      <td>0.259541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.965206</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.280666</td>\n",
       "      <td>0.295983</td>\n",
       "      <td>0.271454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.824800</td>\n",
       "      <td>0.899943</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.293613</td>\n",
       "      <td>0.315122</td>\n",
       "      <td>0.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.866628</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.331285</td>\n",
       "      <td>0.335363</td>\n",
       "      <td>0.311021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.833254</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.354239</td>\n",
       "      <td>0.358105</td>\n",
       "      <td>0.338725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:34:32,197] Trial 79 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 80 with params: {'learning_rate': 0.000482322168974171, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.161200</td>\n",
       "      <td>1.839872</td>\n",
       "      <td>0.425298</td>\n",
       "      <td>0.061343</td>\n",
       "      <td>0.100387</td>\n",
       "      <td>0.073340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.627800</td>\n",
       "      <td>1.398153</td>\n",
       "      <td>0.556370</td>\n",
       "      <td>0.195288</td>\n",
       "      <td>0.193775</td>\n",
       "      <td>0.171620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.228800</td>\n",
       "      <td>1.106354</td>\n",
       "      <td>0.656279</td>\n",
       "      <td>0.277236</td>\n",
       "      <td>0.259075</td>\n",
       "      <td>0.242219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.941348</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.290195</td>\n",
       "      <td>0.303141</td>\n",
       "      <td>0.281906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>0.850263</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.331523</td>\n",
       "      <td>0.334469</td>\n",
       "      <td>0.312299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.788956</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.381975</td>\n",
       "      <td>0.375990</td>\n",
       "      <td>0.360361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526500</td>\n",
       "      <td>0.767219</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.432223</td>\n",
       "      <td>0.395485</td>\n",
       "      <td>0.389176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.734182</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.420929</td>\n",
       "      <td>0.412914</td>\n",
       "      <td>0.404288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.705041</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.489619</td>\n",
       "      <td>0.436199</td>\n",
       "      <td>0.437093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.690017</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.535750</td>\n",
       "      <td>0.461446</td>\n",
       "      <td>0.473544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.687719</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.530425</td>\n",
       "      <td>0.500198</td>\n",
       "      <td>0.501877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.663177</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.546053</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>0.518948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.655680</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.582821</td>\n",
       "      <td>0.545225</td>\n",
       "      <td>0.554541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.637070</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.605213</td>\n",
       "      <td>0.563795</td>\n",
       "      <td>0.572475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.649189</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.620445</td>\n",
       "      <td>0.572743</td>\n",
       "      <td>0.585236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.645891</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.643085</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>0.599225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.646583</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.634866</td>\n",
       "      <td>0.578965</td>\n",
       "      <td>0.592042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.645428</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.650655</td>\n",
       "      <td>0.594357</td>\n",
       "      <td>0.607369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.651644</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.638314</td>\n",
       "      <td>0.600438</td>\n",
       "      <td>0.606710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.645525</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.668016</td>\n",
       "      <td>0.603597</td>\n",
       "      <td>0.616496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.651360</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.664780</td>\n",
       "      <td>0.616838</td>\n",
       "      <td>0.625326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.649877</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.651348</td>\n",
       "      <td>0.613428</td>\n",
       "      <td>0.616863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.647229</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.659281</td>\n",
       "      <td>0.614179</td>\n",
       "      <td>0.620904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.642984</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.668626</td>\n",
       "      <td>0.614201</td>\n",
       "      <td>0.623556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.644177</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.663403</td>\n",
       "      <td>0.608571</td>\n",
       "      <td>0.617091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.123900</td>\n",
       "      <td>0.643866</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.667568</td>\n",
       "      <td>0.615766</td>\n",
       "      <td>0.625038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.643669</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.667747</td>\n",
       "      <td>0.609213</td>\n",
       "      <td>0.619023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.644865</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.688374</td>\n",
       "      <td>0.624716</td>\n",
       "      <td>0.637477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.643224</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.687838</td>\n",
       "      <td>0.634609</td>\n",
       "      <td>0.643677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.643798</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.687569</td>\n",
       "      <td>0.634482</td>\n",
       "      <td>0.643440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:37:12,325] Trial 80 finished with value: 0.6434396489980481 and parameters: {'learning_rate': 0.000482322168974171, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 81 with params: {'learning_rate': 0.00030341891380744656, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.58 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.236300</td>\n",
       "      <td>1.992979</td>\n",
       "      <td>0.373052</td>\n",
       "      <td>0.064683</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>0.061874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.827900</td>\n",
       "      <td>1.620351</td>\n",
       "      <td>0.506874</td>\n",
       "      <td>0.168646</td>\n",
       "      <td>0.151278</td>\n",
       "      <td>0.131572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.490500</td>\n",
       "      <td>1.329911</td>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.221760</td>\n",
       "      <td>0.200730</td>\n",
       "      <td>0.182643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.219000</td>\n",
       "      <td>1.136013</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.262731</td>\n",
       "      <td>0.276372</td>\n",
       "      <td>0.257597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.019600</td>\n",
       "      <td>1.000922</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>0.274854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.298187</td>\n",
       "      <td>0.319336</td>\n",
       "      <td>0.296959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.846137</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.320632</td>\n",
       "      <td>0.316674</td>\n",
       "      <td>0.297373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.651900</td>\n",
       "      <td>0.816225</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.363098</td>\n",
       "      <td>0.368583</td>\n",
       "      <td>0.346285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.786489</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.384380</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>0.358413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.764073</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.454066</td>\n",
       "      <td>0.392230</td>\n",
       "      <td>0.386221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>0.739997</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.460982</td>\n",
       "      <td>0.418938</td>\n",
       "      <td>0.414088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.725407</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.479231</td>\n",
       "      <td>0.422243</td>\n",
       "      <td>0.420676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.708601</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.488787</td>\n",
       "      <td>0.450747</td>\n",
       "      <td>0.450976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.700090</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.472617</td>\n",
       "      <td>0.452270</td>\n",
       "      <td>0.446550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.689168</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.455183</td>\n",
       "      <td>0.441831</td>\n",
       "      <td>0.437540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.678349</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.498687</td>\n",
       "      <td>0.467798</td>\n",
       "      <td>0.463811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.678522</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.455952</td>\n",
       "      <td>0.457731</td>\n",
       "      <td>0.449407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.667581</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.530173</td>\n",
       "      <td>0.502443</td>\n",
       "      <td>0.505293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.671433</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.515087</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.501122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.658193</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.540112</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>0.522182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:38:59,383] Trial 81 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 82 with params: {'learning_rate': 0.0004900879987755265, 'weight_decay': 0.007, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.54 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.140800</td>\n",
       "      <td>1.838497</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.063213</td>\n",
       "      <td>0.100505</td>\n",
       "      <td>0.073931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.644900</td>\n",
       "      <td>1.432655</td>\n",
       "      <td>0.542621</td>\n",
       "      <td>0.206721</td>\n",
       "      <td>0.175691</td>\n",
       "      <td>0.155222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.273000</td>\n",
       "      <td>1.148571</td>\n",
       "      <td>0.637030</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.244914</td>\n",
       "      <td>0.226192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.984525</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.255653</td>\n",
       "      <td>0.286469</td>\n",
       "      <td>0.257503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>0.877949</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.287782</td>\n",
       "      <td>0.312031</td>\n",
       "      <td>0.286806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.803322</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.379846</td>\n",
       "      <td>0.363590</td>\n",
       "      <td>0.345493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>0.769475</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.419864</td>\n",
       "      <td>0.411931</td>\n",
       "      <td>0.395133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.747883</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.396575</td>\n",
       "      <td>0.417395</td>\n",
       "      <td>0.395628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.717354</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.446065</td>\n",
       "      <td>0.439990</td>\n",
       "      <td>0.432959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.722656</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.477282</td>\n",
       "      <td>0.457670</td>\n",
       "      <td>0.453741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.695629</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.494248</td>\n",
       "      <td>0.461640</td>\n",
       "      <td>0.460084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.676862</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.560348</td>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.519908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.681336</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.537693</td>\n",
       "      <td>0.512344</td>\n",
       "      <td>0.514821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.656275</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.562458</td>\n",
       "      <td>0.533384</td>\n",
       "      <td>0.537249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.666705</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.575257</td>\n",
       "      <td>0.543332</td>\n",
       "      <td>0.549085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.658172</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.635435</td>\n",
       "      <td>0.570196</td>\n",
       "      <td>0.585834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.653994</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.608299</td>\n",
       "      <td>0.548568</td>\n",
       "      <td>0.563240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.653002</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.629594</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.585573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.660977</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.627526</td>\n",
       "      <td>0.572709</td>\n",
       "      <td>0.584422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.657637</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.618175</td>\n",
       "      <td>0.581024</td>\n",
       "      <td>0.585914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:40:47,230] Trial 82 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83 with params: {'learning_rate': 8.639644976082928e-06, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 6.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:44, 6.71 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.462400</td>\n",
       "      <td>2.435615</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.431500</td>\n",
       "      <td>2.406449</td>\n",
       "      <td>0.122823</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.034825</td>\n",
       "      <td>0.008681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.406300</td>\n",
       "      <td>2.378402</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.016814</td>\n",
       "      <td>0.022973</td>\n",
       "      <td>0.011024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.382700</td>\n",
       "      <td>2.355246</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.011787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.360900</td>\n",
       "      <td>2.335207</td>\n",
       "      <td>0.186984</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.010683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.341300</td>\n",
       "      <td>2.317022</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.325900</td>\n",
       "      <td>2.300238</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.310500</td>\n",
       "      <td>2.284456</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.295800</td>\n",
       "      <td>2.269510</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.283400</td>\n",
       "      <td>2.255983</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:41:40,171] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 with params: {'learning_rate': 0.00023656822479860973, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:44, 6.71 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.279800</td>\n",
       "      <td>2.077249</td>\n",
       "      <td>0.203483</td>\n",
       "      <td>0.058890</td>\n",
       "      <td>0.027312</td>\n",
       "      <td>0.018344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.780882</td>\n",
       "      <td>0.443630</td>\n",
       "      <td>0.081551</td>\n",
       "      <td>0.111808</td>\n",
       "      <td>0.084574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.678200</td>\n",
       "      <td>1.522669</td>\n",
       "      <td>0.523373</td>\n",
       "      <td>0.189621</td>\n",
       "      <td>0.159951</td>\n",
       "      <td>0.141225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.425300</td>\n",
       "      <td>1.319507</td>\n",
       "      <td>0.582951</td>\n",
       "      <td>0.201355</td>\n",
       "      <td>0.198880</td>\n",
       "      <td>0.174603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.234300</td>\n",
       "      <td>1.166366</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.260793</td>\n",
       "      <td>0.247090</td>\n",
       "      <td>0.228775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.061800</td>\n",
       "      <td>1.047582</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.266763</td>\n",
       "      <td>0.284809</td>\n",
       "      <td>0.262693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>0.965253</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.274113</td>\n",
       "      <td>0.291397</td>\n",
       "      <td>0.269393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.830100</td>\n",
       "      <td>0.905067</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.301820</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>0.297365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.866358</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.309409</td>\n",
       "      <td>0.324884</td>\n",
       "      <td>0.302473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.837476</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.328373</td>\n",
       "      <td>0.356565</td>\n",
       "      <td>0.332639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:42:33,533] Trial 84 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85 with params: {'learning_rate': 0.0003930226508449561, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.923379</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.055562</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>0.063260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.734100</td>\n",
       "      <td>1.515202</td>\n",
       "      <td>0.530706</td>\n",
       "      <td>0.188014</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.158306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.363000</td>\n",
       "      <td>1.213517</td>\n",
       "      <td>0.629698</td>\n",
       "      <td>0.228124</td>\n",
       "      <td>0.229605</td>\n",
       "      <td>0.214003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.081500</td>\n",
       "      <td>1.038890</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.284562</td>\n",
       "      <td>0.284129</td>\n",
       "      <td>0.262978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.920658</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.306780</td>\n",
       "      <td>0.314635</td>\n",
       "      <td>0.290908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>0.831513</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.345481</td>\n",
       "      <td>0.343329</td>\n",
       "      <td>0.326037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.804213</td>\n",
       "      <td>0.727773</td>\n",
       "      <td>0.370742</td>\n",
       "      <td>0.355466</td>\n",
       "      <td>0.341293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.398152</td>\n",
       "      <td>0.399787</td>\n",
       "      <td>0.384374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>0.738277</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.434396</td>\n",
       "      <td>0.415767</td>\n",
       "      <td>0.409440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.717087</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.477832</td>\n",
       "      <td>0.427854</td>\n",
       "      <td>0.429857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.464187</td>\n",
       "      <td>0.449644</td>\n",
       "      <td>0.442192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.684175</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.512665</td>\n",
       "      <td>0.467927</td>\n",
       "      <td>0.474241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.676628</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.515807</td>\n",
       "      <td>0.488112</td>\n",
       "      <td>0.492876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.665258</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.577679</td>\n",
       "      <td>0.522533</td>\n",
       "      <td>0.530406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.661607</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.516716</td>\n",
       "      <td>0.529161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.649903</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.572520</td>\n",
       "      <td>0.535822</td>\n",
       "      <td>0.540634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.649493</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.583048</td>\n",
       "      <td>0.535275</td>\n",
       "      <td>0.544020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.646636</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.587669</td>\n",
       "      <td>0.538212</td>\n",
       "      <td>0.547744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.650706</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.597054</td>\n",
       "      <td>0.551530</td>\n",
       "      <td>0.559763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.639875</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.591941</td>\n",
       "      <td>0.547809</td>\n",
       "      <td>0.555037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.645117</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.600914</td>\n",
       "      <td>0.550857</td>\n",
       "      <td>0.560764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.641621</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.624868</td>\n",
       "      <td>0.572569</td>\n",
       "      <td>0.583354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.638037</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.625064</td>\n",
       "      <td>0.566977</td>\n",
       "      <td>0.579579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.635108</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.625624</td>\n",
       "      <td>0.563510</td>\n",
       "      <td>0.575734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.635519</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.616531</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.582538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.631488</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.642929</td>\n",
       "      <td>0.586797</td>\n",
       "      <td>0.598823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.634623</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.629488</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.589668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.635191</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.619587</td>\n",
       "      <td>0.570857</td>\n",
       "      <td>0.581175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.633453</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.642075</td>\n",
       "      <td>0.580313</td>\n",
       "      <td>0.594553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.634080</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.641945</td>\n",
       "      <td>0.581162</td>\n",
       "      <td>0.594939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:45:20,201] Trial 85 finished with value: 0.5949393258686231 and parameters: {'learning_rate': 0.0003930226508449561, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 3.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 86 with params: {'learning_rate': 4.058724127258315e-06, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4, 'lambda_param': 0.7000000000000001, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:59 < 01:00, 5.82 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.470300</td>\n",
       "      <td>2.452230</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.454300</td>\n",
       "      <td>2.437497</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.023531</td>\n",
       "      <td>0.004358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.442300</td>\n",
       "      <td>2.424098</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.006646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.431100</td>\n",
       "      <td>2.411335</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>0.008767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>2.398489</td>\n",
       "      <td>0.156737</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.009774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.403800</td>\n",
       "      <td>2.386508</td>\n",
       "      <td>0.174152</td>\n",
       "      <td>0.012936</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.394500</td>\n",
       "      <td>2.375893</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.017037</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.384200</td>\n",
       "      <td>2.366325</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>0.023180</td>\n",
       "      <td>0.011140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.376700</td>\n",
       "      <td>2.357591</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.015098</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.010688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.369000</td>\n",
       "      <td>2.349743</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.010514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.361200</td>\n",
       "      <td>2.342653</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>0.016442</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.010547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.355400</td>\n",
       "      <td>2.336040</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.350100</td>\n",
       "      <td>2.329861</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.343400</td>\n",
       "      <td>2.324212</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.337400</td>\n",
       "      <td>2.318932</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.332300</td>\n",
       "      <td>2.314014</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.009339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.328900</td>\n",
       "      <td>2.309453</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.325200</td>\n",
       "      <td>2.305326</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.301371</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.319900</td>\n",
       "      <td>2.297880</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:47:21,121] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87 with params: {'learning_rate': 0.0004727674222465423, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:13, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.161100</td>\n",
       "      <td>1.845547</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.061149</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.074419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.639800</td>\n",
       "      <td>1.416634</td>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.195187</td>\n",
       "      <td>0.196259</td>\n",
       "      <td>0.174681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.244900</td>\n",
       "      <td>1.119457</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.234709</td>\n",
       "      <td>0.258649</td>\n",
       "      <td>0.238439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.967900</td>\n",
       "      <td>0.950800</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.276843</td>\n",
       "      <td>0.295902</td>\n",
       "      <td>0.269671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>0.859662</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.328580</td>\n",
       "      <td>0.331918</td>\n",
       "      <td>0.307708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.795560</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.400893</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>0.369876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.534600</td>\n",
       "      <td>0.758101</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.400968</td>\n",
       "      <td>0.384933</td>\n",
       "      <td>0.373960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.733127</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.431943</td>\n",
       "      <td>0.416079</td>\n",
       "      <td>0.408988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.709892</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.471756</td>\n",
       "      <td>0.442364</td>\n",
       "      <td>0.439821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.691321</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.496595</td>\n",
       "      <td>0.459362</td>\n",
       "      <td>0.466443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.684814</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.493223</td>\n",
       "      <td>0.467309</td>\n",
       "      <td>0.464862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.657560</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.585581</td>\n",
       "      <td>0.513192</td>\n",
       "      <td>0.530867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.657049</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.597358</td>\n",
       "      <td>0.535453</td>\n",
       "      <td>0.547486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.649229</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.625035</td>\n",
       "      <td>0.572702</td>\n",
       "      <td>0.582161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.617869</td>\n",
       "      <td>0.572024</td>\n",
       "      <td>0.583893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.643106</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.620842</td>\n",
       "      <td>0.571816</td>\n",
       "      <td>0.585440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.642419</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.621559</td>\n",
       "      <td>0.577324</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.642901</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.624752</td>\n",
       "      <td>0.576140</td>\n",
       "      <td>0.586892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.641927</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.594185</td>\n",
       "      <td>0.604797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.642999</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.672490</td>\n",
       "      <td>0.602296</td>\n",
       "      <td>0.619538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.642294</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.658150</td>\n",
       "      <td>0.598988</td>\n",
       "      <td>0.612327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.643681</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.652096</td>\n",
       "      <td>0.606010</td>\n",
       "      <td>0.613321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.643664</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.636241</td>\n",
       "      <td>0.589148</td>\n",
       "      <td>0.597417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.636987</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.655505</td>\n",
       "      <td>0.592047</td>\n",
       "      <td>0.606541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.634186</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.646640</td>\n",
       "      <td>0.595289</td>\n",
       "      <td>0.604020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.635387</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.655526</td>\n",
       "      <td>0.606095</td>\n",
       "      <td>0.612625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.636619</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666155</td>\n",
       "      <td>0.599114</td>\n",
       "      <td>0.613512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.639358</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.660852</td>\n",
       "      <td>0.592726</td>\n",
       "      <td>0.606230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.636408</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.660828</td>\n",
       "      <td>0.596821</td>\n",
       "      <td>0.610428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.636301</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.661312</td>\n",
       "      <td>0.597308</td>\n",
       "      <td>0.610882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--recall/11f90e583db35601050aed380d48e83202a896976b9608432fba9244fb447f24 (last modified on Fri Jan 10 23:14:00 2025) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-15 16:50:37,086] Trial 87 finished with value: 0.6108822045896217 and parameters: {'learning_rate': 0.0004727674222465423, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 3.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88 with params: {'learning_rate': 0.0004096873003513377, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.216100</td>\n",
       "      <td>1.951073</td>\n",
       "      <td>0.383135</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>0.081252</td>\n",
       "      <td>0.061843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.756900</td>\n",
       "      <td>1.540393</td>\n",
       "      <td>0.511457</td>\n",
       "      <td>0.151392</td>\n",
       "      <td>0.154141</td>\n",
       "      <td>0.129401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.386200</td>\n",
       "      <td>1.233472</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.226573</td>\n",
       "      <td>0.218753</td>\n",
       "      <td>0.200941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.096900</td>\n",
       "      <td>1.049171</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.262536</td>\n",
       "      <td>0.273840</td>\n",
       "      <td>0.252474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.925911</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.275904</td>\n",
       "      <td>0.302434</td>\n",
       "      <td>0.276485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>0.840209</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.377726</td>\n",
       "      <td>0.342531</td>\n",
       "      <td>0.327919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.808501</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.395123</td>\n",
       "      <td>0.361744</td>\n",
       "      <td>0.353749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.771740</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.413084</td>\n",
       "      <td>0.397768</td>\n",
       "      <td>0.386027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.751963</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.453666</td>\n",
       "      <td>0.434956</td>\n",
       "      <td>0.431833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.734373</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.468220</td>\n",
       "      <td>0.446076</td>\n",
       "      <td>0.441360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.704011</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.496107</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.448608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.685443</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.557178</td>\n",
       "      <td>0.476824</td>\n",
       "      <td>0.489887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.676073</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.522096</td>\n",
       "      <td>0.491564</td>\n",
       "      <td>0.493929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.671368</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.553059</td>\n",
       "      <td>0.520102</td>\n",
       "      <td>0.522235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.549549</td>\n",
       "      <td>0.524756</td>\n",
       "      <td>0.523890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>0.654271</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.616248</td>\n",
       "      <td>0.557252</td>\n",
       "      <td>0.569269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.650967</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.584741</td>\n",
       "      <td>0.546198</td>\n",
       "      <td>0.553319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.204600</td>\n",
       "      <td>0.644906</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.577766</td>\n",
       "      <td>0.603093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.189900</td>\n",
       "      <td>0.645744</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.636763</td>\n",
       "      <td>0.575015</td>\n",
       "      <td>0.587688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.644216</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.631678</td>\n",
       "      <td>0.580399</td>\n",
       "      <td>0.591487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.645122</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.666458</td>\n",
       "      <td>0.603605</td>\n",
       "      <td>0.618956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.641299</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.640962</td>\n",
       "      <td>0.594508</td>\n",
       "      <td>0.604363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.639169</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.583996</td>\n",
       "      <td>0.599478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.641574</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.659404</td>\n",
       "      <td>0.592365</td>\n",
       "      <td>0.607765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.641882</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.629268</td>\n",
       "      <td>0.584727</td>\n",
       "      <td>0.591116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.637605</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.654924</td>\n",
       "      <td>0.593191</td>\n",
       "      <td>0.607027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.638369</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.591657</td>\n",
       "      <td>0.611948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.637422</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.666294</td>\n",
       "      <td>0.588183</td>\n",
       "      <td>0.608610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.657831</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.608338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.637203</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.657603</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.608123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:53:17,892] Trial 88 finished with value: 0.6081234749779998 and parameters: {'learning_rate': 0.0004096873003513377, 'weight_decay': 0.002, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.1, 'temperature': 3.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 89 with params: {'learning_rate': 0.0004912331295718279, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1, 'lambda_param': 0.9, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 02:03 < 01:02, 5.63 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.165400</td>\n",
       "      <td>1.863477</td>\n",
       "      <td>0.414299</td>\n",
       "      <td>0.065695</td>\n",
       "      <td>0.095633</td>\n",
       "      <td>0.072657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.668200</td>\n",
       "      <td>1.448784</td>\n",
       "      <td>0.544455</td>\n",
       "      <td>0.180533</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>0.157583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.289900</td>\n",
       "      <td>1.171771</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.231773</td>\n",
       "      <td>0.212286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.010700</td>\n",
       "      <td>0.995473</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.263260</td>\n",
       "      <td>0.284817</td>\n",
       "      <td>0.256609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.889097</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.287773</td>\n",
       "      <td>0.313427</td>\n",
       "      <td>0.288925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.819208</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.406264</td>\n",
       "      <td>0.364352</td>\n",
       "      <td>0.355231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.784343</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.390866</td>\n",
       "      <td>0.394835</td>\n",
       "      <td>0.378037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.757763</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.401067</td>\n",
       "      <td>0.403804</td>\n",
       "      <td>0.387773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.727938</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.443430</td>\n",
       "      <td>0.436440</td>\n",
       "      <td>0.431310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>0.725344</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.484962</td>\n",
       "      <td>0.461202</td>\n",
       "      <td>0.456979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.695947</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.486738</td>\n",
       "      <td>0.483271</td>\n",
       "      <td>0.476444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.679331</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.572585</td>\n",
       "      <td>0.509663</td>\n",
       "      <td>0.523405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.665864</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.541406</td>\n",
       "      <td>0.555751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.656035</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.598182</td>\n",
       "      <td>0.567115</td>\n",
       "      <td>0.573888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.663287</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.595927</td>\n",
       "      <td>0.562872</td>\n",
       "      <td>0.568721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.654608</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.622680</td>\n",
       "      <td>0.574892</td>\n",
       "      <td>0.585898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.651866</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.601424</td>\n",
       "      <td>0.557969</td>\n",
       "      <td>0.566146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.647722</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.622001</td>\n",
       "      <td>0.567285</td>\n",
       "      <td>0.579917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.648422</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.621792</td>\n",
       "      <td>0.568016</td>\n",
       "      <td>0.578105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.648929</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.619156</td>\n",
       "      <td>0.578542</td>\n",
       "      <td>0.585127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:55:22,762] Trial 89 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 with params: {'learning_rate': 0.0003984445702332602, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.191700</td>\n",
       "      <td>1.917307</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.055825</td>\n",
       "      <td>0.085013</td>\n",
       "      <td>0.063308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.511801</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.172064</td>\n",
       "      <td>0.174238</td>\n",
       "      <td>0.156975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.362400</td>\n",
       "      <td>1.219666</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.215470</td>\n",
       "      <td>0.205688</td>\n",
       "      <td>0.187492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.085500</td>\n",
       "      <td>1.039284</td>\n",
       "      <td>0.684693</td>\n",
       "      <td>0.260385</td>\n",
       "      <td>0.283630</td>\n",
       "      <td>0.262366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.926767</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.276847</td>\n",
       "      <td>0.309423</td>\n",
       "      <td>0.280457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.832689</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.341314</td>\n",
       "      <td>0.338408</td>\n",
       "      <td>0.322213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>0.804957</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.354395</td>\n",
       "      <td>0.355687</td>\n",
       "      <td>0.339805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.543400</td>\n",
       "      <td>0.760317</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.397350</td>\n",
       "      <td>0.394942</td>\n",
       "      <td>0.381302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.735927</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.440588</td>\n",
       "      <td>0.416031</td>\n",
       "      <td>0.409434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.722363</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.486921</td>\n",
       "      <td>0.438408</td>\n",
       "      <td>0.440559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.702429</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.429223</td>\n",
       "      <td>0.441164</td>\n",
       "      <td>0.426934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.676949</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.521721</td>\n",
       "      <td>0.476423</td>\n",
       "      <td>0.486166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>0.680230</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.519826</td>\n",
       "      <td>0.494016</td>\n",
       "      <td>0.492498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.655829</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.555952</td>\n",
       "      <td>0.526324</td>\n",
       "      <td>0.530281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.654653</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.552838</td>\n",
       "      <td>0.523804</td>\n",
       "      <td>0.525951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.644049</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.579669</td>\n",
       "      <td>0.545112</td>\n",
       "      <td>0.549275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.645864</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.567305</td>\n",
       "      <td>0.544642</td>\n",
       "      <td>0.545820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.647058</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.587667</td>\n",
       "      <td>0.553401</td>\n",
       "      <td>0.557162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.646523</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.590694</td>\n",
       "      <td>0.556738</td>\n",
       "      <td>0.560299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.637040</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.613905</td>\n",
       "      <td>0.569320</td>\n",
       "      <td>0.576527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.635516</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.641737</td>\n",
       "      <td>0.590174</td>\n",
       "      <td>0.603234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.633502</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.638497</td>\n",
       "      <td>0.586536</td>\n",
       "      <td>0.596396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.633546</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.661744</td>\n",
       "      <td>0.596655</td>\n",
       "      <td>0.611994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.630524</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.645097</td>\n",
       "      <td>0.589128</td>\n",
       "      <td>0.601963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.630670</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.645849</td>\n",
       "      <td>0.589775</td>\n",
       "      <td>0.601739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.630149</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.666225</td>\n",
       "      <td>0.610730</td>\n",
       "      <td>0.623132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.631445</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.668929</td>\n",
       "      <td>0.603845</td>\n",
       "      <td>0.621011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.633816</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.664550</td>\n",
       "      <td>0.595711</td>\n",
       "      <td>0.611579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.632351</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.660411</td>\n",
       "      <td>0.594997</td>\n",
       "      <td>0.608868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.632424</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.663525</td>\n",
       "      <td>0.597964</td>\n",
       "      <td>0.612579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 16:58:14,256] Trial 90 finished with value: 0.6125792440648197 and parameters: {'learning_rate': 0.0003984445702332602, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91 with params: {'learning_rate': 0.0004541839293619056, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.140500</td>\n",
       "      <td>1.825048</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.062248</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>0.074881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.622500</td>\n",
       "      <td>1.400143</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.197745</td>\n",
       "      <td>0.196221</td>\n",
       "      <td>0.174042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.239800</td>\n",
       "      <td>1.110647</td>\n",
       "      <td>0.661778</td>\n",
       "      <td>0.237201</td>\n",
       "      <td>0.267420</td>\n",
       "      <td>0.244366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.953107</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.262183</td>\n",
       "      <td>0.298610</td>\n",
       "      <td>0.271320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.785900</td>\n",
       "      <td>0.861525</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.333573</td>\n",
       "      <td>0.333826</td>\n",
       "      <td>0.310629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.795530</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.388666</td>\n",
       "      <td>0.378792</td>\n",
       "      <td>0.362561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.546300</td>\n",
       "      <td>0.757210</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>0.386358</td>\n",
       "      <td>0.372173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.734016</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.451426</td>\n",
       "      <td>0.425137</td>\n",
       "      <td>0.416750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.707046</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.441711</td>\n",
       "      <td>0.438176</td>\n",
       "      <td>0.426322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.512951</td>\n",
       "      <td>0.463199</td>\n",
       "      <td>0.467850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.696564</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.496398</td>\n",
       "      <td>0.474917</td>\n",
       "      <td>0.469038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.671037</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.539640</td>\n",
       "      <td>0.494557</td>\n",
       "      <td>0.501864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.669543</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.548877</td>\n",
       "      <td>0.515871</td>\n",
       "      <td>0.517905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.655982</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.602269</td>\n",
       "      <td>0.567173</td>\n",
       "      <td>0.572668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.654230</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.589699</td>\n",
       "      <td>0.539653</td>\n",
       "      <td>0.549668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.650198</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.633782</td>\n",
       "      <td>0.572960</td>\n",
       "      <td>0.588415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.181600</td>\n",
       "      <td>0.648730</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.634219</td>\n",
       "      <td>0.568012</td>\n",
       "      <td>0.585359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.647578</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.664890</td>\n",
       "      <td>0.583321</td>\n",
       "      <td>0.603331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.649706</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.647819</td>\n",
       "      <td>0.584957</td>\n",
       "      <td>0.599926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.645198</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.681156</td>\n",
       "      <td>0.591661</td>\n",
       "      <td>0.612039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.649779</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.592531</td>\n",
       "      <td>0.613666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.649527</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.671143</td>\n",
       "      <td>0.600936</td>\n",
       "      <td>0.616662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.647296</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.685730</td>\n",
       "      <td>0.597031</td>\n",
       "      <td>0.620811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.642877</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.680031</td>\n",
       "      <td>0.594379</td>\n",
       "      <td>0.613653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.640637</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.662343</td>\n",
       "      <td>0.593650</td>\n",
       "      <td>0.607089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.640698</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.678656</td>\n",
       "      <td>0.605440</td>\n",
       "      <td>0.620673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.639900</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.684008</td>\n",
       "      <td>0.598323</td>\n",
       "      <td>0.619935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.643186</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.667347</td>\n",
       "      <td>0.596284</td>\n",
       "      <td>0.611871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.641632</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.663034</td>\n",
       "      <td>0.596657</td>\n",
       "      <td>0.610184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.641442</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.662341</td>\n",
       "      <td>0.596895</td>\n",
       "      <td>0.610649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:00:55,809] Trial 91 finished with value: 0.6106490970611218 and parameters: {'learning_rate': 0.0004541839293619056, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 0, 'lambda_param': 0.0, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 92 with params: {'learning_rate': 0.0001522132346482949, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:09, 6.76 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.324500</td>\n",
       "      <td>2.164983</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.081400</td>\n",
       "      <td>1.953317</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>0.055251</td>\n",
       "      <td>0.086830</td>\n",
       "      <td>0.064330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.884300</td>\n",
       "      <td>1.749290</td>\n",
       "      <td>0.461962</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.121721</td>\n",
       "      <td>0.091693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.683700</td>\n",
       "      <td>1.572867</td>\n",
       "      <td>0.510541</td>\n",
       "      <td>0.153551</td>\n",
       "      <td>0.153298</td>\n",
       "      <td>0.133009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.523200</td>\n",
       "      <td>1.425990</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>0.188788</td>\n",
       "      <td>0.169936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:01:22,463] Trial 92 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 93 with params: {'learning_rate': 3.7262129220584106e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 1, 'lambda_param': 0.0, 'temperature': 6.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:49, 6.41 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.353207</td>\n",
       "      <td>0.191567</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>0.012595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.329200</td>\n",
       "      <td>2.275892</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.263000</td>\n",
       "      <td>2.207810</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.006558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.198800</td>\n",
       "      <td>2.145761</td>\n",
       "      <td>0.219065</td>\n",
       "      <td>0.078708</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>0.025198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.145300</td>\n",
       "      <td>2.085881</td>\n",
       "      <td>0.340972</td>\n",
       "      <td>0.069318</td>\n",
       "      <td>0.067721</td>\n",
       "      <td>0.059726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.085300</td>\n",
       "      <td>2.031475</td>\n",
       "      <td>0.389551</td>\n",
       "      <td>0.080005</td>\n",
       "      <td>0.082394</td>\n",
       "      <td>0.065580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.032000</td>\n",
       "      <td>1.980226</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.087416</td>\n",
       "      <td>0.064461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.985100</td>\n",
       "      <td>1.931487</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.092476</td>\n",
       "      <td>0.093360</td>\n",
       "      <td>0.070430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.937400</td>\n",
       "      <td>1.884888</td>\n",
       "      <td>0.432631</td>\n",
       "      <td>0.086565</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>0.076695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.895400</td>\n",
       "      <td>1.842573</td>\n",
       "      <td>0.446379</td>\n",
       "      <td>0.104663</td>\n",
       "      <td>0.108513</td>\n",
       "      <td>0.083745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:02:17,801] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 94 with params: {'learning_rate': 0.0003669577048396535, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:54, 6.11 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.206700</td>\n",
       "      <td>1.930734</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.075357</td>\n",
       "      <td>0.085727</td>\n",
       "      <td>0.063836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.745900</td>\n",
       "      <td>1.524736</td>\n",
       "      <td>0.524290</td>\n",
       "      <td>0.181289</td>\n",
       "      <td>0.163908</td>\n",
       "      <td>0.146521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.377700</td>\n",
       "      <td>1.224636</td>\n",
       "      <td>0.636114</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>0.235944</td>\n",
       "      <td>0.215866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.049281</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.260873</td>\n",
       "      <td>0.284193</td>\n",
       "      <td>0.261488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.903100</td>\n",
       "      <td>0.930969</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.299131</td>\n",
       "      <td>0.320589</td>\n",
       "      <td>0.297189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.749500</td>\n",
       "      <td>0.841673</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.328602</td>\n",
       "      <td>0.330562</td>\n",
       "      <td>0.311127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.806005</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.359838</td>\n",
       "      <td>0.345797</td>\n",
       "      <td>0.331954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.773852</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.372782</td>\n",
       "      <td>0.388941</td>\n",
       "      <td>0.368668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.746490</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.439894</td>\n",
       "      <td>0.403315</td>\n",
       "      <td>0.400152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.431300</td>\n",
       "      <td>0.726615</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.435794</td>\n",
       "      <td>0.412508</td>\n",
       "      <td>0.408217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:03:15,857] Trial 94 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95 with params: {'learning_rate': 0.0004930404167082212, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.171100</td>\n",
       "      <td>1.851310</td>\n",
       "      <td>0.423465</td>\n",
       "      <td>0.064792</td>\n",
       "      <td>0.099801</td>\n",
       "      <td>0.074909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.635800</td>\n",
       "      <td>1.407231</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.197176</td>\n",
       "      <td>0.199528</td>\n",
       "      <td>0.177492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.230800</td>\n",
       "      <td>1.102859</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.260788</td>\n",
       "      <td>0.266218</td>\n",
       "      <td>0.247140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.947600</td>\n",
       "      <td>0.933515</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.278866</td>\n",
       "      <td>0.296434</td>\n",
       "      <td>0.273132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.751000</td>\n",
       "      <td>0.846184</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.339928</td>\n",
       "      <td>0.332548</td>\n",
       "      <td>0.307176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.619500</td>\n",
       "      <td>0.785160</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.416813</td>\n",
       "      <td>0.387462</td>\n",
       "      <td>0.379502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.754072</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.395099</td>\n",
       "      <td>0.379181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.729053</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.451727</td>\n",
       "      <td>0.423251</td>\n",
       "      <td>0.418485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.710077</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.515461</td>\n",
       "      <td>0.457492</td>\n",
       "      <td>0.462243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.686855</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.524873</td>\n",
       "      <td>0.452048</td>\n",
       "      <td>0.458879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.687206</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.519595</td>\n",
       "      <td>0.485366</td>\n",
       "      <td>0.480537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.675758</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.574425</td>\n",
       "      <td>0.508339</td>\n",
       "      <td>0.522481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.666462</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.596902</td>\n",
       "      <td>0.535872</td>\n",
       "      <td>0.548744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.649269</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.622086</td>\n",
       "      <td>0.562814</td>\n",
       "      <td>0.574933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.653924</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.627262</td>\n",
       "      <td>0.564437</td>\n",
       "      <td>0.583982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.655385</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.602661</td>\n",
       "      <td>0.562813</td>\n",
       "      <td>0.569232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.645505</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.627625</td>\n",
       "      <td>0.577927</td>\n",
       "      <td>0.589486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.644169</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.625952</td>\n",
       "      <td>0.580280</td>\n",
       "      <td>0.589009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.636731</td>\n",
       "      <td>0.592863</td>\n",
       "      <td>0.598573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.649543</td>\n",
       "      <td>0.808433</td>\n",
       "      <td>0.659196</td>\n",
       "      <td>0.606147</td>\n",
       "      <td>0.615878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.648627</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.650790</td>\n",
       "      <td>0.600422</td>\n",
       "      <td>0.608619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.647965</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.643532</td>\n",
       "      <td>0.606788</td>\n",
       "      <td>0.610905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.643626</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.675077</td>\n",
       "      <td>0.617575</td>\n",
       "      <td>0.630017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.641516</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.676831</td>\n",
       "      <td>0.628409</td>\n",
       "      <td>0.638088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.642883</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.672837</td>\n",
       "      <td>0.634229</td>\n",
       "      <td>0.638554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.641805</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.675392</td>\n",
       "      <td>0.623255</td>\n",
       "      <td>0.634033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.642584</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.678993</td>\n",
       "      <td>0.619851</td>\n",
       "      <td>0.633205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.644974</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.674972</td>\n",
       "      <td>0.618992</td>\n",
       "      <td>0.631187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.642525</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.675094</td>\n",
       "      <td>0.622352</td>\n",
       "      <td>0.632365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.642495</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.675710</td>\n",
       "      <td>0.622319</td>\n",
       "      <td>0.632949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:05:56,426] Trial 95 finished with value: 0.6329485293395112 and parameters: {'learning_rate': 0.0004930404167082212, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 96 with params: {'learning_rate': 0.0004284611116378763, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:53 < 00:56, 6.16 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.185100</td>\n",
       "      <td>1.891037</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>0.088920</td>\n",
       "      <td>0.067207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.690600</td>\n",
       "      <td>1.466537</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.168383</td>\n",
       "      <td>0.183196</td>\n",
       "      <td>0.162436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.306500</td>\n",
       "      <td>1.166556</td>\n",
       "      <td>0.638863</td>\n",
       "      <td>0.262524</td>\n",
       "      <td>0.241098</td>\n",
       "      <td>0.224396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.026700</td>\n",
       "      <td>0.994735</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.297359</td>\n",
       "      <td>0.294194</td>\n",
       "      <td>0.271284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.829900</td>\n",
       "      <td>0.884194</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.322457</td>\n",
       "      <td>0.322054</td>\n",
       "      <td>0.301074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.810534</td>\n",
       "      <td>0.727773</td>\n",
       "      <td>0.382957</td>\n",
       "      <td>0.356642</td>\n",
       "      <td>0.345912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.786310</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.408877</td>\n",
       "      <td>0.382791</td>\n",
       "      <td>0.373974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>0.748353</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.420944</td>\n",
       "      <td>0.411293</td>\n",
       "      <td>0.399824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.733877</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.458233</td>\n",
       "      <td>0.419612</td>\n",
       "      <td>0.417003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.709839</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.491151</td>\n",
       "      <td>0.441049</td>\n",
       "      <td>0.445468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.495824</td>\n",
       "      <td>0.462746</td>\n",
       "      <td>0.463193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.671516</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.525880</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.491425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.581861</td>\n",
       "      <td>0.512807</td>\n",
       "      <td>0.524309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.655638</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.593654</td>\n",
       "      <td>0.547666</td>\n",
       "      <td>0.553968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.652344</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.603020</td>\n",
       "      <td>0.550945</td>\n",
       "      <td>0.561635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.643189</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.616616</td>\n",
       "      <td>0.558181</td>\n",
       "      <td>0.571943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.651693</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.597039</td>\n",
       "      <td>0.553157</td>\n",
       "      <td>0.561444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.649977</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.604597</td>\n",
       "      <td>0.574348</td>\n",
       "      <td>0.577787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.652140</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.592236</td>\n",
       "      <td>0.575345</td>\n",
       "      <td>0.573358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.642808</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.620794</td>\n",
       "      <td>0.586471</td>\n",
       "      <td>0.591893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:07:50,829] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 97 with params: {'learning_rate': 0.0004019522458764724, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.208700</td>\n",
       "      <td>1.938532</td>\n",
       "      <td>0.378552</td>\n",
       "      <td>0.077102</td>\n",
       "      <td>0.080319</td>\n",
       "      <td>0.060279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.745600</td>\n",
       "      <td>1.529682</td>\n",
       "      <td>0.531622</td>\n",
       "      <td>0.184905</td>\n",
       "      <td>0.173611</td>\n",
       "      <td>0.156397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.374600</td>\n",
       "      <td>1.221599</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.220026</td>\n",
       "      <td>0.219224</td>\n",
       "      <td>0.201771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.084800</td>\n",
       "      <td>1.036798</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.266932</td>\n",
       "      <td>0.281734</td>\n",
       "      <td>0.260018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>0.916528</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.313591</td>\n",
       "      <td>0.315667</td>\n",
       "      <td>0.292140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.824974</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.382445</td>\n",
       "      <td>0.341503</td>\n",
       "      <td>0.325939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.606600</td>\n",
       "      <td>0.797023</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.401417</td>\n",
       "      <td>0.373184</td>\n",
       "      <td>0.362799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.760409</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.394895</td>\n",
       "      <td>0.384201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.740283</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.440101</td>\n",
       "      <td>0.425091</td>\n",
       "      <td>0.416393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.723547</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.482276</td>\n",
       "      <td>0.437347</td>\n",
       "      <td>0.439665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.695742</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.494639</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.442933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.691230</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.547641</td>\n",
       "      <td>0.481890</td>\n",
       "      <td>0.494949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.678369</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.514028</td>\n",
       "      <td>0.503343</td>\n",
       "      <td>0.502520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.674090</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.528538</td>\n",
       "      <td>0.517511</td>\n",
       "      <td>0.513211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.669259</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.575818</td>\n",
       "      <td>0.533973</td>\n",
       "      <td>0.540629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.656877</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.636706</td>\n",
       "      <td>0.561797</td>\n",
       "      <td>0.578790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.654730</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.609555</td>\n",
       "      <td>0.548790</td>\n",
       "      <td>0.563516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.651070</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.623376</td>\n",
       "      <td>0.558056</td>\n",
       "      <td>0.573549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.653262</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.640408</td>\n",
       "      <td>0.568424</td>\n",
       "      <td>0.585325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.651559</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.638314</td>\n",
       "      <td>0.572305</td>\n",
       "      <td>0.590505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.650498</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.645041</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>0.604197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.647751</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.645880</td>\n",
       "      <td>0.582417</td>\n",
       "      <td>0.597806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.642789</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.626961</td>\n",
       "      <td>0.573971</td>\n",
       "      <td>0.587109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.642145</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.638473</td>\n",
       "      <td>0.582621</td>\n",
       "      <td>0.595966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.639880</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.622609</td>\n",
       "      <td>0.572259</td>\n",
       "      <td>0.584856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.637699</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.643073</td>\n",
       "      <td>0.584915</td>\n",
       "      <td>0.599997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.638470</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.628343</td>\n",
       "      <td>0.575261</td>\n",
       "      <td>0.588298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.639534</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.629615</td>\n",
       "      <td>0.571936</td>\n",
       "      <td>0.586457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.637402</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.644704</td>\n",
       "      <td>0.582274</td>\n",
       "      <td>0.598244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.637707</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.641332</td>\n",
       "      <td>0.581820</td>\n",
       "      <td>0.596832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:10:40,078] Trial 97 finished with value: 0.5968315955539868 and parameters: {'learning_rate': 0.0004019522458764724, 'weight_decay': 0.002, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98 with params: {'learning_rate': 3.508998405099427e-05, 'weight_decay': 0.003, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.8, 'temperature': 6.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:46 < 00:53, 6.57 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.420900</td>\n",
       "      <td>2.353206</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.021133</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.011417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>2.276414</td>\n",
       "      <td>0.180568</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>0.008087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.264400</td>\n",
       "      <td>2.208102</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.200700</td>\n",
       "      <td>2.145925</td>\n",
       "      <td>0.248396</td>\n",
       "      <td>0.076661</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>0.035716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.147100</td>\n",
       "      <td>2.086784</td>\n",
       "      <td>0.353804</td>\n",
       "      <td>0.065348</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>0.060984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.088300</td>\n",
       "      <td>2.034380</td>\n",
       "      <td>0.391384</td>\n",
       "      <td>0.078778</td>\n",
       "      <td>0.082996</td>\n",
       "      <td>0.065253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.036300</td>\n",
       "      <td>1.984298</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.074660</td>\n",
       "      <td>0.087656</td>\n",
       "      <td>0.065592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.990900</td>\n",
       "      <td>1.936973</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.092365</td>\n",
       "      <td>0.069887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.944700</td>\n",
       "      <td>1.892182</td>\n",
       "      <td>0.432631</td>\n",
       "      <td>0.088401</td>\n",
       "      <td>0.100169</td>\n",
       "      <td>0.077539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.904500</td>\n",
       "      <td>1.851851</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.105750</td>\n",
       "      <td>0.108370</td>\n",
       "      <td>0.085030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.866300</td>\n",
       "      <td>1.813479</td>\n",
       "      <td>0.459212</td>\n",
       "      <td>0.103909</td>\n",
       "      <td>0.116470</td>\n",
       "      <td>0.095531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.824000</td>\n",
       "      <td>1.779278</td>\n",
       "      <td>0.470211</td>\n",
       "      <td>0.104151</td>\n",
       "      <td>0.122624</td>\n",
       "      <td>0.100961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.799700</td>\n",
       "      <td>1.747810</td>\n",
       "      <td>0.484876</td>\n",
       "      <td>0.103443</td>\n",
       "      <td>0.130792</td>\n",
       "      <td>0.106468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.764200</td>\n",
       "      <td>1.718149</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>0.101755</td>\n",
       "      <td>0.132659</td>\n",
       "      <td>0.106952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.735300</td>\n",
       "      <td>1.691286</td>\n",
       "      <td>0.494042</td>\n",
       "      <td>0.102397</td>\n",
       "      <td>0.137168</td>\n",
       "      <td>0.110685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.706500</td>\n",
       "      <td>1.667181</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.101535</td>\n",
       "      <td>0.138293</td>\n",
       "      <td>0.111361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.679800</td>\n",
       "      <td>1.644662</td>\n",
       "      <td>0.501375</td>\n",
       "      <td>0.120383</td>\n",
       "      <td>0.141334</td>\n",
       "      <td>0.113915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.664400</td>\n",
       "      <td>1.624703</td>\n",
       "      <td>0.502291</td>\n",
       "      <td>0.120513</td>\n",
       "      <td>0.142103</td>\n",
       "      <td>0.114362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.645700</td>\n",
       "      <td>1.605806</td>\n",
       "      <td>0.506874</td>\n",
       "      <td>0.142997</td>\n",
       "      <td>0.144811</td>\n",
       "      <td>0.118455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.629300</td>\n",
       "      <td>1.589601</td>\n",
       "      <td>0.512374</td>\n",
       "      <td>0.172786</td>\n",
       "      <td>0.147025</td>\n",
       "      <td>0.121402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:12:27,410] Trial 98 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 99 with params: {'learning_rate': 2.8092689649211102e-06, 'weight_decay': 0.002, 'adam_beta1': 0.97, 'warmup_steps': 1, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:20, 6.22 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>2.456541</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.460500</td>\n",
       "      <td>2.445854</td>\n",
       "      <td>0.016499</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.022113</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.452200</td>\n",
       "      <td>2.436333</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.024996</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>0.004825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.444500</td>\n",
       "      <td>2.427523</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.027066</td>\n",
       "      <td>0.007054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.433200</td>\n",
       "      <td>2.419011</td>\n",
       "      <td>0.085243</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.030406</td>\n",
       "      <td>0.008002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:12:56,534] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 with params: {'learning_rate': 0.00031247000160355786, 'weight_decay': 0.002, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 4, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:52 < 00:56, 6.18 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.253900</td>\n",
       "      <td>2.030040</td>\n",
       "      <td>0.272227</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>0.045897</td>\n",
       "      <td>0.035807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>1.688236</td>\n",
       "      <td>0.466544</td>\n",
       "      <td>0.121142</td>\n",
       "      <td>0.127952</td>\n",
       "      <td>0.100736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.569200</td>\n",
       "      <td>1.414270</td>\n",
       "      <td>0.555454</td>\n",
       "      <td>0.200050</td>\n",
       "      <td>0.182409</td>\n",
       "      <td>0.158408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.293100</td>\n",
       "      <td>1.201154</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.254216</td>\n",
       "      <td>0.241035</td>\n",
       "      <td>0.220316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.084000</td>\n",
       "      <td>1.054075</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.260374</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>0.245711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.943815</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.281101</td>\n",
       "      <td>0.298793</td>\n",
       "      <td>0.275323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.777200</td>\n",
       "      <td>0.875425</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.298723</td>\n",
       "      <td>0.309676</td>\n",
       "      <td>0.289083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.835590</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.372141</td>\n",
       "      <td>0.362534</td>\n",
       "      <td>0.340858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.610700</td>\n",
       "      <td>0.802968</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.396140</td>\n",
       "      <td>0.371583</td>\n",
       "      <td>0.359147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.787309</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.412075</td>\n",
       "      <td>0.389830</td>\n",
       "      <td>0.378635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.758027</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.424730</td>\n",
       "      <td>0.414525</td>\n",
       "      <td>0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.731642</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.455868</td>\n",
       "      <td>0.417417</td>\n",
       "      <td>0.416443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.720176</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.465763</td>\n",
       "      <td>0.435237</td>\n",
       "      <td>0.431314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.477954</td>\n",
       "      <td>0.444340</td>\n",
       "      <td>0.442396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.703416</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.519316</td>\n",
       "      <td>0.466141</td>\n",
       "      <td>0.465972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.692919</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.486702</td>\n",
       "      <td>0.464846</td>\n",
       "      <td>0.461785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.690515</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.502801</td>\n",
       "      <td>0.474317</td>\n",
       "      <td>0.475112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.681802</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.543304</td>\n",
       "      <td>0.503313</td>\n",
       "      <td>0.506003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.506938</td>\n",
       "      <td>0.501616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.671559</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.548405</td>\n",
       "      <td>0.520764</td>\n",
       "      <td>0.522020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:14:50,623] Trial 100 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 101 with params: {'learning_rate': 3.6527199497217975e-06, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 0, 'lambda_param': 0.8, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:14, 6.49 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.470000</td>\n",
       "      <td>2.452723</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.002259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.455400</td>\n",
       "      <td>2.439488</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.022982</td>\n",
       "      <td>0.003626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.444800</td>\n",
       "      <td>2.427739</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.006148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.435000</td>\n",
       "      <td>2.416688</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>0.029577</td>\n",
       "      <td>0.007174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.421800</td>\n",
       "      <td>2.406082</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.009076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:15:18,580] Trial 101 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 102 with params: {'learning_rate': 0.00043327985348239675, 'weight_decay': 0.008, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.189100</td>\n",
       "      <td>1.907102</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>0.075481</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.064480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>1.495747</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.158830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.341200</td>\n",
       "      <td>1.204152</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.212086</td>\n",
       "      <td>0.218742</td>\n",
       "      <td>0.197401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.059800</td>\n",
       "      <td>1.024990</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.247970</td>\n",
       "      <td>0.283511</td>\n",
       "      <td>0.257663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>0.908323</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.283022</td>\n",
       "      <td>0.311517</td>\n",
       "      <td>0.285457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>0.825777</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.372613</td>\n",
       "      <td>0.348679</td>\n",
       "      <td>0.336158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.796247</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.380373</td>\n",
       "      <td>0.370682</td>\n",
       "      <td>0.358837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.761729</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.412985</td>\n",
       "      <td>0.410479</td>\n",
       "      <td>0.396296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.741191</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.430063</td>\n",
       "      <td>0.421238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.718575</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.500716</td>\n",
       "      <td>0.446203</td>\n",
       "      <td>0.446965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.700438</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.493344</td>\n",
       "      <td>0.459227</td>\n",
       "      <td>0.455795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.311300</td>\n",
       "      <td>0.675918</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.540856</td>\n",
       "      <td>0.473726</td>\n",
       "      <td>0.488129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.673599</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.547143</td>\n",
       "      <td>0.509594</td>\n",
       "      <td>0.512482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.656917</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.577532</td>\n",
       "      <td>0.534705</td>\n",
       "      <td>0.540074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.658098</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.566475</td>\n",
       "      <td>0.539794</td>\n",
       "      <td>0.544203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.659808</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.623092</td>\n",
       "      <td>0.565714</td>\n",
       "      <td>0.580394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.653618</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.587015</td>\n",
       "      <td>0.549713</td>\n",
       "      <td>0.555849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.647190</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.550122</td>\n",
       "      <td>0.559437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.600930</td>\n",
       "      <td>0.562264</td>\n",
       "      <td>0.569028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.647876</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.635704</td>\n",
       "      <td>0.592195</td>\n",
       "      <td>0.599759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>0.646345</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.616509</td>\n",
       "      <td>0.583618</td>\n",
       "      <td>0.590312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.639407</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.641795</td>\n",
       "      <td>0.593493</td>\n",
       "      <td>0.603543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.639747</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.615307</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.585646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.641681</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.620469</td>\n",
       "      <td>0.575747</td>\n",
       "      <td>0.583566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.641903</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.612152</td>\n",
       "      <td>0.579572</td>\n",
       "      <td>0.583439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.639222</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.616652</td>\n",
       "      <td>0.575952</td>\n",
       "      <td>0.584037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.641196</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.617772</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.588684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.642870</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.617201</td>\n",
       "      <td>0.577171</td>\n",
       "      <td>0.583813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.641719</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.613099</td>\n",
       "      <td>0.575329</td>\n",
       "      <td>0.581375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.641468</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.613066</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.581520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:18:12,594] Trial 102 finished with value: 0.5815200451182833 and parameters: {'learning_rate': 0.00043327985348239675, 'weight_decay': 0.008, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 4.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 103 with params: {'learning_rate': 0.00021433149271320713, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:52, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.283000</td>\n",
       "      <td>2.095176</td>\n",
       "      <td>0.177819</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.020238</td>\n",
       "      <td>0.006488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.988700</td>\n",
       "      <td>1.832732</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.063062</td>\n",
       "      <td>0.101499</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.737200</td>\n",
       "      <td>1.587104</td>\n",
       "      <td>0.496792</td>\n",
       "      <td>0.156789</td>\n",
       "      <td>0.147351</td>\n",
       "      <td>0.125438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.496000</td>\n",
       "      <td>1.385781</td>\n",
       "      <td>0.571036</td>\n",
       "      <td>0.199103</td>\n",
       "      <td>0.192226</td>\n",
       "      <td>0.168731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.309300</td>\n",
       "      <td>1.233766</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>0.254525</td>\n",
       "      <td>0.238142</td>\n",
       "      <td>0.219579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.139800</td>\n",
       "      <td>1.111964</td>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.262910</td>\n",
       "      <td>0.276766</td>\n",
       "      <td>0.253703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.004400</td>\n",
       "      <td>1.023592</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.291431</td>\n",
       "      <td>0.283307</td>\n",
       "      <td>0.262395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.955900</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.282807</td>\n",
       "      <td>0.305651</td>\n",
       "      <td>0.278437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.909540</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.314625</td>\n",
       "      <td>0.292243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>0.871049</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.338324</td>\n",
       "      <td>0.328839</td>\n",
       "      <td>0.308191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.349025</td>\n",
       "      <td>0.348317</td>\n",
       "      <td>0.327065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.621800</td>\n",
       "      <td>0.812044</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.346657</td>\n",
       "      <td>0.350245</td>\n",
       "      <td>0.332828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.788652</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.376814</td>\n",
       "      <td>0.372876</td>\n",
       "      <td>0.356291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.769249</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.386113</td>\n",
       "      <td>0.383315</td>\n",
       "      <td>0.369072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.518200</td>\n",
       "      <td>0.762312</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.380562</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>0.374568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.487200</td>\n",
       "      <td>0.758779</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.409631</td>\n",
       "      <td>0.405847</td>\n",
       "      <td>0.395313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.749692</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.430982</td>\n",
       "      <td>0.418363</td>\n",
       "      <td>0.408083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.734795</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.420081</td>\n",
       "      <td>0.410649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.729204</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.433727</td>\n",
       "      <td>0.428897</td>\n",
       "      <td>0.415703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.720975</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.455439</td>\n",
       "      <td>0.438284</td>\n",
       "      <td>0.427412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.717911</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.455296</td>\n",
       "      <td>0.437713</td>\n",
       "      <td>0.428075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.710587</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.449927</td>\n",
       "      <td>0.444157</td>\n",
       "      <td>0.432398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.704977</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.445444</td>\n",
       "      <td>0.436752</td>\n",
       "      <td>0.426555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.453270</td>\n",
       "      <td>0.446649</td>\n",
       "      <td>0.434698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.479440</td>\n",
       "      <td>0.454533</td>\n",
       "      <td>0.445948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.701235</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.474525</td>\n",
       "      <td>0.454766</td>\n",
       "      <td>0.447576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.696097</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.479790</td>\n",
       "      <td>0.453927</td>\n",
       "      <td>0.448635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.694235</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.481261</td>\n",
       "      <td>0.454872</td>\n",
       "      <td>0.449731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.693738</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.477435</td>\n",
       "      <td>0.454769</td>\n",
       "      <td>0.447632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.693487</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.478457</td>\n",
       "      <td>0.454733</td>\n",
       "      <td>0.447968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:21:06,429] Trial 103 finished with value: 0.44796836715617006 and parameters: {'learning_rate': 0.00021433149271320713, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 104 with params: {'learning_rate': 0.0004426928660035583, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:42, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.174900</td>\n",
       "      <td>1.878299</td>\n",
       "      <td>0.417049</td>\n",
       "      <td>0.067434</td>\n",
       "      <td>0.095871</td>\n",
       "      <td>0.073997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>1.458836</td>\n",
       "      <td>0.552704</td>\n",
       "      <td>0.178361</td>\n",
       "      <td>0.184218</td>\n",
       "      <td>0.164558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.169682</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.239033</td>\n",
       "      <td>0.235527</td>\n",
       "      <td>0.218443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>0.993084</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.267239</td>\n",
       "      <td>0.287982</td>\n",
       "      <td>0.265184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>0.890703</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.285251</td>\n",
       "      <td>0.320310</td>\n",
       "      <td>0.292231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.810972</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>0.360665</td>\n",
       "      <td>0.349567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>0.782125</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.403610</td>\n",
       "      <td>0.368250</td>\n",
       "      <td>0.357693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.745551</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.408398</td>\n",
       "      <td>0.400606</td>\n",
       "      <td>0.387045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.721341</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.487173</td>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.436484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.711175</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.489292</td>\n",
       "      <td>0.447127</td>\n",
       "      <td>0.450301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.694861</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.476146</td>\n",
       "      <td>0.470314</td>\n",
       "      <td>0.463628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.666569</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.539662</td>\n",
       "      <td>0.489029</td>\n",
       "      <td>0.502109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.666932</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.567818</td>\n",
       "      <td>0.534540</td>\n",
       "      <td>0.534389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.651147</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.581249</td>\n",
       "      <td>0.547803</td>\n",
       "      <td>0.552597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.627714</td>\n",
       "      <td>0.566679</td>\n",
       "      <td>0.579189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.642396</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.617627</td>\n",
       "      <td>0.560029</td>\n",
       "      <td>0.573021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.645406</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.630610</td>\n",
       "      <td>0.572765</td>\n",
       "      <td>0.585961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.639004</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.642398</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.603860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.583603</td>\n",
       "      <td>0.594570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.634428</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.669064</td>\n",
       "      <td>0.604535</td>\n",
       "      <td>0.618921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.639793</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.673821</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.621217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.636879</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.665448</td>\n",
       "      <td>0.608467</td>\n",
       "      <td>0.620431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.637058</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.668360</td>\n",
       "      <td>0.603984</td>\n",
       "      <td>0.619285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.631057</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.659877</td>\n",
       "      <td>0.596079</td>\n",
       "      <td>0.609013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.634545</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.655409</td>\n",
       "      <td>0.607023</td>\n",
       "      <td>0.613823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.630544</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.658348</td>\n",
       "      <td>0.608228</td>\n",
       "      <td>0.615983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.631809</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.658513</td>\n",
       "      <td>0.602871</td>\n",
       "      <td>0.614575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.634819</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.665386</td>\n",
       "      <td>0.602602</td>\n",
       "      <td>0.615864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.632708</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.662223</td>\n",
       "      <td>0.605462</td>\n",
       "      <td>0.616299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.632673</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.660940</td>\n",
       "      <td>0.604510</td>\n",
       "      <td>0.614791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:23:51,331] Trial 104 finished with value: 0.6147910496206811 and parameters: {'learning_rate': 0.0004426928660035583, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 105 with params: {'learning_rate': 0.00042239367254374836, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.198400</td>\n",
       "      <td>1.911746</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.074833</td>\n",
       "      <td>0.087381</td>\n",
       "      <td>0.066579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.711200</td>\n",
       "      <td>1.485274</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>0.165902</td>\n",
       "      <td>0.173724</td>\n",
       "      <td>0.154042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.326600</td>\n",
       "      <td>1.170916</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.248776</td>\n",
       "      <td>0.244778</td>\n",
       "      <td>0.225923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.034200</td>\n",
       "      <td>0.997757</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.270992</td>\n",
       "      <td>0.288657</td>\n",
       "      <td>0.263869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.834300</td>\n",
       "      <td>0.891740</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.315229</td>\n",
       "      <td>0.326237</td>\n",
       "      <td>0.300842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.810611</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.381968</td>\n",
       "      <td>0.361363</td>\n",
       "      <td>0.348960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.785230</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.387409</td>\n",
       "      <td>0.379322</td>\n",
       "      <td>0.365482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>0.746596</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.392155</td>\n",
       "      <td>0.391120</td>\n",
       "      <td>0.376550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.443071</td>\n",
       "      <td>0.439193</td>\n",
       "      <td>0.433874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.712796</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.478933</td>\n",
       "      <td>0.446363</td>\n",
       "      <td>0.446017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.690995</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.507557</td>\n",
       "      <td>0.471930</td>\n",
       "      <td>0.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.687401</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.542482</td>\n",
       "      <td>0.493862</td>\n",
       "      <td>0.502203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.672809</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.522265</td>\n",
       "      <td>0.502596</td>\n",
       "      <td>0.505092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.670305</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.565012</td>\n",
       "      <td>0.544334</td>\n",
       "      <td>0.545195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.663656</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.652145</td>\n",
       "      <td>0.561643</td>\n",
       "      <td>0.584084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.654276</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.643262</td>\n",
       "      <td>0.562860</td>\n",
       "      <td>0.581480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.654562</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.617177</td>\n",
       "      <td>0.551034</td>\n",
       "      <td>0.568425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.652391</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.645585</td>\n",
       "      <td>0.561872</td>\n",
       "      <td>0.582276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.655894</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.638662</td>\n",
       "      <td>0.574130</td>\n",
       "      <td>0.587445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.651518</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.638658</td>\n",
       "      <td>0.567132</td>\n",
       "      <td>0.585070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.648995</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.618334</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.583733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.649527</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.642166</td>\n",
       "      <td>0.599516</td>\n",
       "      <td>0.605444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.644433</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.620992</td>\n",
       "      <td>0.577196</td>\n",
       "      <td>0.586672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.642129</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.634084</td>\n",
       "      <td>0.581190</td>\n",
       "      <td>0.592496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.640230</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.633828</td>\n",
       "      <td>0.585882</td>\n",
       "      <td>0.596367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.639253</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.664688</td>\n",
       "      <td>0.596175</td>\n",
       "      <td>0.612156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.636550</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.624511</td>\n",
       "      <td>0.581418</td>\n",
       "      <td>0.590294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.142900</td>\n",
       "      <td>0.638691</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.627694</td>\n",
       "      <td>0.578081</td>\n",
       "      <td>0.589070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.635867</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.643412</td>\n",
       "      <td>0.587978</td>\n",
       "      <td>0.600881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.636383</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.643496</td>\n",
       "      <td>0.588329</td>\n",
       "      <td>0.601067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:26:39,257] Trial 105 finished with value: 0.6010673472486326 and parameters: {'learning_rate': 0.00042239367254374836, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.2, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106 with params: {'learning_rate': 0.0004967417678089857, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.128000</td>\n",
       "      <td>1.803430</td>\n",
       "      <td>0.434464</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.073716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.593400</td>\n",
       "      <td>1.377466</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.226731</td>\n",
       "      <td>0.206159</td>\n",
       "      <td>0.183338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.206700</td>\n",
       "      <td>1.084097</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.247005</td>\n",
       "      <td>0.262157</td>\n",
       "      <td>0.240156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.929987</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.277927</td>\n",
       "      <td>0.300001</td>\n",
       "      <td>0.270560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.849577</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.346862</td>\n",
       "      <td>0.336231</td>\n",
       "      <td>0.312638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.780425</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.429296</td>\n",
       "      <td>0.392016</td>\n",
       "      <td>0.380567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.751801</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.399829</td>\n",
       "      <td>0.397048</td>\n",
       "      <td>0.382430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.446570</td>\n",
       "      <td>0.432601</td>\n",
       "      <td>0.424656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.704151</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.486890</td>\n",
       "      <td>0.453389</td>\n",
       "      <td>0.448986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.695027</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.543887</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>0.493705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.686019</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.531165</td>\n",
       "      <td>0.498827</td>\n",
       "      <td>0.497304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.670871</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.581063</td>\n",
       "      <td>0.511708</td>\n",
       "      <td>0.524123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.662377</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.581849</td>\n",
       "      <td>0.529237</td>\n",
       "      <td>0.537986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.649998</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.618418</td>\n",
       "      <td>0.576096</td>\n",
       "      <td>0.584987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.651157</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.602013</td>\n",
       "      <td>0.557188</td>\n",
       "      <td>0.565934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.651993</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.649665</td>\n",
       "      <td>0.582842</td>\n",
       "      <td>0.597981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.645218</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.568793</td>\n",
       "      <td>0.580287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.647767</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.634971</td>\n",
       "      <td>0.581642</td>\n",
       "      <td>0.592636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.646870</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.650660</td>\n",
       "      <td>0.587496</td>\n",
       "      <td>0.602399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.645389</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.647295</td>\n",
       "      <td>0.593495</td>\n",
       "      <td>0.604809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.658303</td>\n",
       "      <td>0.593028</td>\n",
       "      <td>0.607121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.649018</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.648533</td>\n",
       "      <td>0.599147</td>\n",
       "      <td>0.609527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.651105</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.684267</td>\n",
       "      <td>0.604894</td>\n",
       "      <td>0.622771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.646066</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.638996</td>\n",
       "      <td>0.593548</td>\n",
       "      <td>0.600540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.642143</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.670177</td>\n",
       "      <td>0.604610</td>\n",
       "      <td>0.618517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.637694</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.670490</td>\n",
       "      <td>0.610177</td>\n",
       "      <td>0.620957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.636387</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.665679</td>\n",
       "      <td>0.602929</td>\n",
       "      <td>0.616180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.638960</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.669549</td>\n",
       "      <td>0.603055</td>\n",
       "      <td>0.617617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.639471</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666232</td>\n",
       "      <td>0.602565</td>\n",
       "      <td>0.616280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.639473</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666037</td>\n",
       "      <td>0.602565</td>\n",
       "      <td>0.615834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:29:25,062] Trial 106 finished with value: 0.6158337605951746 and parameters: {'learning_rate': 0.0004967417678089857, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 107 with params: {'learning_rate': 7.723958845300466e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.49 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.374000</td>\n",
       "      <td>2.271842</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.224900</td>\n",
       "      <td>2.141753</td>\n",
       "      <td>0.186068</td>\n",
       "      <td>0.043571</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.010941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.111400</td>\n",
       "      <td>2.025009</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.058969</td>\n",
       "      <td>0.081384</td>\n",
       "      <td>0.063467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.997400</td>\n",
       "      <td>1.913824</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.071456</td>\n",
       "      <td>0.093261</td>\n",
       "      <td>0.069376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.897900</td>\n",
       "      <td>1.809557</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.103677</td>\n",
       "      <td>0.109823</td>\n",
       "      <td>0.084810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.788700</td>\n",
       "      <td>1.712882</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.101762</td>\n",
       "      <td>0.128383</td>\n",
       "      <td>0.101871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.695200</td>\n",
       "      <td>1.629927</td>\n",
       "      <td>0.500458</td>\n",
       "      <td>0.161381</td>\n",
       "      <td>0.144981</td>\n",
       "      <td>0.122439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.619600</td>\n",
       "      <td>1.556277</td>\n",
       "      <td>0.522456</td>\n",
       "      <td>0.213527</td>\n",
       "      <td>0.158456</td>\n",
       "      <td>0.138892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.541300</td>\n",
       "      <td>1.489417</td>\n",
       "      <td>0.551787</td>\n",
       "      <td>0.226358</td>\n",
       "      <td>0.183737</td>\n",
       "      <td>0.168716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.471100</td>\n",
       "      <td>1.432701</td>\n",
       "      <td>0.575619</td>\n",
       "      <td>0.240237</td>\n",
       "      <td>0.199093</td>\n",
       "      <td>0.182512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:30:19,811] Trial 107 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 108 with params: {'learning_rate': 0.0004945187043874618, 'weight_decay': 0.005, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.153700</td>\n",
       "      <td>1.830289</td>\n",
       "      <td>0.433547</td>\n",
       "      <td>0.059676</td>\n",
       "      <td>0.106491</td>\n",
       "      <td>0.073840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.619800</td>\n",
       "      <td>1.397166</td>\n",
       "      <td>0.570119</td>\n",
       "      <td>0.195290</td>\n",
       "      <td>0.201003</td>\n",
       "      <td>0.178496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.219900</td>\n",
       "      <td>1.099652</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.256666</td>\n",
       "      <td>0.269407</td>\n",
       "      <td>0.247353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.936614</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.287360</td>\n",
       "      <td>0.298620</td>\n",
       "      <td>0.274430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>0.852160</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.344647</td>\n",
       "      <td>0.333449</td>\n",
       "      <td>0.310840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>0.785748</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.425448</td>\n",
       "      <td>0.388676</td>\n",
       "      <td>0.380623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.756949</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.389789</td>\n",
       "      <td>0.387591</td>\n",
       "      <td>0.374682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.439187</td>\n",
       "      <td>0.428058</td>\n",
       "      <td>0.419510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.704464</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.494043</td>\n",
       "      <td>0.457184</td>\n",
       "      <td>0.459874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.509156</td>\n",
       "      <td>0.478116</td>\n",
       "      <td>0.483099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.293900</td>\n",
       "      <td>0.679661</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.496158</td>\n",
       "      <td>0.482838</td>\n",
       "      <td>0.477189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.657635</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.616970</td>\n",
       "      <td>0.525845</td>\n",
       "      <td>0.547293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.654652</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.604607</td>\n",
       "      <td>0.550685</td>\n",
       "      <td>0.561795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.651011</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.624085</td>\n",
       "      <td>0.580106</td>\n",
       "      <td>0.589235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.643860</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.613890</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.574514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.651441</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.640608</td>\n",
       "      <td>0.581258</td>\n",
       "      <td>0.594757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.643179</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.624435</td>\n",
       "      <td>0.577599</td>\n",
       "      <td>0.586476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.644804</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.643858</td>\n",
       "      <td>0.592139</td>\n",
       "      <td>0.601171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.645315</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.666575</td>\n",
       "      <td>0.593906</td>\n",
       "      <td>0.611971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.637969</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.648007</td>\n",
       "      <td>0.596019</td>\n",
       "      <td>0.606249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.643794</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.650199</td>\n",
       "      <td>0.605653</td>\n",
       "      <td>0.611787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.642028</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.648132</td>\n",
       "      <td>0.606641</td>\n",
       "      <td>0.612048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.643203</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.652910</td>\n",
       "      <td>0.610962</td>\n",
       "      <td>0.617539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.645066</td>\n",
       "      <td>0.601370</td>\n",
       "      <td>0.605584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.633507</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.650437</td>\n",
       "      <td>0.611919</td>\n",
       "      <td>0.613080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.635911</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.641665</td>\n",
       "      <td>0.608521</td>\n",
       "      <td>0.607123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.636272</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.651690</td>\n",
       "      <td>0.612828</td>\n",
       "      <td>0.614759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.638272</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.653819</td>\n",
       "      <td>0.610319</td>\n",
       "      <td>0.614267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.636770</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.647262</td>\n",
       "      <td>0.608744</td>\n",
       "      <td>0.610630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.636685</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.647909</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.611741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:33:07,893] Trial 108 finished with value: 0.6117411500762256 and parameters: {'learning_rate': 0.0004945187043874618, 'weight_decay': 0.005, 'adam_beta1': 0.92, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 109 with params: {'learning_rate': 0.00047340348163873975, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:48, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.139700</td>\n",
       "      <td>1.829575</td>\n",
       "      <td>0.428048</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.103873</td>\n",
       "      <td>0.074085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.630500</td>\n",
       "      <td>1.412046</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.207433</td>\n",
       "      <td>0.197211</td>\n",
       "      <td>0.174673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.253900</td>\n",
       "      <td>1.126022</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.229003</td>\n",
       "      <td>0.248883</td>\n",
       "      <td>0.230720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.984500</td>\n",
       "      <td>0.960920</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.252516</td>\n",
       "      <td>0.289589</td>\n",
       "      <td>0.262491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>0.870122</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.315256</td>\n",
       "      <td>0.326717</td>\n",
       "      <td>0.299867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.796825</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.411442</td>\n",
       "      <td>0.376201</td>\n",
       "      <td>0.360850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.543100</td>\n",
       "      <td>0.753604</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.402296</td>\n",
       "      <td>0.398828</td>\n",
       "      <td>0.386790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.728568</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.422218</td>\n",
       "      <td>0.422240</td>\n",
       "      <td>0.411607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.712462</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.479335</td>\n",
       "      <td>0.442455</td>\n",
       "      <td>0.436262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.708191</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.526648</td>\n",
       "      <td>0.469112</td>\n",
       "      <td>0.472238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.522887</td>\n",
       "      <td>0.490189</td>\n",
       "      <td>0.489036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.670562</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.553579</td>\n",
       "      <td>0.502249</td>\n",
       "      <td>0.511399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.660585</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.563323</td>\n",
       "      <td>0.537830</td>\n",
       "      <td>0.540605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.650873</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.586575</td>\n",
       "      <td>0.553773</td>\n",
       "      <td>0.558359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.659214</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.604535</td>\n",
       "      <td>0.554378</td>\n",
       "      <td>0.565044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.652098</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.624975</td>\n",
       "      <td>0.572875</td>\n",
       "      <td>0.586316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.646205</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.625590</td>\n",
       "      <td>0.561605</td>\n",
       "      <td>0.574744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.645374</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.600253</td>\n",
       "      <td>0.559145</td>\n",
       "      <td>0.567941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.654965</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.647299</td>\n",
       "      <td>0.587856</td>\n",
       "      <td>0.602449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.653486</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.637703</td>\n",
       "      <td>0.589691</td>\n",
       "      <td>0.597968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.655114</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.654174</td>\n",
       "      <td>0.591782</td>\n",
       "      <td>0.604886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.655815</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.676935</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.620881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.655221</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.658827</td>\n",
       "      <td>0.589708</td>\n",
       "      <td>0.603741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.653625</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.648899</td>\n",
       "      <td>0.588275</td>\n",
       "      <td>0.600441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.653320</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.655804</td>\n",
       "      <td>0.591891</td>\n",
       "      <td>0.604316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.650987</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.666889</td>\n",
       "      <td>0.602148</td>\n",
       "      <td>0.616244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.650187</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.669387</td>\n",
       "      <td>0.601061</td>\n",
       "      <td>0.616151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.654870</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.671033</td>\n",
       "      <td>0.597980</td>\n",
       "      <td>0.613811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.651532</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.672062</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.616032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.651461</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.670842</td>\n",
       "      <td>0.600111</td>\n",
       "      <td>0.615127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:35:58,161] Trial 109 finished with value: 0.6151267952214267 and parameters: {'learning_rate': 0.00047340348163873975, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 3.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 110 with params: {'learning_rate': 0.0002690764526187635, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:55 < 01:50, 6.32 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.236900</td>\n",
       "      <td>2.027408</td>\n",
       "      <td>0.330889</td>\n",
       "      <td>0.071073</td>\n",
       "      <td>0.063603</td>\n",
       "      <td>0.051141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.888100</td>\n",
       "      <td>1.700961</td>\n",
       "      <td>0.450962</td>\n",
       "      <td>0.102441</td>\n",
       "      <td>0.119399</td>\n",
       "      <td>0.092898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.591000</td>\n",
       "      <td>1.437846</td>\n",
       "      <td>0.542621</td>\n",
       "      <td>0.163289</td>\n",
       "      <td>0.170539</td>\n",
       "      <td>0.149340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.334300</td>\n",
       "      <td>1.241783</td>\n",
       "      <td>0.600367</td>\n",
       "      <td>0.219442</td>\n",
       "      <td>0.216414</td>\n",
       "      <td>0.196362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.142800</td>\n",
       "      <td>1.092333</td>\n",
       "      <td>0.677360</td>\n",
       "      <td>0.256125</td>\n",
       "      <td>0.272079</td>\n",
       "      <td>0.251365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>0.978407</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.281136</td>\n",
       "      <td>0.294011</td>\n",
       "      <td>0.269976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>0.905176</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.314901</td>\n",
       "      <td>0.306265</td>\n",
       "      <td>0.286204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.855869</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.312386</td>\n",
       "      <td>0.334383</td>\n",
       "      <td>0.309936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.820265</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.324708</td>\n",
       "      <td>0.345018</td>\n",
       "      <td>0.321965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.798253</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.372282</td>\n",
       "      <td>0.372882</td>\n",
       "      <td>0.350651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:36:54,427] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 111 with params: {'learning_rate': 0.0004573571215089706, 'weight_decay': 0.01, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.30000000000000004, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.188000</td>\n",
       "      <td>1.892822</td>\n",
       "      <td>0.405133</td>\n",
       "      <td>0.072974</td>\n",
       "      <td>0.090904</td>\n",
       "      <td>0.070237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.687600</td>\n",
       "      <td>1.462570</td>\n",
       "      <td>0.547204</td>\n",
       "      <td>0.170906</td>\n",
       "      <td>0.183777</td>\n",
       "      <td>0.162342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.296300</td>\n",
       "      <td>1.148003</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.240543</td>\n",
       "      <td>0.242577</td>\n",
       "      <td>0.224622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.003900</td>\n",
       "      <td>0.974327</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.272480</td>\n",
       "      <td>0.290819</td>\n",
       "      <td>0.266292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.871029</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.313539</td>\n",
       "      <td>0.325638</td>\n",
       "      <td>0.300424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.799536</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.391225</td>\n",
       "      <td>0.366396</td>\n",
       "      <td>0.351218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>0.777753</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.394030</td>\n",
       "      <td>0.387136</td>\n",
       "      <td>0.368955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.742215</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.407223</td>\n",
       "      <td>0.407020</td>\n",
       "      <td>0.391846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.730688</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.464129</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.437308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.709490</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.521767</td>\n",
       "      <td>0.451716</td>\n",
       "      <td>0.458344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.507440</td>\n",
       "      <td>0.475969</td>\n",
       "      <td>0.471671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.289600</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.566178</td>\n",
       "      <td>0.506298</td>\n",
       "      <td>0.517476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.664454</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.557796</td>\n",
       "      <td>0.528583</td>\n",
       "      <td>0.531389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.662434</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.592032</td>\n",
       "      <td>0.542838</td>\n",
       "      <td>0.550804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.656002</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.633815</td>\n",
       "      <td>0.565846</td>\n",
       "      <td>0.584308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.648584</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.550511</td>\n",
       "      <td>0.566702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.642707</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.632042</td>\n",
       "      <td>0.563260</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.642142</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.652651</td>\n",
       "      <td>0.572721</td>\n",
       "      <td>0.594717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.649368</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.631741</td>\n",
       "      <td>0.571534</td>\n",
       "      <td>0.584578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.646476</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.628328</td>\n",
       "      <td>0.582561</td>\n",
       "      <td>0.592171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.642452</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.635737</td>\n",
       "      <td>0.588378</td>\n",
       "      <td>0.597571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.641840</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.636535</td>\n",
       "      <td>0.593651</td>\n",
       "      <td>0.601342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.641125</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.615872</td>\n",
       "      <td>0.576734</td>\n",
       "      <td>0.583914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.636747</td>\n",
       "      <td>0.584986</td>\n",
       "      <td>0.595170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.635973</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.639619</td>\n",
       "      <td>0.595602</td>\n",
       "      <td>0.603452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.632319</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.634252</td>\n",
       "      <td>0.592722</td>\n",
       "      <td>0.599495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.632154</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.642965</td>\n",
       "      <td>0.587797</td>\n",
       "      <td>0.600522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.634847</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.649872</td>\n",
       "      <td>0.590718</td>\n",
       "      <td>0.603549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.631408</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.643929</td>\n",
       "      <td>0.592079</td>\n",
       "      <td>0.602899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.631794</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.643929</td>\n",
       "      <td>0.592079</td>\n",
       "      <td>0.602899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:39:46,280] Trial 111 finished with value: 0.6028985974782685 and parameters: {'learning_rate': 0.0004573571215089706, 'weight_decay': 0.01, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.30000000000000004, 'temperature': 3.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 112 with params: {'learning_rate': 0.00028695272086166576, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:00, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.227000</td>\n",
       "      <td>2.008240</td>\n",
       "      <td>0.361137</td>\n",
       "      <td>0.066490</td>\n",
       "      <td>0.072995</td>\n",
       "      <td>0.059026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.861300</td>\n",
       "      <td>1.668346</td>\n",
       "      <td>0.462878</td>\n",
       "      <td>0.142420</td>\n",
       "      <td>0.126167</td>\n",
       "      <td>0.101912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.552100</td>\n",
       "      <td>1.399514</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.201982</td>\n",
       "      <td>0.174713</td>\n",
       "      <td>0.153572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.291100</td>\n",
       "      <td>1.207059</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.236520</td>\n",
       "      <td>0.219048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.100200</td>\n",
       "      <td>1.060250</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.260588</td>\n",
       "      <td>0.283627</td>\n",
       "      <td>0.260731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.948507</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.307025</td>\n",
       "      <td>0.285664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>0.881115</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.319294</td>\n",
       "      <td>0.308192</td>\n",
       "      <td>0.288522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.836626</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.309717</td>\n",
       "      <td>0.338979</td>\n",
       "      <td>0.311652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.632000</td>\n",
       "      <td>0.801104</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.368403</td>\n",
       "      <td>0.373464</td>\n",
       "      <td>0.358308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.560600</td>\n",
       "      <td>0.792081</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.423483</td>\n",
       "      <td>0.391009</td>\n",
       "      <td>0.376003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.441426</td>\n",
       "      <td>0.412602</td>\n",
       "      <td>0.401343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.732927</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.423812</td>\n",
       "      <td>0.417737</td>\n",
       "      <td>0.405902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>0.439123</td>\n",
       "      <td>0.429770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.702636</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.484299</td>\n",
       "      <td>0.451861</td>\n",
       "      <td>0.445135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.696754</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.466477</td>\n",
       "      <td>0.447725</td>\n",
       "      <td>0.442597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.688171</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.465604</td>\n",
       "      <td>0.462001</td>\n",
       "      <td>0.451977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.679765</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.485864</td>\n",
       "      <td>0.461114</td>\n",
       "      <td>0.454647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.674111</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.535158</td>\n",
       "      <td>0.486045</td>\n",
       "      <td>0.489066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.286400</td>\n",
       "      <td>0.664975</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.529424</td>\n",
       "      <td>0.487725</td>\n",
       "      <td>0.488294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.657505</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.537759</td>\n",
       "      <td>0.500710</td>\n",
       "      <td>0.501802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.660867</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.562915</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.514254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.656621</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.549841</td>\n",
       "      <td>0.519686</td>\n",
       "      <td>0.519765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.653193</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.552008</td>\n",
       "      <td>0.519895</td>\n",
       "      <td>0.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.652845</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.566882</td>\n",
       "      <td>0.540595</td>\n",
       "      <td>0.542159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.654021</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.567315</td>\n",
       "      <td>0.530983</td>\n",
       "      <td>0.535294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.648605</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.615866</td>\n",
       "      <td>0.562842</td>\n",
       "      <td>0.573018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.646534</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.585049</td>\n",
       "      <td>0.534950</td>\n",
       "      <td>0.545223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.647225</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.578496</td>\n",
       "      <td>0.534625</td>\n",
       "      <td>0.541666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.646091</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.597534</td>\n",
       "      <td>0.554317</td>\n",
       "      <td>0.562688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.645930</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.601701</td>\n",
       "      <td>0.558317</td>\n",
       "      <td>0.566913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:42:49,679] Trial 112 finished with value: 0.5669131632795275 and parameters: {'learning_rate': 0.00028695272086166576, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 113 with params: {'learning_rate': 0.00031654079483112716, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.52 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.205200</td>\n",
       "      <td>1.954818</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>0.055622</td>\n",
       "      <td>0.085444</td>\n",
       "      <td>0.063795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.789800</td>\n",
       "      <td>1.579841</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.141763</td>\n",
       "      <td>0.147665</td>\n",
       "      <td>0.124889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.451900</td>\n",
       "      <td>1.298661</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.219796</td>\n",
       "      <td>0.204667</td>\n",
       "      <td>0.187111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.186500</td>\n",
       "      <td>1.117799</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.256162</td>\n",
       "      <td>0.273659</td>\n",
       "      <td>0.253378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.986746</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.290259</td>\n",
       "      <td>0.300952</td>\n",
       "      <td>0.277954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.889564</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.301144</td>\n",
       "      <td>0.322609</td>\n",
       "      <td>0.301505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>0.835463</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.337826</td>\n",
       "      <td>0.332248</td>\n",
       "      <td>0.313309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.806138</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.358304</td>\n",
       "      <td>0.364257</td>\n",
       "      <td>0.342856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.768592</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.384593</td>\n",
       "      <td>0.394128</td>\n",
       "      <td>0.379050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.756601</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.454385</td>\n",
       "      <td>0.404951</td>\n",
       "      <td>0.401052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:43:44,156] Trial 113 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 114 with params: {'learning_rate': 0.0001989498646178862, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:08, 6.79 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.277500</td>\n",
       "      <td>2.095581</td>\n",
       "      <td>0.190651</td>\n",
       "      <td>0.039156</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.012018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.989100</td>\n",
       "      <td>1.834742</td>\n",
       "      <td>0.426214</td>\n",
       "      <td>0.066224</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>0.073713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>1.595624</td>\n",
       "      <td>0.494042</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.141939</td>\n",
       "      <td>0.121838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.512600</td>\n",
       "      <td>1.403831</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.212828</td>\n",
       "      <td>0.184177</td>\n",
       "      <td>0.163604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.254871</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>0.221277</td>\n",
       "      <td>0.202815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:44:10,753] Trial 114 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 115 with params: {'learning_rate': 0.0004126342357093529, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:50, 6.34 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.189000</td>\n",
       "      <td>1.914884</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.064439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.726800</td>\n",
       "      <td>1.512942</td>\n",
       "      <td>0.534372</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>0.173775</td>\n",
       "      <td>0.156121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.363800</td>\n",
       "      <td>1.225156</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>0.216159</td>\n",
       "      <td>0.206639</td>\n",
       "      <td>0.185785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.085500</td>\n",
       "      <td>1.043268</td>\n",
       "      <td>0.673694</td>\n",
       "      <td>0.248693</td>\n",
       "      <td>0.277131</td>\n",
       "      <td>0.253868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>0.923197</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.304048</td>\n",
       "      <td>0.278044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.831985</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.345888</td>\n",
       "      <td>0.334901</td>\n",
       "      <td>0.319537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.802685</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.385574</td>\n",
       "      <td>0.362323</td>\n",
       "      <td>0.348866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.764789</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.400109</td>\n",
       "      <td>0.395024</td>\n",
       "      <td>0.378259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.740555</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.465585</td>\n",
       "      <td>0.418453</td>\n",
       "      <td>0.415178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.722265</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.442362</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:45:06,661] Trial 115 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 116 with params: {'learning_rate': 0.00037502909704266116, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 01:07 < 02:16, 5.13 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.210500</td>\n",
       "      <td>1.929825</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>0.075181</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.064303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.739600</td>\n",
       "      <td>1.514488</td>\n",
       "      <td>0.526123</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.162589</td>\n",
       "      <td>0.143573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.364100</td>\n",
       "      <td>1.210308</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.247707</td>\n",
       "      <td>0.245712</td>\n",
       "      <td>0.226212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.085400</td>\n",
       "      <td>1.036270</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.260895</td>\n",
       "      <td>0.285806</td>\n",
       "      <td>0.263464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.889500</td>\n",
       "      <td>0.924084</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.303545</td>\n",
       "      <td>0.318719</td>\n",
       "      <td>0.297574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.836011</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.339855</td>\n",
       "      <td>0.323258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.801567</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.367502</td>\n",
       "      <td>0.358337</td>\n",
       "      <td>0.343420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.771676</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.377783</td>\n",
       "      <td>0.390816</td>\n",
       "      <td>0.371129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.741452</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.451180</td>\n",
       "      <td>0.402309</td>\n",
       "      <td>0.397262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.719981</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.477340</td>\n",
       "      <td>0.419949</td>\n",
       "      <td>0.421953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:46:15,621] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 117 with params: {'learning_rate': 0.0004309777887751602, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.179200</td>\n",
       "      <td>1.888159</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.071227</td>\n",
       "      <td>0.092748</td>\n",
       "      <td>0.072397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.693700</td>\n",
       "      <td>1.471798</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.174969</td>\n",
       "      <td>0.181071</td>\n",
       "      <td>0.162166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.316800</td>\n",
       "      <td>1.182090</td>\n",
       "      <td>0.632447</td>\n",
       "      <td>0.240837</td>\n",
       "      <td>0.235703</td>\n",
       "      <td>0.219220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>1.004092</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.261349</td>\n",
       "      <td>0.287728</td>\n",
       "      <td>0.265050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>0.897964</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.284299</td>\n",
       "      <td>0.319606</td>\n",
       "      <td>0.290624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>0.814067</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.384581</td>\n",
       "      <td>0.352358</td>\n",
       "      <td>0.339280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.788888</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.396601</td>\n",
       "      <td>0.366468</td>\n",
       "      <td>0.353525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.404685</td>\n",
       "      <td>0.401503</td>\n",
       "      <td>0.386562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.459674</td>\n",
       "      <td>0.427182</td>\n",
       "      <td>0.419503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.713682</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.489508</td>\n",
       "      <td>0.429908</td>\n",
       "      <td>0.433696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.698290</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.502990</td>\n",
       "      <td>0.478636</td>\n",
       "      <td>0.474762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.667898</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.536609</td>\n",
       "      <td>0.482662</td>\n",
       "      <td>0.495632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.275200</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.526046</td>\n",
       "      <td>0.508012</td>\n",
       "      <td>0.508189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.647826</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.577292</td>\n",
       "      <td>0.541666</td>\n",
       "      <td>0.544449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.653548</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.576039</td>\n",
       "      <td>0.545827</td>\n",
       "      <td>0.548578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.644034</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.597273</td>\n",
       "      <td>0.557768</td>\n",
       "      <td>0.566124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.643581</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.628902</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.579969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.642245</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.565796</td>\n",
       "      <td>0.574196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.648710</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.630756</td>\n",
       "      <td>0.578505</td>\n",
       "      <td>0.587549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.617242</td>\n",
       "      <td>0.575783</td>\n",
       "      <td>0.583609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.639100</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.671836</td>\n",
       "      <td>0.599615</td>\n",
       "      <td>0.618991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.639606</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.666963</td>\n",
       "      <td>0.606377</td>\n",
       "      <td>0.619673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.639311</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.670312</td>\n",
       "      <td>0.603692</td>\n",
       "      <td>0.619672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.635287</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.663301</td>\n",
       "      <td>0.598363</td>\n",
       "      <td>0.613530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.638374</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.667969</td>\n",
       "      <td>0.603147</td>\n",
       "      <td>0.616397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.636337</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.662844</td>\n",
       "      <td>0.607599</td>\n",
       "      <td>0.617979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.637551</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.666269</td>\n",
       "      <td>0.601919</td>\n",
       "      <td>0.616136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.640430</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.599508</td>\n",
       "      <td>0.614845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.665414</td>\n",
       "      <td>0.604055</td>\n",
       "      <td>0.617006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.638533</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.664894</td>\n",
       "      <td>0.604055</td>\n",
       "      <td>0.616642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:49:04,151] Trial 117 finished with value: 0.6166420359604712 and parameters: {'learning_rate': 0.0004309777887751602, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.2, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 118 with params: {'learning_rate': 2.6227222122491564e-06, 'weight_decay': 0.004, 'adam_beta1': 0.93, 'warmup_steps': 0, 'lambda_param': 0.7000000000000001, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:20, 6.25 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.472100</td>\n",
       "      <td>2.457042</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.461400</td>\n",
       "      <td>2.446976</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.453600</td>\n",
       "      <td>2.437929</td>\n",
       "      <td>0.027498</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.023707</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.446300</td>\n",
       "      <td>2.429494</td>\n",
       "      <td>0.045830</td>\n",
       "      <td>0.034210</td>\n",
       "      <td>0.026957</td>\n",
       "      <td>0.007786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.435400</td>\n",
       "      <td>2.421363</td>\n",
       "      <td>0.080660</td>\n",
       "      <td>0.010830</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0.009066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:49:33,035] Trial 118 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 119 with params: {'learning_rate': 2.5882714663975125e-05, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.4, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:11, 6.64 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.438400</td>\n",
       "      <td>2.381711</td>\n",
       "      <td>0.183318</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.010424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.363500</td>\n",
       "      <td>2.318153</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.311100</td>\n",
       "      <td>2.265212</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.260900</td>\n",
       "      <td>2.216817</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.218500</td>\n",
       "      <td>2.169059</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.027958</td>\n",
       "      <td>0.018723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:50:00,368] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 120 with params: {'learning_rate': 0.0004264639421009551, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.180900</td>\n",
       "      <td>1.891959</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.071194</td>\n",
       "      <td>0.092103</td>\n",
       "      <td>0.071821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.698300</td>\n",
       "      <td>1.476900</td>\n",
       "      <td>0.542621</td>\n",
       "      <td>0.176828</td>\n",
       "      <td>0.179674</td>\n",
       "      <td>0.161008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.322700</td>\n",
       "      <td>1.186864</td>\n",
       "      <td>0.628781</td>\n",
       "      <td>0.220507</td>\n",
       "      <td>0.231169</td>\n",
       "      <td>0.213177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.047300</td>\n",
       "      <td>1.008272</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.262149</td>\n",
       "      <td>0.287057</td>\n",
       "      <td>0.265105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.901242</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.283229</td>\n",
       "      <td>0.316273</td>\n",
       "      <td>0.287757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>0.815961</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.374390</td>\n",
       "      <td>0.350955</td>\n",
       "      <td>0.337847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>0.791814</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.383607</td>\n",
       "      <td>0.362408</td>\n",
       "      <td>0.349226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>0.747680</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.400150</td>\n",
       "      <td>0.397781</td>\n",
       "      <td>0.382372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.456709</td>\n",
       "      <td>0.425760</td>\n",
       "      <td>0.418508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.714142</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.492068</td>\n",
       "      <td>0.429735</td>\n",
       "      <td>0.433070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.698895</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.500418</td>\n",
       "      <td>0.476017</td>\n",
       "      <td>0.469766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.668708</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.541098</td>\n",
       "      <td>0.479326</td>\n",
       "      <td>0.492585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.671639</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.526086</td>\n",
       "      <td>0.509179</td>\n",
       "      <td>0.508748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.646074</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.538317</td>\n",
       "      <td>0.525212</td>\n",
       "      <td>0.523331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.651018</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.558631</td>\n",
       "      <td>0.536952</td>\n",
       "      <td>0.536495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.641723</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.602837</td>\n",
       "      <td>0.556126</td>\n",
       "      <td>0.566958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.641542</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.609422</td>\n",
       "      <td>0.555310</td>\n",
       "      <td>0.566254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.640412</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.611446</td>\n",
       "      <td>0.565113</td>\n",
       "      <td>0.573397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.644718</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.572728</td>\n",
       "      <td>0.579929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.638487</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.640338</td>\n",
       "      <td>0.585191</td>\n",
       "      <td>0.597502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.638386</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.675141</td>\n",
       "      <td>0.601863</td>\n",
       "      <td>0.621910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.634477</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.665570</td>\n",
       "      <td>0.603294</td>\n",
       "      <td>0.617742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.636769</td>\n",
       "      <td>0.805683</td>\n",
       "      <td>0.671842</td>\n",
       "      <td>0.603771</td>\n",
       "      <td>0.620632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.631922</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.662712</td>\n",
       "      <td>0.596662</td>\n",
       "      <td>0.612520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.633870</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.667912</td>\n",
       "      <td>0.602695</td>\n",
       "      <td>0.616471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.632469</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.668995</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.621634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.633845</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.667853</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.616921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.636510</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.665329</td>\n",
       "      <td>0.595416</td>\n",
       "      <td>0.611922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.634787</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.666082</td>\n",
       "      <td>0.601232</td>\n",
       "      <td>0.616169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.634689</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.666313</td>\n",
       "      <td>0.604179</td>\n",
       "      <td>0.617643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:52:47,498] Trial 120 finished with value: 0.6176431183358355 and parameters: {'learning_rate': 0.0004264639421009551, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.4, 'temperature': 2.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 121 with params: {'learning_rate': 0.0002747483312779424, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 01:00 < 02:01, 5.78 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.244900</td>\n",
       "      <td>2.019988</td>\n",
       "      <td>0.348304</td>\n",
       "      <td>0.068189</td>\n",
       "      <td>0.068801</td>\n",
       "      <td>0.057566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.873200</td>\n",
       "      <td>1.677106</td>\n",
       "      <td>0.469294</td>\n",
       "      <td>0.144501</td>\n",
       "      <td>0.130614</td>\n",
       "      <td>0.105160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.560800</td>\n",
       "      <td>1.402766</td>\n",
       "      <td>0.548121</td>\n",
       "      <td>0.170187</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.152759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.297600</td>\n",
       "      <td>1.204252</td>\n",
       "      <td>0.647113</td>\n",
       "      <td>0.266273</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.229639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.101800</td>\n",
       "      <td>1.059887</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.267377</td>\n",
       "      <td>0.287483</td>\n",
       "      <td>0.263578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.952653</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.290765</td>\n",
       "      <td>0.298846</td>\n",
       "      <td>0.276301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.886732</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.298864</td>\n",
       "      <td>0.306443</td>\n",
       "      <td>0.285160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.718200</td>\n",
       "      <td>0.842596</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.319425</td>\n",
       "      <td>0.334908</td>\n",
       "      <td>0.312089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.811231</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.364568</td>\n",
       "      <td>0.355187</td>\n",
       "      <td>0.341496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.786559</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.360683</td>\n",
       "      <td>0.367540</td>\n",
       "      <td>0.349411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:53:48,737] Trial 121 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 122 with params: {'learning_rate': 0.00024573288218512017, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.5, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:50, 6.34 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.264300</td>\n",
       "      <td>2.062002</td>\n",
       "      <td>0.244730</td>\n",
       "      <td>0.054461</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>0.029749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.935200</td>\n",
       "      <td>1.759344</td>\n",
       "      <td>0.450962</td>\n",
       "      <td>0.101261</td>\n",
       "      <td>0.119895</td>\n",
       "      <td>0.094610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.653600</td>\n",
       "      <td>1.498522</td>\n",
       "      <td>0.527039</td>\n",
       "      <td>0.177338</td>\n",
       "      <td>0.161636</td>\n",
       "      <td>0.141716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.397800</td>\n",
       "      <td>1.294189</td>\n",
       "      <td>0.593034</td>\n",
       "      <td>0.245064</td>\n",
       "      <td>0.209511</td>\n",
       "      <td>0.187922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.204200</td>\n",
       "      <td>1.142360</td>\n",
       "      <td>0.656279</td>\n",
       "      <td>0.246988</td>\n",
       "      <td>0.252816</td>\n",
       "      <td>0.232322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.032600</td>\n",
       "      <td>1.025274</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.260159</td>\n",
       "      <td>0.283033</td>\n",
       "      <td>0.259810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.945041</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.295388</td>\n",
       "      <td>0.273217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.803700</td>\n",
       "      <td>0.889376</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.344947</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.312222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.851624</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.310207</td>\n",
       "      <td>0.327129</td>\n",
       "      <td>0.304742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.827350</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.333122</td>\n",
       "      <td>0.358072</td>\n",
       "      <td>0.334044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:54:44,876] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 123 with params: {'learning_rate': 0.000225476497722167, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 2, 'lambda_param': 0.2, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:51 < 01:43, 6.75 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.282600</td>\n",
       "      <td>2.090776</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.983900</td>\n",
       "      <td>1.830105</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>0.063612</td>\n",
       "      <td>0.099348</td>\n",
       "      <td>0.073218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.584567</td>\n",
       "      <td>0.492209</td>\n",
       "      <td>0.148903</td>\n",
       "      <td>0.144949</td>\n",
       "      <td>0.119053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>1.381029</td>\n",
       "      <td>0.569203</td>\n",
       "      <td>0.203110</td>\n",
       "      <td>0.192833</td>\n",
       "      <td>0.168034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.298500</td>\n",
       "      <td>1.222084</td>\n",
       "      <td>0.623281</td>\n",
       "      <td>0.234273</td>\n",
       "      <td>0.229326</td>\n",
       "      <td>0.208756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.125100</td>\n",
       "      <td>1.101674</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.258951</td>\n",
       "      <td>0.271230</td>\n",
       "      <td>0.248973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>1.015421</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.294304</td>\n",
       "      <td>0.291469</td>\n",
       "      <td>0.270627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.944778</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.288481</td>\n",
       "      <td>0.306404</td>\n",
       "      <td>0.282662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.904753</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.301858</td>\n",
       "      <td>0.320862</td>\n",
       "      <td>0.293622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.865963</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.346322</td>\n",
       "      <td>0.335664</td>\n",
       "      <td>0.318346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:55:37,535] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 124 with params: {'learning_rate': 0.00027081581353237687, 'weight_decay': 0.005, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.5, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:27 < 02:20, 6.24 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.252100</td>\n",
       "      <td>2.043668</td>\n",
       "      <td>0.271311</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.045682</td>\n",
       "      <td>0.035193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.909000</td>\n",
       "      <td>1.728611</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.101472</td>\n",
       "      <td>0.122509</td>\n",
       "      <td>0.096488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.618100</td>\n",
       "      <td>1.465507</td>\n",
       "      <td>0.535289</td>\n",
       "      <td>0.143932</td>\n",
       "      <td>0.162637</td>\n",
       "      <td>0.139521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.355000</td>\n",
       "      <td>1.256922</td>\n",
       "      <td>0.604033</td>\n",
       "      <td>0.241191</td>\n",
       "      <td>0.217417</td>\n",
       "      <td>0.193762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>1.104540</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.243912</td>\n",
       "      <td>0.257922</td>\n",
       "      <td>0.234257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:56:06,476] Trial 124 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 125 with params: {'learning_rate': 0.0001133580450782148, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 2, 'lambda_param': 0.7000000000000001, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:53 < 00:56, 6.14 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.357600</td>\n",
       "      <td>2.237941</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.181300</td>\n",
       "      <td>2.097282</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.063800</td>\n",
       "      <td>1.983472</td>\n",
       "      <td>0.352887</td>\n",
       "      <td>0.063397</td>\n",
       "      <td>0.070910</td>\n",
       "      <td>0.054380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.949500</td>\n",
       "      <td>1.874308</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.070070</td>\n",
       "      <td>0.089115</td>\n",
       "      <td>0.064062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.853100</td>\n",
       "      <td>1.769125</td>\n",
       "      <td>0.445463</td>\n",
       "      <td>0.077370</td>\n",
       "      <td>0.111635</td>\n",
       "      <td>0.084002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.739600</td>\n",
       "      <td>1.671148</td>\n",
       "      <td>0.460128</td>\n",
       "      <td>0.122066</td>\n",
       "      <td>0.124776</td>\n",
       "      <td>0.099617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.644400</td>\n",
       "      <td>1.590039</td>\n",
       "      <td>0.489459</td>\n",
       "      <td>0.151337</td>\n",
       "      <td>0.147124</td>\n",
       "      <td>0.122794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.565200</td>\n",
       "      <td>1.511156</td>\n",
       "      <td>0.494959</td>\n",
       "      <td>0.167672</td>\n",
       "      <td>0.149985</td>\n",
       "      <td>0.128503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.479200</td>\n",
       "      <td>1.440922</td>\n",
       "      <td>0.546288</td>\n",
       "      <td>0.202617</td>\n",
       "      <td>0.186870</td>\n",
       "      <td>0.169434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.403800</td>\n",
       "      <td>1.374349</td>\n",
       "      <td>0.560037</td>\n",
       "      <td>0.219170</td>\n",
       "      <td>0.192556</td>\n",
       "      <td>0.175150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.344900</td>\n",
       "      <td>1.320683</td>\n",
       "      <td>0.583868</td>\n",
       "      <td>0.226772</td>\n",
       "      <td>0.209650</td>\n",
       "      <td>0.189251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.270800</td>\n",
       "      <td>1.265558</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.233935</td>\n",
       "      <td>0.214842</td>\n",
       "      <td>0.194679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.222100</td>\n",
       "      <td>1.223648</td>\n",
       "      <td>0.608616</td>\n",
       "      <td>0.209760</td>\n",
       "      <td>0.222991</td>\n",
       "      <td>0.195412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.170200</td>\n",
       "      <td>1.180497</td>\n",
       "      <td>0.637947</td>\n",
       "      <td>0.215830</td>\n",
       "      <td>0.239427</td>\n",
       "      <td>0.215515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.118300</td>\n",
       "      <td>1.142745</td>\n",
       "      <td>0.656279</td>\n",
       "      <td>0.251703</td>\n",
       "      <td>0.256989</td>\n",
       "      <td>0.237428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.080300</td>\n",
       "      <td>1.111129</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.268284</td>\n",
       "      <td>0.266571</td>\n",
       "      <td>0.247977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.032200</td>\n",
       "      <td>1.083996</td>\n",
       "      <td>0.670944</td>\n",
       "      <td>0.266670</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.248929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>1.057220</td>\n",
       "      <td>0.684693</td>\n",
       "      <td>0.267997</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.258168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.983600</td>\n",
       "      <td>1.031571</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.275503</td>\n",
       "      <td>0.290465</td>\n",
       "      <td>0.266245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.951800</td>\n",
       "      <td>1.015714</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.291434</td>\n",
       "      <td>0.294743</td>\n",
       "      <td>0.270207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:58:01,162] Trial 125 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 126 with params: {'learning_rate': 8.333610690449199e-05, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.7000000000000001, 'temperature': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:09, 6.77 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.379800</td>\n",
       "      <td>2.271132</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.218400</td>\n",
       "      <td>2.128913</td>\n",
       "      <td>0.208066</td>\n",
       "      <td>0.078960</td>\n",
       "      <td>0.028879</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.094200</td>\n",
       "      <td>2.001700</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.055413</td>\n",
       "      <td>0.083232</td>\n",
       "      <td>0.062732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.969000</td>\n",
       "      <td>1.879192</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.068261</td>\n",
       "      <td>0.099237</td>\n",
       "      <td>0.075472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.859200</td>\n",
       "      <td>1.766047</td>\n",
       "      <td>0.460128</td>\n",
       "      <td>0.103561</td>\n",
       "      <td>0.119376</td>\n",
       "      <td>0.093947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 17:58:27,853] Trial 126 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 127 with params: {'learning_rate': 0.0003327161612826989, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 0, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.39 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.204600</td>\n",
       "      <td>1.967865</td>\n",
       "      <td>0.384968</td>\n",
       "      <td>0.060712</td>\n",
       "      <td>0.080665</td>\n",
       "      <td>0.062735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.806900</td>\n",
       "      <td>1.607489</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>0.164852</td>\n",
       "      <td>0.141528</td>\n",
       "      <td>0.122249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.478300</td>\n",
       "      <td>1.328476</td>\n",
       "      <td>0.564620</td>\n",
       "      <td>0.196065</td>\n",
       "      <td>0.184429</td>\n",
       "      <td>0.163399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.206600</td>\n",
       "      <td>1.140086</td>\n",
       "      <td>0.653529</td>\n",
       "      <td>0.266929</td>\n",
       "      <td>0.256480</td>\n",
       "      <td>0.240257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.014500</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.288331</td>\n",
       "      <td>0.267212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.851200</td>\n",
       "      <td>0.898704</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.299444</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>0.291633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.334335</td>\n",
       "      <td>0.327239</td>\n",
       "      <td>0.308152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.804187</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.359762</td>\n",
       "      <td>0.371948</td>\n",
       "      <td>0.349050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.772183</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.386958</td>\n",
       "      <td>0.379301</td>\n",
       "      <td>0.363444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.763451</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.410337</td>\n",
       "      <td>0.399783</td>\n",
       "      <td>0.384162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.736801</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.454498</td>\n",
       "      <td>0.426212</td>\n",
       "      <td>0.421002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.707759</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.454064</td>\n",
       "      <td>0.430743</td>\n",
       "      <td>0.425821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.707929</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.452449</td>\n",
       "      <td>0.451129</td>\n",
       "      <td>0.443611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.343900</td>\n",
       "      <td>0.685687</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.545948</td>\n",
       "      <td>0.483325</td>\n",
       "      <td>0.487288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.675553</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.546653</td>\n",
       "      <td>0.489057</td>\n",
       "      <td>0.491853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.674629</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.542058</td>\n",
       "      <td>0.496075</td>\n",
       "      <td>0.494719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.667206</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.563886</td>\n",
       "      <td>0.509127</td>\n",
       "      <td>0.514621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.658134</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.571910</td>\n",
       "      <td>0.518910</td>\n",
       "      <td>0.525692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.655336</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.574008</td>\n",
       "      <td>0.539292</td>\n",
       "      <td>0.544746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.650201</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.545288</td>\n",
       "      <td>0.549164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:00:18,263] Trial 127 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 128 with params: {'learning_rate': 4.050179936036871e-06, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:25 < 02:11, 6.67 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.469900</td>\n",
       "      <td>2.451859</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.002225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.454000</td>\n",
       "      <td>2.437075</td>\n",
       "      <td>0.025665</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.004437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.442000</td>\n",
       "      <td>2.423541</td>\n",
       "      <td>0.063245</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.028791</td>\n",
       "      <td>0.008402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.430700</td>\n",
       "      <td>2.410530</td>\n",
       "      <td>0.112741</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>0.008729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.415900</td>\n",
       "      <td>2.397321</td>\n",
       "      <td>0.159487</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>0.019585</td>\n",
       "      <td>0.009780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:00:45,401] Trial 128 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 129 with params: {'learning_rate': 0.00012611009075295032, 'weight_decay': 0.0, 'adam_beta1': 0.97, 'warmup_steps': 3, 'lambda_param': 0.8, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 01:49 < 00:54, 6.36 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.350700</td>\n",
       "      <td>2.218286</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.153200</td>\n",
       "      <td>2.053837</td>\n",
       "      <td>0.298808</td>\n",
       "      <td>0.073710</td>\n",
       "      <td>0.055308</td>\n",
       "      <td>0.046489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.008900</td>\n",
       "      <td>1.906645</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.071480</td>\n",
       "      <td>0.089568</td>\n",
       "      <td>0.065167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.857600</td>\n",
       "      <td>1.760359</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.078845</td>\n",
       "      <td>0.116809</td>\n",
       "      <td>0.089669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.726400</td>\n",
       "      <td>1.630583</td>\n",
       "      <td>0.487626</td>\n",
       "      <td>0.138349</td>\n",
       "      <td>0.142074</td>\n",
       "      <td>0.115997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.587200</td>\n",
       "      <td>1.512621</td>\n",
       "      <td>0.520623</td>\n",
       "      <td>0.159603</td>\n",
       "      <td>0.160252</td>\n",
       "      <td>0.139034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.467700</td>\n",
       "      <td>1.414230</td>\n",
       "      <td>0.561870</td>\n",
       "      <td>0.240137</td>\n",
       "      <td>0.192834</td>\n",
       "      <td>0.175670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.373000</td>\n",
       "      <td>1.333377</td>\n",
       "      <td>0.595784</td>\n",
       "      <td>0.245119</td>\n",
       "      <td>0.218082</td>\n",
       "      <td>0.198490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.277100</td>\n",
       "      <td>1.252372</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.240285</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.220834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.188500</td>\n",
       "      <td>1.186586</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.268321</td>\n",
       "      <td>0.266295</td>\n",
       "      <td>0.248085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.123300</td>\n",
       "      <td>1.127675</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.264695</td>\n",
       "      <td>0.266419</td>\n",
       "      <td>0.248206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.048400</td>\n",
       "      <td>1.077215</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.263323</td>\n",
       "      <td>0.278714</td>\n",
       "      <td>0.256351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>1.037643</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.273405</td>\n",
       "      <td>0.291051</td>\n",
       "      <td>0.266689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.944500</td>\n",
       "      <td>1.000183</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.291579</td>\n",
       "      <td>0.291244</td>\n",
       "      <td>0.267132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.971163</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.298699</td>\n",
       "      <td>0.300948</td>\n",
       "      <td>0.278389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.950171</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.300296</td>\n",
       "      <td>0.310444</td>\n",
       "      <td>0.288741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.925851</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.312721</td>\n",
       "      <td>0.317904</td>\n",
       "      <td>0.293445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.904610</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.300933</td>\n",
       "      <td>0.315533</td>\n",
       "      <td>0.291323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.892708</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.317920</td>\n",
       "      <td>0.316366</td>\n",
       "      <td>0.292601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.878942</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.338282</td>\n",
       "      <td>0.323640</td>\n",
       "      <td>0.302932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:02:36,226] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 130 with params: {'learning_rate': 0.00037726309733610843, 'weight_decay': 0.0, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 4, 'lambda_param': 0.5, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:58 < 01:58, 5.91 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.228000</td>\n",
       "      <td>1.976296</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.075102</td>\n",
       "      <td>0.058859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.793800</td>\n",
       "      <td>1.583103</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.144459</td>\n",
       "      <td>0.142663</td>\n",
       "      <td>0.115606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.437400</td>\n",
       "      <td>1.280160</td>\n",
       "      <td>0.582951</td>\n",
       "      <td>0.226116</td>\n",
       "      <td>0.206238</td>\n",
       "      <td>0.187207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.148300</td>\n",
       "      <td>1.088148</td>\n",
       "      <td>0.659028</td>\n",
       "      <td>0.268594</td>\n",
       "      <td>0.267422</td>\n",
       "      <td>0.249586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>0.954381</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.285010</td>\n",
       "      <td>0.296455</td>\n",
       "      <td>0.273324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.781800</td>\n",
       "      <td>0.860272</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.332445</td>\n",
       "      <td>0.328381</td>\n",
       "      <td>0.308675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.818100</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.366335</td>\n",
       "      <td>0.340811</td>\n",
       "      <td>0.330505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>0.782223</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.375023</td>\n",
       "      <td>0.383787</td>\n",
       "      <td>0.364415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.759071</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.440334</td>\n",
       "      <td>0.421815</td>\n",
       "      <td>0.415267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>0.745977</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.452315</td>\n",
       "      <td>0.424144</td>\n",
       "      <td>0.420275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:03:36,545] Trial 130 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 131 with params: {'learning_rate': 4.259068386605202e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.91, 'warmup_steps': 2, 'lambda_param': 1.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:26 < 02:13, 6.55 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.416600</td>\n",
       "      <td>2.339890</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.008897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.311500</td>\n",
       "      <td>2.250890</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.234200</td>\n",
       "      <td>2.170177</td>\n",
       "      <td>0.192484</td>\n",
       "      <td>0.063594</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.013886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.159000</td>\n",
       "      <td>2.095795</td>\n",
       "      <td>0.340972</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>0.067847</td>\n",
       "      <td>0.060249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.095500</td>\n",
       "      <td>2.028180</td>\n",
       "      <td>0.394134</td>\n",
       "      <td>0.077749</td>\n",
       "      <td>0.083813</td>\n",
       "      <td>0.065061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:04:04,451] Trial 131 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132 with params: {'learning_rate': 0.00033352490482726013, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 0, 'lambda_param': 0.30000000000000004, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:49, 6.41 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.199900</td>\n",
       "      <td>1.950496</td>\n",
       "      <td>0.396884</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.084798</td>\n",
       "      <td>0.063752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.782500</td>\n",
       "      <td>1.575207</td>\n",
       "      <td>0.514207</td>\n",
       "      <td>0.159847</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.133142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.443000</td>\n",
       "      <td>1.293167</td>\n",
       "      <td>0.577452</td>\n",
       "      <td>0.219109</td>\n",
       "      <td>0.196789</td>\n",
       "      <td>0.182912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.173600</td>\n",
       "      <td>1.108505</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.256914</td>\n",
       "      <td>0.273413</td>\n",
       "      <td>0.253807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.979032</td>\n",
       "      <td>0.697525</td>\n",
       "      <td>0.288390</td>\n",
       "      <td>0.297479</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.877371</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.298196</td>\n",
       "      <td>0.321211</td>\n",
       "      <td>0.299057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.827421</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.337134</td>\n",
       "      <td>0.330505</td>\n",
       "      <td>0.313754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.791912</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.387726</td>\n",
       "      <td>0.383188</td>\n",
       "      <td>0.364550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.548200</td>\n",
       "      <td>0.760347</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.404568</td>\n",
       "      <td>0.400785</td>\n",
       "      <td>0.386334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.748168</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.441488</td>\n",
       "      <td>0.411905</td>\n",
       "      <td>0.403208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:04:59,868] Trial 132 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 133 with params: {'learning_rate': 0.000370529114492857, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:53 < 01:47, 6.49 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.215000</td>\n",
       "      <td>1.965557</td>\n",
       "      <td>0.367553</td>\n",
       "      <td>0.063682</td>\n",
       "      <td>0.075079</td>\n",
       "      <td>0.059285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.787100</td>\n",
       "      <td>1.584666</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.161582</td>\n",
       "      <td>0.145084</td>\n",
       "      <td>0.121573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>1.289012</td>\n",
       "      <td>0.581118</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.199786</td>\n",
       "      <td>0.179838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.154800</td>\n",
       "      <td>1.094723</td>\n",
       "      <td>0.658112</td>\n",
       "      <td>0.268126</td>\n",
       "      <td>0.266697</td>\n",
       "      <td>0.249111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.950900</td>\n",
       "      <td>0.959768</td>\n",
       "      <td>0.698442</td>\n",
       "      <td>0.285642</td>\n",
       "      <td>0.294509</td>\n",
       "      <td>0.270403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.861272</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.331708</td>\n",
       "      <td>0.328996</td>\n",
       "      <td>0.312198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.822258</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.320437</td>\n",
       "      <td>0.329786</td>\n",
       "      <td>0.310539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.395166</td>\n",
       "      <td>0.387477</td>\n",
       "      <td>0.370507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.756852</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.418320</td>\n",
       "      <td>0.405376</td>\n",
       "      <td>0.395869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.745866</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.466210</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.402676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:05:54,712] Trial 133 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 134 with params: {'learning_rate': 6.975941321850453e-05, 'weight_decay': 0.007, 'adam_beta1': 0.98, 'warmup_steps': 4, 'lambda_param': 0.2, 'temperature': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:54 < 01:48, 6.44 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.396600</td>\n",
       "      <td>2.302834</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.263300</td>\n",
       "      <td>2.194788</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.169000</td>\n",
       "      <td>2.102807</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.063558</td>\n",
       "      <td>0.021477</td>\n",
       "      <td>0.008912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.081600</td>\n",
       "      <td>2.020543</td>\n",
       "      <td>0.365720</td>\n",
       "      <td>0.063763</td>\n",
       "      <td>0.075328</td>\n",
       "      <td>0.060556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.009500</td>\n",
       "      <td>1.937204</td>\n",
       "      <td>0.406966</td>\n",
       "      <td>0.053761</td>\n",
       "      <td>0.087201</td>\n",
       "      <td>0.063888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.921000</td>\n",
       "      <td>1.856056</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.100638</td>\n",
       "      <td>0.073926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.840700</td>\n",
       "      <td>1.779932</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.080692</td>\n",
       "      <td>0.111437</td>\n",
       "      <td>0.084325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.772500</td>\n",
       "      <td>1.709583</td>\n",
       "      <td>0.471127</td>\n",
       "      <td>0.098346</td>\n",
       "      <td>0.126446</td>\n",
       "      <td>0.095335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.698900</td>\n",
       "      <td>1.642884</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.097713</td>\n",
       "      <td>0.129780</td>\n",
       "      <td>0.099316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.633600</td>\n",
       "      <td>1.584071</td>\n",
       "      <td>0.494959</td>\n",
       "      <td>0.144188</td>\n",
       "      <td>0.141519</td>\n",
       "      <td>0.115493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:06:49,954] Trial 134 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 135 with params: {'learning_rate': 0.0004237596334706623, 'weight_decay': 0.0, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.200900</td>\n",
       "      <td>1.921538</td>\n",
       "      <td>0.389551</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>0.084651</td>\n",
       "      <td>0.064125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.723300</td>\n",
       "      <td>1.504215</td>\n",
       "      <td>0.537122</td>\n",
       "      <td>0.159875</td>\n",
       "      <td>0.173674</td>\n",
       "      <td>0.152025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.344500</td>\n",
       "      <td>1.191721</td>\n",
       "      <td>0.620532</td>\n",
       "      <td>0.220916</td>\n",
       "      <td>0.228272</td>\n",
       "      <td>0.211094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.052000</td>\n",
       "      <td>1.011369</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.271706</td>\n",
       "      <td>0.285815</td>\n",
       "      <td>0.261695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.897661</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.344370</td>\n",
       "      <td>0.329492</td>\n",
       "      <td>0.309132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.814218</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.400850</td>\n",
       "      <td>0.364764</td>\n",
       "      <td>0.352362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.789888</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.399394</td>\n",
       "      <td>0.375123</td>\n",
       "      <td>0.363110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.752312</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.421109</td>\n",
       "      <td>0.398378</td>\n",
       "      <td>0.384834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.738057</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.442154</td>\n",
       "      <td>0.430493</td>\n",
       "      <td>0.423578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.717542</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.479596</td>\n",
       "      <td>0.441777</td>\n",
       "      <td>0.441985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.690811</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.481881</td>\n",
       "      <td>0.450451</td>\n",
       "      <td>0.443383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.686656</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.549607</td>\n",
       "      <td>0.489817</td>\n",
       "      <td>0.503688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.672650</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.513963</td>\n",
       "      <td>0.510628</td>\n",
       "      <td>0.507881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.669867</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.572610</td>\n",
       "      <td>0.540468</td>\n",
       "      <td>0.541067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.669330</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.604442</td>\n",
       "      <td>0.544658</td>\n",
       "      <td>0.553583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.658633</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.647349</td>\n",
       "      <td>0.573891</td>\n",
       "      <td>0.592559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.652078</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.625070</td>\n",
       "      <td>0.557674</td>\n",
       "      <td>0.576513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.646098</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.646912</td>\n",
       "      <td>0.572011</td>\n",
       "      <td>0.592761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.649988</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.638502</td>\n",
       "      <td>0.579724</td>\n",
       "      <td>0.592670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>0.649949</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.641774</td>\n",
       "      <td>0.578194</td>\n",
       "      <td>0.595875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.648728</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.646651</td>\n",
       "      <td>0.591118</td>\n",
       "      <td>0.605359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.645830</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.644732</td>\n",
       "      <td>0.604848</td>\n",
       "      <td>0.610877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.641779</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.649209</td>\n",
       "      <td>0.596173</td>\n",
       "      <td>0.610267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.642647</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.640395</td>\n",
       "      <td>0.580074</td>\n",
       "      <td>0.595178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.638728</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.630877</td>\n",
       "      <td>0.581830</td>\n",
       "      <td>0.592820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.636767</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.640945</td>\n",
       "      <td>0.584855</td>\n",
       "      <td>0.598007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.636387</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.584212</td>\n",
       "      <td>0.599827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.637629</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.652408</td>\n",
       "      <td>0.580940</td>\n",
       "      <td>0.600230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.635618</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.650032</td>\n",
       "      <td>0.585544</td>\n",
       "      <td>0.602323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.636091</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.647189</td>\n",
       "      <td>0.585544</td>\n",
       "      <td>0.601079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:09:40,927] Trial 135 finished with value: 0.6010794807545907 and parameters: {'learning_rate': 0.0004237596334706623, 'weight_decay': 0.0, 'adam_beta1': 0.93, 'warmup_steps': 3, 'lambda_param': 0.1, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 136 with params: {'learning_rate': 1.078901511573681e-06, 'weight_decay': 0.006, 'adam_beta1': 0.98, 'warmup_steps': 4, 'lambda_param': 0.9, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:52 < 01:45, 6.62 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.475700</td>\n",
       "      <td>2.464223</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.002464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.471200</td>\n",
       "      <td>2.459575</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.002109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.468400</td>\n",
       "      <td>2.455349</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.002161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.465300</td>\n",
       "      <td>2.451499</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.002385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.458100</td>\n",
       "      <td>2.447879</td>\n",
       "      <td>0.014665</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.454600</td>\n",
       "      <td>2.444537</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.452600</td>\n",
       "      <td>2.441371</td>\n",
       "      <td>0.021998</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.023085</td>\n",
       "      <td>0.003634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.448800</td>\n",
       "      <td>2.438374</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.023603</td>\n",
       "      <td>0.004186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.446800</td>\n",
       "      <td>2.435532</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.025170</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>0.005281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.444000</td>\n",
       "      <td>2.432899</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>0.029962</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.005892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:10:34,621] Trial 136 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 137 with params: {'learning_rate': 0.0004391250190024249, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.180300</td>\n",
       "      <td>1.880683</td>\n",
       "      <td>0.410632</td>\n",
       "      <td>0.066511</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.070090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.678400</td>\n",
       "      <td>1.453476</td>\n",
       "      <td>0.550871</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.185478</td>\n",
       "      <td>0.165678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.291000</td>\n",
       "      <td>1.154616</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.256149</td>\n",
       "      <td>0.246035</td>\n",
       "      <td>0.230022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.012100</td>\n",
       "      <td>0.984695</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.298380</td>\n",
       "      <td>0.296791</td>\n",
       "      <td>0.274991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.877073</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.321543</td>\n",
       "      <td>0.322053</td>\n",
       "      <td>0.300946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.804930</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.394703</td>\n",
       "      <td>0.361631</td>\n",
       "      <td>0.352468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.782912</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.408879</td>\n",
       "      <td>0.388883</td>\n",
       "      <td>0.379385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.747336</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.422434</td>\n",
       "      <td>0.416530</td>\n",
       "      <td>0.406648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.727446</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.480337</td>\n",
       "      <td>0.430588</td>\n",
       "      <td>0.432317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.707455</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.495095</td>\n",
       "      <td>0.449356</td>\n",
       "      <td>0.455646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.694974</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.497974</td>\n",
       "      <td>0.464585</td>\n",
       "      <td>0.464665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.673413</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.553518</td>\n",
       "      <td>0.490934</td>\n",
       "      <td>0.506881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.672066</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.581102</td>\n",
       "      <td>0.516853</td>\n",
       "      <td>0.525654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.653402</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.594067</td>\n",
       "      <td>0.549818</td>\n",
       "      <td>0.558998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.658445</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.633870</td>\n",
       "      <td>0.562153</td>\n",
       "      <td>0.580531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.204700</td>\n",
       "      <td>0.652071</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.610879</td>\n",
       "      <td>0.560317</td>\n",
       "      <td>0.570546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.654266</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.589554</td>\n",
       "      <td>0.556473</td>\n",
       "      <td>0.559425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.180600</td>\n",
       "      <td>0.647735</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.589539</td>\n",
       "      <td>0.560093</td>\n",
       "      <td>0.563842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.653681</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.610736</td>\n",
       "      <td>0.584244</td>\n",
       "      <td>0.583577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.649741</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.622225</td>\n",
       "      <td>0.587013</td>\n",
       "      <td>0.590860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.650994</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.620636</td>\n",
       "      <td>0.583506</td>\n",
       "      <td>0.589049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.648407</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.640127</td>\n",
       "      <td>0.593844</td>\n",
       "      <td>0.600246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.647260</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.620283</td>\n",
       "      <td>0.585165</td>\n",
       "      <td>0.590025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.642985</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.619032</td>\n",
       "      <td>0.577583</td>\n",
       "      <td>0.583956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.643428</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.618326</td>\n",
       "      <td>0.580681</td>\n",
       "      <td>0.585231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.643124</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.644726</td>\n",
       "      <td>0.601145</td>\n",
       "      <td>0.606675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.644341</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.626748</td>\n",
       "      <td>0.583608</td>\n",
       "      <td>0.590114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.644501</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.625647</td>\n",
       "      <td>0.584034</td>\n",
       "      <td>0.589645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.642776</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.643376</td>\n",
       "      <td>0.592255</td>\n",
       "      <td>0.601416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.643502</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.642224</td>\n",
       "      <td>0.595654</td>\n",
       "      <td>0.602668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:13:22,525] Trial 137 finished with value: 0.6026684427898672 and parameters: {'learning_rate': 0.0004391250190024249, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 138 with params: {'learning_rate': 0.0003042767857655148, 'weight_decay': 0.01, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:53, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.233000</td>\n",
       "      <td>2.003091</td>\n",
       "      <td>0.361137</td>\n",
       "      <td>0.066988</td>\n",
       "      <td>0.072521</td>\n",
       "      <td>0.059529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.849900</td>\n",
       "      <td>1.651209</td>\n",
       "      <td>0.468378</td>\n",
       "      <td>0.137093</td>\n",
       "      <td>0.130277</td>\n",
       "      <td>0.105341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.527900</td>\n",
       "      <td>1.373121</td>\n",
       "      <td>0.554537</td>\n",
       "      <td>0.204371</td>\n",
       "      <td>0.177267</td>\n",
       "      <td>0.157570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.258500</td>\n",
       "      <td>1.179804</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.268915</td>\n",
       "      <td>0.244671</td>\n",
       "      <td>0.227597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.064500</td>\n",
       "      <td>1.036598</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.261506</td>\n",
       "      <td>0.287284</td>\n",
       "      <td>0.262062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.895700</td>\n",
       "      <td>0.925361</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.301601</td>\n",
       "      <td>0.278017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.764800</td>\n",
       "      <td>0.867059</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.311167</td>\n",
       "      <td>0.307938</td>\n",
       "      <td>0.287912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.826555</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.316264</td>\n",
       "      <td>0.342719</td>\n",
       "      <td>0.315823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.795565</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.387708</td>\n",
       "      <td>0.374636</td>\n",
       "      <td>0.361959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.783692</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.418307</td>\n",
       "      <td>0.393776</td>\n",
       "      <td>0.381747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.750145</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.408240</td>\n",
       "      <td>0.402531</td>\n",
       "      <td>0.391454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.722824</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.446918</td>\n",
       "      <td>0.413627</td>\n",
       "      <td>0.407842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.714457</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.459597</td>\n",
       "      <td>0.447878</td>\n",
       "      <td>0.439940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.696004</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.466693</td>\n",
       "      <td>0.454528</td>\n",
       "      <td>0.446418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.687468</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.494078</td>\n",
       "      <td>0.459528</td>\n",
       "      <td>0.457180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.678523</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.463577</td>\n",
       "      <td>0.465266</td>\n",
       "      <td>0.454035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.672022</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.495280</td>\n",
       "      <td>0.469597</td>\n",
       "      <td>0.470122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.668103</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.550145</td>\n",
       "      <td>0.499941</td>\n",
       "      <td>0.505512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.663648</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.553345</td>\n",
       "      <td>0.508839</td>\n",
       "      <td>0.511953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.655773</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.541272</td>\n",
       "      <td>0.518413</td>\n",
       "      <td>0.517629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.656665</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.560018</td>\n",
       "      <td>0.525461</td>\n",
       "      <td>0.531910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.653858</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.574063</td>\n",
       "      <td>0.545977</td>\n",
       "      <td>0.550103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.651419</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.579632</td>\n",
       "      <td>0.538650</td>\n",
       "      <td>0.546111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.649970</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.577199</td>\n",
       "      <td>0.554338</td>\n",
       "      <td>0.556089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.650463</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.564580</td>\n",
       "      <td>0.543660</td>\n",
       "      <td>0.544291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.646941</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.601744</td>\n",
       "      <td>0.565564</td>\n",
       "      <td>0.570401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.646458</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.574648</td>\n",
       "      <td>0.534697</td>\n",
       "      <td>0.544011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.647664</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.573776</td>\n",
       "      <td>0.538911</td>\n",
       "      <td>0.545212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.648349</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.572993</td>\n",
       "      <td>0.549873</td>\n",
       "      <td>0.552104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.648371</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.573311</td>\n",
       "      <td>0.550328</td>\n",
       "      <td>0.552482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:16:18,110] Trial 138 finished with value: 0.5524819558256364 and parameters: {'learning_rate': 0.0003042767857655148, 'weight_decay': 0.01, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 139 with params: {'learning_rate': 0.000497979662262894, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:42, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.156000</td>\n",
       "      <td>1.837092</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>0.060101</td>\n",
       "      <td>0.104407</td>\n",
       "      <td>0.073346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.631200</td>\n",
       "      <td>1.409424</td>\n",
       "      <td>0.569203</td>\n",
       "      <td>0.177828</td>\n",
       "      <td>0.203896</td>\n",
       "      <td>0.180017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.240200</td>\n",
       "      <td>1.119877</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.234037</td>\n",
       "      <td>0.258883</td>\n",
       "      <td>0.239226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.963700</td>\n",
       "      <td>0.954404</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.275717</td>\n",
       "      <td>0.294001</td>\n",
       "      <td>0.268986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.865735</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.308849</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>0.300727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.795453</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.407948</td>\n",
       "      <td>0.376276</td>\n",
       "      <td>0.365495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.526500</td>\n",
       "      <td>0.763241</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.417145</td>\n",
       "      <td>0.393650</td>\n",
       "      <td>0.384696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.738738</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.440772</td>\n",
       "      <td>0.418765</td>\n",
       "      <td>0.411697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.717988</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.470838</td>\n",
       "      <td>0.445579</td>\n",
       "      <td>0.445480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.702184</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.517615</td>\n",
       "      <td>0.470170</td>\n",
       "      <td>0.475436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.547531</td>\n",
       "      <td>0.505710</td>\n",
       "      <td>0.511513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.662213</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.587478</td>\n",
       "      <td>0.521413</td>\n",
       "      <td>0.537073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.657829</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.585528</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>0.552823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.649496</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.626635</td>\n",
       "      <td>0.582737</td>\n",
       "      <td>0.591450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.650906</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.622957</td>\n",
       "      <td>0.575654</td>\n",
       "      <td>0.588162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.648984</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.651484</td>\n",
       "      <td>0.585109</td>\n",
       "      <td>0.601565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.647022</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.630965</td>\n",
       "      <td>0.582968</td>\n",
       "      <td>0.593586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.644022</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.627030</td>\n",
       "      <td>0.583298</td>\n",
       "      <td>0.591602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.646134</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.642447</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>0.602540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.642048</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.623516</td>\n",
       "      <td>0.590244</td>\n",
       "      <td>0.594578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.646450</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.647664</td>\n",
       "      <td>0.606475</td>\n",
       "      <td>0.611635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.643840</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.644034</td>\n",
       "      <td>0.609290</td>\n",
       "      <td>0.608796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.643851</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.645356</td>\n",
       "      <td>0.601301</td>\n",
       "      <td>0.608105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.639119</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.647582</td>\n",
       "      <td>0.610078</td>\n",
       "      <td>0.612504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.639156</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.651020</td>\n",
       "      <td>0.610823</td>\n",
       "      <td>0.613777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.638346</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.650915</td>\n",
       "      <td>0.611750</td>\n",
       "      <td>0.613910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.637658</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.648930</td>\n",
       "      <td>0.609868</td>\n",
       "      <td>0.613362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.638881</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.650182</td>\n",
       "      <td>0.609100</td>\n",
       "      <td>0.613156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.637198</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.649683</td>\n",
       "      <td>0.608737</td>\n",
       "      <td>0.612724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.637057</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.650757</td>\n",
       "      <td>0.610828</td>\n",
       "      <td>0.614237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:19:12,638] Trial 139 finished with value: 0.6142365945161642 and parameters: {'learning_rate': 0.000497979662262894, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 3.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 140 with params: {'learning_rate': 0.0003297414087448796, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:53, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.221200</td>\n",
       "      <td>1.980249</td>\n",
       "      <td>0.380385</td>\n",
       "      <td>0.064266</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.063484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.815900</td>\n",
       "      <td>1.612411</td>\n",
       "      <td>0.484876</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>0.139979</td>\n",
       "      <td>0.117842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.479500</td>\n",
       "      <td>1.324996</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.190731</td>\n",
       "      <td>0.183327</td>\n",
       "      <td>0.163393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.204900</td>\n",
       "      <td>1.136547</td>\n",
       "      <td>0.652612</td>\n",
       "      <td>0.264711</td>\n",
       "      <td>0.258267</td>\n",
       "      <td>0.241762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.010600</td>\n",
       "      <td>1.000685</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.259678</td>\n",
       "      <td>0.289353</td>\n",
       "      <td>0.262093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.843500</td>\n",
       "      <td>0.891581</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.311693</td>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.293984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.843459</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.334320</td>\n",
       "      <td>0.318258</td>\n",
       "      <td>0.300719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.805327</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.385083</td>\n",
       "      <td>0.376063</td>\n",
       "      <td>0.357339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.776309</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.398317</td>\n",
       "      <td>0.390738</td>\n",
       "      <td>0.379417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.761678</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.452294</td>\n",
       "      <td>0.408420</td>\n",
       "      <td>0.400904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.729193</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.456911</td>\n",
       "      <td>0.420696</td>\n",
       "      <td>0.414556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.707264</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.458457</td>\n",
       "      <td>0.428543</td>\n",
       "      <td>0.424430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.700815</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.456840</td>\n",
       "      <td>0.457969</td>\n",
       "      <td>0.449560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.687643</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.511674</td>\n",
       "      <td>0.469133</td>\n",
       "      <td>0.468855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.674348</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.538950</td>\n",
       "      <td>0.477704</td>\n",
       "      <td>0.482297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.536703</td>\n",
       "      <td>0.488491</td>\n",
       "      <td>0.489999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.667146</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.588122</td>\n",
       "      <td>0.512059</td>\n",
       "      <td>0.522950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.660274</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.562042</td>\n",
       "      <td>0.524184</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.658593</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.568667</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>0.535450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.651941</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.571666</td>\n",
       "      <td>0.550092</td>\n",
       "      <td>0.550073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.650951</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.552175</td>\n",
       "      <td>0.533822</td>\n",
       "      <td>0.536550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.648591</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.590298</td>\n",
       "      <td>0.554475</td>\n",
       "      <td>0.560679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.648333</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.596377</td>\n",
       "      <td>0.557954</td>\n",
       "      <td>0.565558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.647413</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.596136</td>\n",
       "      <td>0.563121</td>\n",
       "      <td>0.567359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.197300</td>\n",
       "      <td>0.645452</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.599688</td>\n",
       "      <td>0.570053</td>\n",
       "      <td>0.573176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.643666</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.622431</td>\n",
       "      <td>0.575078</td>\n",
       "      <td>0.583482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.644396</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.668512</td>\n",
       "      <td>0.588340</td>\n",
       "      <td>0.608754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.644284</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.642721</td>\n",
       "      <td>0.587746</td>\n",
       "      <td>0.599584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.643900</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.592956</td>\n",
       "      <td>0.602052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.643940</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.642408</td>\n",
       "      <td>0.592956</td>\n",
       "      <td>0.602624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:22:07,705] Trial 140 finished with value: 0.6026238894106295 and parameters: {'learning_rate': 0.0003297414087448796, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 1, 'lambda_param': 0.30000000000000004, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 141 with params: {'learning_rate': 2.6313721811247065e-05, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 4, 'lambda_param': 0.4, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:29 < 02:27, 5.93 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.439400</td>\n",
       "      <td>2.384453</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.368300</td>\n",
       "      <td>2.328525</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.008494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.321000</td>\n",
       "      <td>2.282811</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.278900</td>\n",
       "      <td>2.241368</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.241900</td>\n",
       "      <td>2.204625</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:22:38,081] Trial 141 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 142 with params: {'learning_rate': 0.00046070802354624074, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 02:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.138100</td>\n",
       "      <td>1.820137</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.061634</td>\n",
       "      <td>0.105097</td>\n",
       "      <td>0.074544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.616000</td>\n",
       "      <td>1.393587</td>\n",
       "      <td>0.567369</td>\n",
       "      <td>0.200122</td>\n",
       "      <td>0.200301</td>\n",
       "      <td>0.177415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.232100</td>\n",
       "      <td>1.103963</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.244052</td>\n",
       "      <td>0.269848</td>\n",
       "      <td>0.247269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.963700</td>\n",
       "      <td>0.947600</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.260354</td>\n",
       "      <td>0.302915</td>\n",
       "      <td>0.272938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.858050</td>\n",
       "      <td>0.720440</td>\n",
       "      <td>0.343115</td>\n",
       "      <td>0.331906</td>\n",
       "      <td>0.308228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.389228</td>\n",
       "      <td>0.380069</td>\n",
       "      <td>0.364267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.752147</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.402536</td>\n",
       "      <td>0.392751</td>\n",
       "      <td>0.379393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.722323</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.426852</td>\n",
       "      <td>0.422245</td>\n",
       "      <td>0.411699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.706043</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.489447</td>\n",
       "      <td>0.454325</td>\n",
       "      <td>0.451815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.537155</td>\n",
       "      <td>0.465941</td>\n",
       "      <td>0.474266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.690184</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.522263</td>\n",
       "      <td>0.479745</td>\n",
       "      <td>0.480541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.675747</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.533576</td>\n",
       "      <td>0.490474</td>\n",
       "      <td>0.496555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.672140</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.544089</td>\n",
       "      <td>0.524171</td>\n",
       "      <td>0.523946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.594638</td>\n",
       "      <td>0.545084</td>\n",
       "      <td>0.553379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.657136</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.612690</td>\n",
       "      <td>0.555544</td>\n",
       "      <td>0.570273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.654514</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.626533</td>\n",
       "      <td>0.571120</td>\n",
       "      <td>0.585030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.652949</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.623663</td>\n",
       "      <td>0.568790</td>\n",
       "      <td>0.581376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.649927</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.675234</td>\n",
       "      <td>0.594109</td>\n",
       "      <td>0.611590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.650427</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.683822</td>\n",
       "      <td>0.600133</td>\n",
       "      <td>0.619249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.649850</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.680464</td>\n",
       "      <td>0.602597</td>\n",
       "      <td>0.618965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.653581</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.675404</td>\n",
       "      <td>0.599854</td>\n",
       "      <td>0.615036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.650902</td>\n",
       "      <td>0.803850</td>\n",
       "      <td>0.670240</td>\n",
       "      <td>0.609259</td>\n",
       "      <td>0.621485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.607471</td>\n",
       "      <td>0.617921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.644496</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.676902</td>\n",
       "      <td>0.588923</td>\n",
       "      <td>0.607290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.641794</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.660549</td>\n",
       "      <td>0.598649</td>\n",
       "      <td>0.609157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.640922</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.667980</td>\n",
       "      <td>0.605461</td>\n",
       "      <td>0.618252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.640602</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.676637</td>\n",
       "      <td>0.595096</td>\n",
       "      <td>0.614249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.643165</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.658950</td>\n",
       "      <td>0.592247</td>\n",
       "      <td>0.605382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.642839</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.656434</td>\n",
       "      <td>0.599297</td>\n",
       "      <td>0.609514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.642885</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.656434</td>\n",
       "      <td>0.599297</td>\n",
       "      <td>0.609514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:25:31,980] Trial 142 finished with value: 0.6095136457433374 and parameters: {'learning_rate': 0.00046070802354624074, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 0, 'lambda_param': 0.4, 'temperature': 3.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 143 with params: {'learning_rate': 5.3985154766476e-05, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:31 < 02:38, 5.53 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.406300</td>\n",
       "      <td>2.318637</td>\n",
       "      <td>0.179652</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.283000</td>\n",
       "      <td>2.211671</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.007089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.190400</td>\n",
       "      <td>2.115975</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>0.072735</td>\n",
       "      <td>0.055364</td>\n",
       "      <td>0.050009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.099200</td>\n",
       "      <td>2.027546</td>\n",
       "      <td>0.391384</td>\n",
       "      <td>0.076715</td>\n",
       "      <td>0.082996</td>\n",
       "      <td>0.064240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.022200</td>\n",
       "      <td>1.945466</td>\n",
       "      <td>0.412466</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>0.090295</td>\n",
       "      <td>0.068101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:26:04,356] Trial 143 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144 with params: {'learning_rate': 0.00028379285700209597, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 1, 'lambda_param': 0.1, 'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 02:07 < 01:04, 5.46 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.245100</td>\n",
       "      <td>2.030810</td>\n",
       "      <td>0.308891</td>\n",
       "      <td>0.071173</td>\n",
       "      <td>0.056257</td>\n",
       "      <td>0.043665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.890000</td>\n",
       "      <td>1.704148</td>\n",
       "      <td>0.458295</td>\n",
       "      <td>0.124632</td>\n",
       "      <td>0.125449</td>\n",
       "      <td>0.100119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.590900</td>\n",
       "      <td>1.437336</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.171203</td>\n",
       "      <td>0.171692</td>\n",
       "      <td>0.149521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.324700</td>\n",
       "      <td>1.231050</td>\n",
       "      <td>0.610449</td>\n",
       "      <td>0.235738</td>\n",
       "      <td>0.225930</td>\n",
       "      <td>0.204871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>1.080837</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.247834</td>\n",
       "      <td>0.265394</td>\n",
       "      <td>0.242161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.953300</td>\n",
       "      <td>0.967821</td>\n",
       "      <td>0.693859</td>\n",
       "      <td>0.275306</td>\n",
       "      <td>0.293372</td>\n",
       "      <td>0.269016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.897263</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.314022</td>\n",
       "      <td>0.302426</td>\n",
       "      <td>0.280731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.728100</td>\n",
       "      <td>0.853344</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.310115</td>\n",
       "      <td>0.333786</td>\n",
       "      <td>0.307229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.818769</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.343778</td>\n",
       "      <td>0.342549</td>\n",
       "      <td>0.323061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.801679</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.397927</td>\n",
       "      <td>0.378638</td>\n",
       "      <td>0.363131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.766775</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.431553</td>\n",
       "      <td>0.397620</td>\n",
       "      <td>0.385694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.742996</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.428983</td>\n",
       "      <td>0.415972</td>\n",
       "      <td>0.408851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.726963</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.419864</td>\n",
       "      <td>0.410851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.711574</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.475626</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>0.434090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.705893</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.460420</td>\n",
       "      <td>0.444703</td>\n",
       "      <td>0.436646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.695556</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.443490</td>\n",
       "      <td>0.452178</td>\n",
       "      <td>0.438541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.688028</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.466337</td>\n",
       "      <td>0.463840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.682974</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.483703</td>\n",
       "      <td>0.484422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.543131</td>\n",
       "      <td>0.489193</td>\n",
       "      <td>0.488994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.669011</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.532654</td>\n",
       "      <td>0.505608</td>\n",
       "      <td>0.503753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:28:13,406] Trial 144 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 145 with params: {'learning_rate': 0.00017861054002006786, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 0, 'lambda_param': 0.9, 'temperature': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 175/1050 00:29 < 02:28, 5.90 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.289300</td>\n",
       "      <td>2.113309</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.038257</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.010958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.012300</td>\n",
       "      <td>1.863186</td>\n",
       "      <td>0.421632</td>\n",
       "      <td>0.065907</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.070922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>1.634140</td>\n",
       "      <td>0.486709</td>\n",
       "      <td>0.142302</td>\n",
       "      <td>0.137326</td>\n",
       "      <td>0.117190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>1.447328</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.179428</td>\n",
       "      <td>0.176546</td>\n",
       "      <td>0.156152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.389500</td>\n",
       "      <td>1.300874</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.241907</td>\n",
       "      <td>0.215118</td>\n",
       "      <td>0.198509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:28:44,051] Trial 145 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 146 with params: {'learning_rate': 0.00045386877226676113, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:08, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.180300</td>\n",
       "      <td>1.886778</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.070390</td>\n",
       "      <td>0.089804</td>\n",
       "      <td>0.068665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.688700</td>\n",
       "      <td>1.470497</td>\n",
       "      <td>0.543538</td>\n",
       "      <td>0.179657</td>\n",
       "      <td>0.188226</td>\n",
       "      <td>0.166673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.313200</td>\n",
       "      <td>1.183164</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.231124</td>\n",
       "      <td>0.235623</td>\n",
       "      <td>0.218371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.034600</td>\n",
       "      <td>1.006347</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.249246</td>\n",
       "      <td>0.282518</td>\n",
       "      <td>0.256593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.839100</td>\n",
       "      <td>0.900215</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.280472</td>\n",
       "      <td>0.312577</td>\n",
       "      <td>0.284776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.822978</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.370380</td>\n",
       "      <td>0.353257</td>\n",
       "      <td>0.338652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.574300</td>\n",
       "      <td>0.793037</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.412499</td>\n",
       "      <td>0.385268</td>\n",
       "      <td>0.376695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.755390</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.427212</td>\n",
       "      <td>0.419013</td>\n",
       "      <td>0.406404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.474996</td>\n",
       "      <td>0.441738</td>\n",
       "      <td>0.440472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.717092</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.496810</td>\n",
       "      <td>0.455639</td>\n",
       "      <td>0.457173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.706651</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.506957</td>\n",
       "      <td>0.472775</td>\n",
       "      <td>0.473002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.673595</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>0.475437</td>\n",
       "      <td>0.489504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.676132</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.545811</td>\n",
       "      <td>0.516744</td>\n",
       "      <td>0.518610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.653825</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.565793</td>\n",
       "      <td>0.533964</td>\n",
       "      <td>0.537582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.660534</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.599191</td>\n",
       "      <td>0.555192</td>\n",
       "      <td>0.562868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.657878</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.623040</td>\n",
       "      <td>0.566258</td>\n",
       "      <td>0.576431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.649794</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.611451</td>\n",
       "      <td>0.563992</td>\n",
       "      <td>0.573707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.648798</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>0.571929</td>\n",
       "      <td>0.578855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.654409</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.652116</td>\n",
       "      <td>0.591947</td>\n",
       "      <td>0.602618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.650366</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.638188</td>\n",
       "      <td>0.594370</td>\n",
       "      <td>0.602862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.650496</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.658927</td>\n",
       "      <td>0.603905</td>\n",
       "      <td>0.615579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.650804</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.661474</td>\n",
       "      <td>0.616296</td>\n",
       "      <td>0.624080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.651052</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.638045</td>\n",
       "      <td>0.591450</td>\n",
       "      <td>0.600798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.646643</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.613548</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.585812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.649297</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.633339</td>\n",
       "      <td>0.594295</td>\n",
       "      <td>0.596444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.644790</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.661082</td>\n",
       "      <td>0.611769</td>\n",
       "      <td>0.622246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.646787</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.660495</td>\n",
       "      <td>0.608224</td>\n",
       "      <td>0.620219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.650095</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.657660</td>\n",
       "      <td>0.604890</td>\n",
       "      <td>0.614588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.647507</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.657488</td>\n",
       "      <td>0.600402</td>\n",
       "      <td>0.611161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.647542</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.656249</td>\n",
       "      <td>0.600402</td>\n",
       "      <td>0.610147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:31:54,419] Trial 146 finished with value: 0.6101474990181154 and parameters: {'learning_rate': 0.00045386877226676113, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 147 with params: {'learning_rate': 0.00040321605879245205, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.4, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:03, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.205400</td>\n",
       "      <td>1.926899</td>\n",
       "      <td>0.392301</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.084866</td>\n",
       "      <td>0.063950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.731300</td>\n",
       "      <td>1.508573</td>\n",
       "      <td>0.531622</td>\n",
       "      <td>0.184518</td>\n",
       "      <td>0.173421</td>\n",
       "      <td>0.156387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.353600</td>\n",
       "      <td>1.196201</td>\n",
       "      <td>0.637947</td>\n",
       "      <td>0.249097</td>\n",
       "      <td>0.241120</td>\n",
       "      <td>0.223632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.021551</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.273899</td>\n",
       "      <td>0.285375</td>\n",
       "      <td>0.262189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>0.909241</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.320536</td>\n",
       "      <td>0.321662</td>\n",
       "      <td>0.298487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.819861</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.388903</td>\n",
       "      <td>0.351664</td>\n",
       "      <td>0.339415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.792656</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.392658</td>\n",
       "      <td>0.371032</td>\n",
       "      <td>0.357003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.751240</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.407121</td>\n",
       "      <td>0.395119</td>\n",
       "      <td>0.381093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.737653</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.436051</td>\n",
       "      <td>0.428708</td>\n",
       "      <td>0.423023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.717490</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.481506</td>\n",
       "      <td>0.447054</td>\n",
       "      <td>0.447616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.698093</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.489698</td>\n",
       "      <td>0.461394</td>\n",
       "      <td>0.454911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.522003</td>\n",
       "      <td>0.478777</td>\n",
       "      <td>0.484866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.672940</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.534800</td>\n",
       "      <td>0.501064</td>\n",
       "      <td>0.503073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.672663</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.543051</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>0.515708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.665726</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.626886</td>\n",
       "      <td>0.551867</td>\n",
       "      <td>0.567259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.658415</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.617461</td>\n",
       "      <td>0.552597</td>\n",
       "      <td>0.567421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.658551</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.618797</td>\n",
       "      <td>0.551120</td>\n",
       "      <td>0.568201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.664581</td>\n",
       "      <td>0.567691</td>\n",
       "      <td>0.592386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.658135</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.639634</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>0.589919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.654004</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.641462</td>\n",
       "      <td>0.572197</td>\n",
       "      <td>0.588795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.653078</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.668717</td>\n",
       "      <td>0.589021</td>\n",
       "      <td>0.610580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.651892</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.656945</td>\n",
       "      <td>0.593965</td>\n",
       "      <td>0.608504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.648718</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.626062</td>\n",
       "      <td>0.572116</td>\n",
       "      <td>0.584238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.644859</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.636407</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.643550</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.639574</td>\n",
       "      <td>0.580301</td>\n",
       "      <td>0.594241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.642041</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.666394</td>\n",
       "      <td>0.596222</td>\n",
       "      <td>0.613293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.639731</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.648954</td>\n",
       "      <td>0.581029</td>\n",
       "      <td>0.597262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.640905</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.645831</td>\n",
       "      <td>0.580917</td>\n",
       "      <td>0.595807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.638783</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.665652</td>\n",
       "      <td>0.595248</td>\n",
       "      <td>0.611677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.639328</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.665702</td>\n",
       "      <td>0.596085</td>\n",
       "      <td>0.612390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:34:59,680] Trial 147 finished with value: 0.6123898549200514 and parameters: {'learning_rate': 0.00040321605879245205, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 3, 'lambda_param': 0.4, 'temperature': 4.0}. Best is trial 80 with value: 0.6434396489980481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 148 with params: {'learning_rate': 0.00026678595121843395, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 3, 'lambda_param': 0.0, 'temperature': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/1050 00:56 < 01:53, 6.16 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.259800</td>\n",
       "      <td>2.032393</td>\n",
       "      <td>0.340055</td>\n",
       "      <td>0.068879</td>\n",
       "      <td>0.067265</td>\n",
       "      <td>0.055954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.885800</td>\n",
       "      <td>1.687211</td>\n",
       "      <td>0.470211</td>\n",
       "      <td>0.122042</td>\n",
       "      <td>0.130020</td>\n",
       "      <td>0.102129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.571200</td>\n",
       "      <td>1.409110</td>\n",
       "      <td>0.555454</td>\n",
       "      <td>0.199270</td>\n",
       "      <td>0.179783</td>\n",
       "      <td>0.159659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.308000</td>\n",
       "      <td>1.210186</td>\n",
       "      <td>0.641613</td>\n",
       "      <td>0.246044</td>\n",
       "      <td>0.245828</td>\n",
       "      <td>0.228493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>1.066414</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.264231</td>\n",
       "      <td>0.285820</td>\n",
       "      <td>0.262705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.946400</td>\n",
       "      <td>0.960065</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.297639</td>\n",
       "      <td>0.303135</td>\n",
       "      <td>0.284143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.818400</td>\n",
       "      <td>0.891023</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.301677</td>\n",
       "      <td>0.303268</td>\n",
       "      <td>0.282622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.730500</td>\n",
       "      <td>0.849802</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.316367</td>\n",
       "      <td>0.336772</td>\n",
       "      <td>0.311448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>0.814090</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.344431</td>\n",
       "      <td>0.353958</td>\n",
       "      <td>0.337748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.791032</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.368322</td>\n",
       "      <td>0.368417</td>\n",
       "      <td>0.350451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:35:57,531] Trial 148 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 149 with params: {'learning_rate': 8.049073025334763e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 2, 'lambda_param': 0.2, 'temperature': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1050 02:00 < 01:00, 5.81 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.378100</td>\n",
       "      <td>2.268718</td>\n",
       "      <td>0.176902</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.216100</td>\n",
       "      <td>2.123328</td>\n",
       "      <td>0.241980</td>\n",
       "      <td>0.074620</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.033579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.089600</td>\n",
       "      <td>1.994844</td>\n",
       "      <td>0.401467</td>\n",
       "      <td>0.054775</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.063619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.962700</td>\n",
       "      <td>1.873021</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.090485</td>\n",
       "      <td>0.099238</td>\n",
       "      <td>0.077283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.854300</td>\n",
       "      <td>1.760185</td>\n",
       "      <td>0.467461</td>\n",
       "      <td>0.105408</td>\n",
       "      <td>0.123442</td>\n",
       "      <td>0.100630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.740400</td>\n",
       "      <td>1.663596</td>\n",
       "      <td>0.494959</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>0.138103</td>\n",
       "      <td>0.112556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.644700</td>\n",
       "      <td>1.582850</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>0.147991</td>\n",
       "      <td>0.126248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.568500</td>\n",
       "      <td>1.507406</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>0.222359</td>\n",
       "      <td>0.181137</td>\n",
       "      <td>0.164422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.488200</td>\n",
       "      <td>1.438184</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.242104</td>\n",
       "      <td>0.191957</td>\n",
       "      <td>0.176919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.417500</td>\n",
       "      <td>1.382984</td>\n",
       "      <td>0.599450</td>\n",
       "      <td>0.241767</td>\n",
       "      <td>0.218564</td>\n",
       "      <td>0.203927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.363600</td>\n",
       "      <td>1.329911</td>\n",
       "      <td>0.618698</td>\n",
       "      <td>0.244649</td>\n",
       "      <td>0.222778</td>\n",
       "      <td>0.208801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.296800</td>\n",
       "      <td>1.281859</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>0.247914</td>\n",
       "      <td>0.236650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.253900</td>\n",
       "      <td>1.240339</td>\n",
       "      <td>0.652612</td>\n",
       "      <td>0.287132</td>\n",
       "      <td>0.256986</td>\n",
       "      <td>0.247225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.207500</td>\n",
       "      <td>1.204360</td>\n",
       "      <td>0.662695</td>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.265448</td>\n",
       "      <td>0.252971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.163300</td>\n",
       "      <td>1.172849</td>\n",
       "      <td>0.673694</td>\n",
       "      <td>0.284791</td>\n",
       "      <td>0.277923</td>\n",
       "      <td>0.262330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.127500</td>\n",
       "      <td>1.143862</td>\n",
       "      <td>0.678277</td>\n",
       "      <td>0.288744</td>\n",
       "      <td>0.280040</td>\n",
       "      <td>0.265227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.085100</td>\n",
       "      <td>1.119278</td>\n",
       "      <td>0.685610</td>\n",
       "      <td>0.284893</td>\n",
       "      <td>0.286896</td>\n",
       "      <td>0.268324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.062300</td>\n",
       "      <td>1.096778</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.265370</td>\n",
       "      <td>0.282679</td>\n",
       "      <td>0.262034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>1.076899</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.295362</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.276173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.019100</td>\n",
       "      <td>1.059856</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.272918</td>\n",
       "      <td>0.289764</td>\n",
       "      <td>0.267888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:37:58,916] Trial 149 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_trial2 = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name=\"Test-destilace\",\n",
    "    n_trials=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c277ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='80', objective=0.6434396489980481, hyperparameters={'learning_rate': 0.000482322168974171, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 0.0, 'temperature': 2.5}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b28705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f24245a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base_fine_aug_hp-search\", logging_dir=f\"~/logs/{DATASET}/bert-base_fine_aug_hp-search\", epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e8abf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nápočet epoch na steps\n",
    "data_length = len(all_train_data)\n",
    "min_r = math.ceil(data_length/batch_size)*5\n",
    "max_r = math.ceil(data_length/batch_size)*num_epochs\n",
    "warm_up = math.ceil(data_length/batch_size/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ebd4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    params =  {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 1e-2, step=1e-3),\n",
    "        \"adam_beta1\" : trial.suggest_float(\"adam_beta1\", 0.9, 0.99, step=0.01),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, warm_up),\n",
    "    }\n",
    "    print(f\"Trial {trial.number} with params: {params}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9dc12e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.HyperbandPruner(min_resource=min_r, max_resource=max_r, reduction_factor=2, bootstrap_count=2)\n",
    "sampler = optuna.samplers.TPESampler(seed=42, multivariate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4414ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    model_init = lambda: get_Bert(),\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcbdaec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:38:00,160] A new study created in memory with name: Test-base-aug\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 with params: {'learning_rate': 1.0253509690168497e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:02 < 10:13, 21.39 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.586900</td>\n",
       "      <td>3.324262</td>\n",
       "      <td>0.327223</td>\n",
       "      <td>0.043095</td>\n",
       "      <td>0.065415</td>\n",
       "      <td>0.047883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.091700</td>\n",
       "      <td>2.904099</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.109918</td>\n",
       "      <td>0.110082</td>\n",
       "      <td>0.094993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.703900</td>\n",
       "      <td>2.574500</td>\n",
       "      <td>0.497709</td>\n",
       "      <td>0.129979</td>\n",
       "      <td>0.142067</td>\n",
       "      <td>0.118518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.389700</td>\n",
       "      <td>2.308230</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.215766</td>\n",
       "      <td>0.182629</td>\n",
       "      <td>0.165596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.125800</td>\n",
       "      <td>2.098330</td>\n",
       "      <td>0.593951</td>\n",
       "      <td>0.261190</td>\n",
       "      <td>0.223329</td>\n",
       "      <td>0.206112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:40:04,534] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 with params: {'learning_rate': 2.636875533972305e-06, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:05 < 10:25, 20.98 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>3.688891</td>\n",
       "      <td>0.212649</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.031375</td>\n",
       "      <td>0.015624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.614700</td>\n",
       "      <td>3.542825</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.018667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.478200</td>\n",
       "      <td>3.411683</td>\n",
       "      <td>0.292392</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.054592</td>\n",
       "      <td>0.043705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.354200</td>\n",
       "      <td>3.292677</td>\n",
       "      <td>0.345555</td>\n",
       "      <td>0.040596</td>\n",
       "      <td>0.071558</td>\n",
       "      <td>0.049448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.237800</td>\n",
       "      <td>3.188476</td>\n",
       "      <td>0.354720</td>\n",
       "      <td>0.075693</td>\n",
       "      <td>0.075678</td>\n",
       "      <td>0.052887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:42:11,238] Trial 1 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 with params: {'learning_rate': 4.191711516695204e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 04:21 < 08:43, 20.06 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.963400</td>\n",
       "      <td>2.257744</td>\n",
       "      <td>0.563703</td>\n",
       "      <td>0.221807</td>\n",
       "      <td>0.191460</td>\n",
       "      <td>0.175286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.736700</td>\n",
       "      <td>1.547541</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.321468</td>\n",
       "      <td>0.332184</td>\n",
       "      <td>0.311201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.141700</td>\n",
       "      <td>1.261183</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>0.379198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>1.129119</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.449964</td>\n",
       "      <td>0.444318</td>\n",
       "      <td>0.432488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>1.068489</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.514210</td>\n",
       "      <td>0.477430</td>\n",
       "      <td>0.464806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>1.032280</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.519168</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>0.496572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>1.019468</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.520672</td>\n",
       "      <td>0.504214</td>\n",
       "      <td>0.498168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.996127</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.547521</td>\n",
       "      <td>0.528828</td>\n",
       "      <td>0.526161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>1.005063</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.591663</td>\n",
       "      <td>0.536512</td>\n",
       "      <td>0.542552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.013690</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.623133</td>\n",
       "      <td>0.570096</td>\n",
       "      <td>0.580459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:46:34,051] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 with params: {'learning_rate': 0.0001764971584817573, 'weight_decay': 0.002, 'adam_beta1': 0.91, 'warmup_steps': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 04:10 < 08:20, 20.99 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.718600</td>\n",
       "      <td>1.124124</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.430012</td>\n",
       "      <td>0.452198</td>\n",
       "      <td>0.426921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.984090</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.597037</td>\n",
       "      <td>0.545303</td>\n",
       "      <td>0.548502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>1.028383</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.731124</td>\n",
       "      <td>0.646664</td>\n",
       "      <td>0.667369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>1.095383</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.759182</td>\n",
       "      <td>0.648766</td>\n",
       "      <td>0.683444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>1.127443</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.791740</td>\n",
       "      <td>0.719675</td>\n",
       "      <td>0.741851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>1.223752</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.794673</td>\n",
       "      <td>0.678727</td>\n",
       "      <td>0.714091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>1.268112</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.796494</td>\n",
       "      <td>0.695541</td>\n",
       "      <td>0.722982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>1.269241</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.718443</td>\n",
       "      <td>0.735761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>1.357253</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.785889</td>\n",
       "      <td>0.703764</td>\n",
       "      <td>0.726356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.391741</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.794820</td>\n",
       "      <td>0.698062</td>\n",
       "      <td>0.720098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:50:45,261] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 with params: {'learning_rate': 6.624310605949985e-06, 'weight_decay': 0.005, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 08:13 < 04:06, 21.26 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.662400</td>\n",
       "      <td>3.472984</td>\n",
       "      <td>0.238313</td>\n",
       "      <td>0.050701</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>0.029223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.308600</td>\n",
       "      <td>3.158221</td>\n",
       "      <td>0.364803</td>\n",
       "      <td>0.070743</td>\n",
       "      <td>0.078840</td>\n",
       "      <td>0.057232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.017000</td>\n",
       "      <td>2.909736</td>\n",
       "      <td>0.433547</td>\n",
       "      <td>0.109096</td>\n",
       "      <td>0.107549</td>\n",
       "      <td>0.092126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.778500</td>\n",
       "      <td>2.697402</td>\n",
       "      <td>0.480293</td>\n",
       "      <td>0.107169</td>\n",
       "      <td>0.132580</td>\n",
       "      <td>0.111028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.567000</td>\n",
       "      <td>2.512783</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.121687</td>\n",
       "      <td>0.144439</td>\n",
       "      <td>0.120082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.385500</td>\n",
       "      <td>2.356060</td>\n",
       "      <td>0.529789</td>\n",
       "      <td>0.198279</td>\n",
       "      <td>0.165411</td>\n",
       "      <td>0.147753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.224600</td>\n",
       "      <td>2.222272</td>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.219792</td>\n",
       "      <td>0.204131</td>\n",
       "      <td>0.188635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.089200</td>\n",
       "      <td>2.106822</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.251378</td>\n",
       "      <td>0.217953</td>\n",
       "      <td>0.201807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.968700</td>\n",
       "      <td>2.004083</td>\n",
       "      <td>0.601283</td>\n",
       "      <td>0.272815</td>\n",
       "      <td>0.230190</td>\n",
       "      <td>0.211588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.860800</td>\n",
       "      <td>1.914625</td>\n",
       "      <td>0.617782</td>\n",
       "      <td>0.268337</td>\n",
       "      <td>0.241521</td>\n",
       "      <td>0.225232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.765300</td>\n",
       "      <td>1.835541</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.283287</td>\n",
       "      <td>0.262990</td>\n",
       "      <td>0.246878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.680800</td>\n",
       "      <td>1.765408</td>\n",
       "      <td>0.659028</td>\n",
       "      <td>0.301531</td>\n",
       "      <td>0.272517</td>\n",
       "      <td>0.256187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.605300</td>\n",
       "      <td>1.702985</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.324233</td>\n",
       "      <td>0.296345</td>\n",
       "      <td>0.280072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.538500</td>\n",
       "      <td>1.649594</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.314840</td>\n",
       "      <td>0.300501</td>\n",
       "      <td>0.283629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.474100</td>\n",
       "      <td>1.601696</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.312991</td>\n",
       "      <td>0.304951</td>\n",
       "      <td>0.288045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.422600</td>\n",
       "      <td>1.560780</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.340562</td>\n",
       "      <td>0.324271</td>\n",
       "      <td>0.310487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.372800</td>\n",
       "      <td>1.522735</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.342474</td>\n",
       "      <td>0.330599</td>\n",
       "      <td>0.315166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.332000</td>\n",
       "      <td>1.491521</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.352735</td>\n",
       "      <td>0.347090</td>\n",
       "      <td>0.330204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.293500</td>\n",
       "      <td>1.463231</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.348520</td>\n",
       "      <td>0.344096</td>\n",
       "      <td>0.324406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.256900</td>\n",
       "      <td>1.440040</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.344641</td>\n",
       "      <td>0.347078</td>\n",
       "      <td>0.325787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:59:00,364] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 with params: {'learning_rate': 4.480975918214949e-05, 'weight_decay': 0.001, 'adam_beta1': 0.92, 'warmup_steps': 19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 08:20 < 04:10, 20.98 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.885300</td>\n",
       "      <td>2.177733</td>\n",
       "      <td>0.589368</td>\n",
       "      <td>0.249971</td>\n",
       "      <td>0.220980</td>\n",
       "      <td>0.206135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.651700</td>\n",
       "      <td>1.487526</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.345959</td>\n",
       "      <td>0.345297</td>\n",
       "      <td>0.324896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070300</td>\n",
       "      <td>1.227243</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.394352</td>\n",
       "      <td>0.410050</td>\n",
       "      <td>0.388869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>1.107255</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.450003</td>\n",
       "      <td>0.449766</td>\n",
       "      <td>0.436824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>1.055633</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.474739</td>\n",
       "      <td>0.467482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>1.026470</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.513475</td>\n",
       "      <td>0.497831</td>\n",
       "      <td>0.492131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>1.012889</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.535987</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.507240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>1.000166</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.592805</td>\n",
       "      <td>0.541523</td>\n",
       "      <td>0.547104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>1.015615</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.622227</td>\n",
       "      <td>0.558774</td>\n",
       "      <td>0.572243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>1.031643</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.652405</td>\n",
       "      <td>0.584265</td>\n",
       "      <td>0.597838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>1.030561</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.682120</td>\n",
       "      <td>0.609718</td>\n",
       "      <td>0.625858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>1.046158</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.712077</td>\n",
       "      <td>0.630024</td>\n",
       "      <td>0.647739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>1.048540</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.704751</td>\n",
       "      <td>0.627796</td>\n",
       "      <td>0.647194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>1.065855</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.710701</td>\n",
       "      <td>0.628995</td>\n",
       "      <td>0.650374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>1.090440</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.709228</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>0.646376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>1.103757</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>0.624669</td>\n",
       "      <td>0.643305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>1.109508</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.744547</td>\n",
       "      <td>0.663181</td>\n",
       "      <td>0.684179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>1.112039</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.766191</td>\n",
       "      <td>0.684527</td>\n",
       "      <td>0.705064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>1.115392</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.760110</td>\n",
       "      <td>0.680939</td>\n",
       "      <td>0.701272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>1.134714</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794294</td>\n",
       "      <td>0.696647</td>\n",
       "      <td>0.723206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:07:22,009] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 with params: {'learning_rate': 1.7018418817029176e-05, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 04:07 < 08:14, 21.24 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.407700</td>\n",
       "      <td>3.005297</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.095431</td>\n",
       "      <td>0.096880</td>\n",
       "      <td>0.081330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.672900</td>\n",
       "      <td>2.431465</td>\n",
       "      <td>0.527956</td>\n",
       "      <td>0.168556</td>\n",
       "      <td>0.162442</td>\n",
       "      <td>0.141776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.155900</td>\n",
       "      <td>2.037402</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.259777</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>0.214332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.782100</td>\n",
       "      <td>1.759200</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.319855</td>\n",
       "      <td>0.292980</td>\n",
       "      <td>0.275694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.498800</td>\n",
       "      <td>1.558361</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.337296</td>\n",
       "      <td>0.333356</td>\n",
       "      <td>0.312666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.416147</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>0.370928</td>\n",
       "      <td>0.357678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.120700</td>\n",
       "      <td>1.313405</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.437197</td>\n",
       "      <td>0.410695</td>\n",
       "      <td>0.400339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>1.243192</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.427287</td>\n",
       "      <td>0.435693</td>\n",
       "      <td>0.416039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>1.190155</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.444377</td>\n",
       "      <td>0.436403</td>\n",
       "      <td>0.420250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>1.150756</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.441773</td>\n",
       "      <td>0.446109</td>\n",
       "      <td>0.428558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:11:30,303] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 with params: {'learning_rate': 3.971084710792477e-05, 'weight_decay': 0.0, 'adam_beta1': 0.96, 'warmup_steps': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 04:02 < 08:05, 21.61 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.994700</td>\n",
       "      <td>2.341497</td>\n",
       "      <td>0.547204</td>\n",
       "      <td>0.237477</td>\n",
       "      <td>0.183381</td>\n",
       "      <td>0.170401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.827600</td>\n",
       "      <td>1.616623</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.345354</td>\n",
       "      <td>0.326708</td>\n",
       "      <td>0.308671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.215200</td>\n",
       "      <td>1.303025</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.379732</td>\n",
       "      <td>0.394977</td>\n",
       "      <td>0.374846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>1.158569</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.445515</td>\n",
       "      <td>0.433808</td>\n",
       "      <td>0.420620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>1.085915</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.482219</td>\n",
       "      <td>0.463584</td>\n",
       "      <td>0.453397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.041956</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.481346</td>\n",
       "      <td>0.476196</td>\n",
       "      <td>0.466718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>1.013570</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.494922</td>\n",
       "      <td>0.495012</td>\n",
       "      <td>0.486267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.528371</td>\n",
       "      <td>0.509116</td>\n",
       "      <td>0.500890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>1.003636</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.535255</td>\n",
       "      <td>0.541886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>1.013578</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.618266</td>\n",
       "      <td>0.556763</td>\n",
       "      <td>0.564935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:15:34,294] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 with params: {'learning_rate': 1.4982086432155468e-06, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:04 < 10:21, 21.13 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.828100</td>\n",
       "      <td>3.767170</td>\n",
       "      <td>0.169569</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.021744</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.720400</td>\n",
       "      <td>3.677400</td>\n",
       "      <td>0.208066</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.015985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.638300</td>\n",
       "      <td>3.601206</td>\n",
       "      <td>0.208066</td>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.029315</td>\n",
       "      <td>0.016829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.565600</td>\n",
       "      <td>3.530887</td>\n",
       "      <td>0.217232</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>0.032126</td>\n",
       "      <td>0.021555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.496500</td>\n",
       "      <td>3.464697</td>\n",
       "      <td>0.252979</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.042739</td>\n",
       "      <td>0.034375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:17:39,941] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 with params: {'learning_rate': 6.639623079859462e-06, 'weight_decay': 0.001, 'adam_beta1': 0.96, 'warmup_steps': 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:04 < 10:21, 21.13 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.667900</td>\n",
       "      <td>3.480850</td>\n",
       "      <td>0.233731</td>\n",
       "      <td>0.050931</td>\n",
       "      <td>0.036986</td>\n",
       "      <td>0.027908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.316500</td>\n",
       "      <td>3.166579</td>\n",
       "      <td>0.362053</td>\n",
       "      <td>0.066615</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.055410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.024500</td>\n",
       "      <td>2.916885</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.111535</td>\n",
       "      <td>0.107559</td>\n",
       "      <td>0.092148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.785400</td>\n",
       "      <td>2.704860</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.106412</td>\n",
       "      <td>0.131265</td>\n",
       "      <td>0.110208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.573700</td>\n",
       "      <td>2.519529</td>\n",
       "      <td>0.503208</td>\n",
       "      <td>0.122073</td>\n",
       "      <td>0.144224</td>\n",
       "      <td>0.120137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:19:45,277] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 with params: {'learning_rate': 1.2001988398838804e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 04:12 < 08:26, 20.75 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.518000</td>\n",
       "      <td>3.205220</td>\n",
       "      <td>0.350137</td>\n",
       "      <td>0.055049</td>\n",
       "      <td>0.074043</td>\n",
       "      <td>0.051524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.946000</td>\n",
       "      <td>2.741730</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.105751</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.110928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.518600</td>\n",
       "      <td>2.385682</td>\n",
       "      <td>0.528873</td>\n",
       "      <td>0.170257</td>\n",
       "      <td>0.163522</td>\n",
       "      <td>0.142114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.182300</td>\n",
       "      <td>2.115238</td>\n",
       "      <td>0.589368</td>\n",
       "      <td>0.248729</td>\n",
       "      <td>0.218283</td>\n",
       "      <td>0.201860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.908800</td>\n",
       "      <td>1.901465</td>\n",
       "      <td>0.622365</td>\n",
       "      <td>0.274511</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>0.230142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.694600</td>\n",
       "      <td>1.733898</td>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.304223</td>\n",
       "      <td>0.287003</td>\n",
       "      <td>0.268530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.515700</td>\n",
       "      <td>1.599418</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.314555</td>\n",
       "      <td>0.309381</td>\n",
       "      <td>0.289547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.373200</td>\n",
       "      <td>1.496353</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.362585</td>\n",
       "      <td>0.349149</td>\n",
       "      <td>0.330063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.253200</td>\n",
       "      <td>1.410484</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.382710</td>\n",
       "      <td>0.354997</td>\n",
       "      <td>0.338563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.149400</td>\n",
       "      <td>1.346732</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.416617</td>\n",
       "      <td>0.381850</td>\n",
       "      <td>0.367937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:23:59,479] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 with params: {'learning_rate': 1.577858185676612e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:04 < 10:08, 17.24 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.438000</td>\n",
       "      <td>3.067446</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.099407</td>\n",
       "      <td>0.085457</td>\n",
       "      <td>0.067060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.751700</td>\n",
       "      <td>2.519074</td>\n",
       "      <td>0.507791</td>\n",
       "      <td>0.121991</td>\n",
       "      <td>0.146988</td>\n",
       "      <td>0.122942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.253300</td>\n",
       "      <td>2.129996</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.266787</td>\n",
       "      <td>0.221865</td>\n",
       "      <td>0.205547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.885300</td>\n",
       "      <td>1.847402</td>\n",
       "      <td>0.643446</td>\n",
       "      <td>0.317796</td>\n",
       "      <td>0.267326</td>\n",
       "      <td>0.252052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.600100</td>\n",
       "      <td>1.637958</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.330592</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.294627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.384700</td>\n",
       "      <td>1.484595</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.324190</td>\n",
       "      <td>0.336137</td>\n",
       "      <td>0.317289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.210200</td>\n",
       "      <td>1.371491</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.371356</td>\n",
       "      <td>0.361823</td>\n",
       "      <td>0.345801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.078500</td>\n",
       "      <td>1.293762</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.395413</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.385960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>1.232378</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.420703</td>\n",
       "      <td>0.426944</td>\n",
       "      <td>0.410746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>1.188044</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.474565</td>\n",
       "      <td>0.441655</td>\n",
       "      <td>0.430435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:29:05,127] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 with params: {'learning_rate': 5.635479708422883e-06, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:32 < 12:44, 17.16 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.684400</td>\n",
       "      <td>3.517073</td>\n",
       "      <td>0.215399</td>\n",
       "      <td>0.059229</td>\n",
       "      <td>0.031489</td>\n",
       "      <td>0.020824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.237232</td>\n",
       "      <td>0.352887</td>\n",
       "      <td>0.074870</td>\n",
       "      <td>0.074986</td>\n",
       "      <td>0.052266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.113700</td>\n",
       "      <td>3.012577</td>\n",
       "      <td>0.409716</td>\n",
       "      <td>0.098528</td>\n",
       "      <td>0.092812</td>\n",
       "      <td>0.076391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.898800</td>\n",
       "      <td>2.821987</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>0.107170</td>\n",
       "      <td>0.119530</td>\n",
       "      <td>0.102494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.708900</td>\n",
       "      <td>2.654968</td>\n",
       "      <td>0.485793</td>\n",
       "      <td>0.106385</td>\n",
       "      <td>0.135661</td>\n",
       "      <td>0.112958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:31:39,283] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 with params: {'learning_rate': 5.27784544496764e-05, 'weight_decay': 0.003, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:34 < 05:17, 16.54 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.808300</td>\n",
       "      <td>2.047607</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.271546</td>\n",
       "      <td>0.236932</td>\n",
       "      <td>0.221210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.494400</td>\n",
       "      <td>1.378990</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.379249</td>\n",
       "      <td>0.371657</td>\n",
       "      <td>0.355182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>1.160872</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.456119</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>0.426242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>1.067515</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.488189</td>\n",
       "      <td>0.468614</td>\n",
       "      <td>0.460067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>1.027599</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.496194</td>\n",
       "      <td>0.486961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>1.007094</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.555935</td>\n",
       "      <td>0.523478</td>\n",
       "      <td>0.523892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>1.009299</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.591353</td>\n",
       "      <td>0.536645</td>\n",
       "      <td>0.540437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>1.008855</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.644614</td>\n",
       "      <td>0.578214</td>\n",
       "      <td>0.593452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>1.033532</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.667741</td>\n",
       "      <td>0.602050</td>\n",
       "      <td>0.616064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>1.058804</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.712476</td>\n",
       "      <td>0.621315</td>\n",
       "      <td>0.643494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>1.054056</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.701487</td>\n",
       "      <td>0.625916</td>\n",
       "      <td>0.642904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>1.077558</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.708725</td>\n",
       "      <td>0.637367</td>\n",
       "      <td>0.652239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>1.086081</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.758376</td>\n",
       "      <td>0.678347</td>\n",
       "      <td>0.697638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>1.101493</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.767996</td>\n",
       "      <td>0.678916</td>\n",
       "      <td>0.704021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>1.135894</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.758696</td>\n",
       "      <td>0.665843</td>\n",
       "      <td>0.692086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>1.155254</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787593</td>\n",
       "      <td>0.697088</td>\n",
       "      <td>0.722943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>1.158392</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.796277</td>\n",
       "      <td>0.699069</td>\n",
       "      <td>0.724169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>1.166297</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783327</td>\n",
       "      <td>0.708841</td>\n",
       "      <td>0.726004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.165935</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.792209</td>\n",
       "      <td>0.714721</td>\n",
       "      <td>0.735583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.186522</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.790224</td>\n",
       "      <td>0.713165</td>\n",
       "      <td>0.732305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:42:15,324] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 with params: {'learning_rate': 0.00012841880767531, 'weight_decay': 0.001, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:19 < 05:09, 16.96 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.092100</td>\n",
       "      <td>1.290634</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.385464</td>\n",
       "      <td>0.392653</td>\n",
       "      <td>0.369899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>1.033666</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.475244</td>\n",
       "      <td>0.477607</td>\n",
       "      <td>0.469061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.994893</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.598345</td>\n",
       "      <td>0.566967</td>\n",
       "      <td>0.567210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>1.020460</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.689133</td>\n",
       "      <td>0.616881</td>\n",
       "      <td>0.636310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>1.044970</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.721774</td>\n",
       "      <td>0.648095</td>\n",
       "      <td>0.663717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>1.123988</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.752811</td>\n",
       "      <td>0.641055</td>\n",
       "      <td>0.671302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>1.185724</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.791623</td>\n",
       "      <td>0.688428</td>\n",
       "      <td>0.710994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>1.198250</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.798203</td>\n",
       "      <td>0.717840</td>\n",
       "      <td>0.734623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>1.245417</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>0.712059</td>\n",
       "      <td>0.732071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>1.323773</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.812131</td>\n",
       "      <td>0.709286</td>\n",
       "      <td>0.733499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>1.326089</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.780788</td>\n",
       "      <td>0.700740</td>\n",
       "      <td>0.719985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>1.335412</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.780417</td>\n",
       "      <td>0.728172</td>\n",
       "      <td>0.734594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.396376</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.789513</td>\n",
       "      <td>0.705619</td>\n",
       "      <td>0.722424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.363491</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.814260</td>\n",
       "      <td>0.726591</td>\n",
       "      <td>0.750460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.446437</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.801429</td>\n",
       "      <td>0.729610</td>\n",
       "      <td>0.742232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.492058</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.807735</td>\n",
       "      <td>0.715672</td>\n",
       "      <td>0.737814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.484095</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789083</td>\n",
       "      <td>0.707130</td>\n",
       "      <td>0.726016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.509062</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.785574</td>\n",
       "      <td>0.718317</td>\n",
       "      <td>0.732909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.451240</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.729158</td>\n",
       "      <td>0.741123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.495608</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.809439</td>\n",
       "      <td>0.725531</td>\n",
       "      <td>0.744997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:52:35,669] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 with params: {'learning_rate': 0.0003261896776611827, 'weight_decay': 0.003, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:31 < 05:15, 16.63 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.391600</td>\n",
       "      <td>1.015563</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.494010</td>\n",
       "      <td>0.500846</td>\n",
       "      <td>0.484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>1.066737</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.735549</td>\n",
       "      <td>0.667635</td>\n",
       "      <td>0.676495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>1.185036</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.746064</td>\n",
       "      <td>0.739786</td>\n",
       "      <td>0.726835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>1.210986</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.796874</td>\n",
       "      <td>0.695829</td>\n",
       "      <td>0.719563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>1.243421</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.779564</td>\n",
       "      <td>0.719003</td>\n",
       "      <td>0.729639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>1.294853</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.801863</td>\n",
       "      <td>0.710933</td>\n",
       "      <td>0.728030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>1.446721</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.761606</td>\n",
       "      <td>0.689506</td>\n",
       "      <td>0.705208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>1.428022</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.797977</td>\n",
       "      <td>0.718841</td>\n",
       "      <td>0.732409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.434727</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.765918</td>\n",
       "      <td>0.691926</td>\n",
       "      <td>0.712911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.504764</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.756271</td>\n",
       "      <td>0.711473</td>\n",
       "      <td>0.708808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.566351</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794965</td>\n",
       "      <td>0.722739</td>\n",
       "      <td>0.737156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.616439</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.784169</td>\n",
       "      <td>0.728646</td>\n",
       "      <td>0.733059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.709850</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.755610</td>\n",
       "      <td>0.707212</td>\n",
       "      <td>0.713595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.612543</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.788183</td>\n",
       "      <td>0.744012</td>\n",
       "      <td>0.743527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.705605</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790286</td>\n",
       "      <td>0.733288</td>\n",
       "      <td>0.740416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.734250</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.765970</td>\n",
       "      <td>0.709479</td>\n",
       "      <td>0.713692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.750553</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.770747</td>\n",
       "      <td>0.710812</td>\n",
       "      <td>0.718150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.729156</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.758582</td>\n",
       "      <td>0.691472</td>\n",
       "      <td>0.707047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.820136</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.756075</td>\n",
       "      <td>0.717867</td>\n",
       "      <td>0.716303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.768621</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.769665</td>\n",
       "      <td>0.696550</td>\n",
       "      <td>0.707047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 20:03:08,265] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 with params: {'learning_rate': 3.5590132604984735e-05, 'weight_decay': 0.001, 'adam_beta1': 0.97, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:04 < 05:02, 17.37 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.132100</td>\n",
       "      <td>2.510415</td>\n",
       "      <td>0.500458</td>\n",
       "      <td>0.116679</td>\n",
       "      <td>0.143112</td>\n",
       "      <td>0.118432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.004200</td>\n",
       "      <td>1.756272</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.341124</td>\n",
       "      <td>0.301523</td>\n",
       "      <td>0.286849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.366200</td>\n",
       "      <td>1.393194</td>\n",
       "      <td>0.721357</td>\n",
       "      <td>0.346478</td>\n",
       "      <td>0.361893</td>\n",
       "      <td>0.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.996300</td>\n",
       "      <td>1.216884</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.391722</td>\n",
       "      <td>0.396933</td>\n",
       "      <td>0.374981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>1.123113</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.470034</td>\n",
       "      <td>0.450826</td>\n",
       "      <td>0.439427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>1.066812</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.502409</td>\n",
       "      <td>0.474088</td>\n",
       "      <td>0.466103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>1.032897</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.498869</td>\n",
       "      <td>0.491994</td>\n",
       "      <td>0.485765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.410800</td>\n",
       "      <td>1.016768</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.481445</td>\n",
       "      <td>0.496145</td>\n",
       "      <td>0.480947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>1.004886</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.519367</td>\n",
       "      <td>0.505438</td>\n",
       "      <td>0.500570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>1.005497</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.603822</td>\n",
       "      <td>0.531833</td>\n",
       "      <td>0.539771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>1.000312</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.622809</td>\n",
       "      <td>0.554109</td>\n",
       "      <td>0.562110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>1.009354</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.613734</td>\n",
       "      <td>0.569882</td>\n",
       "      <td>0.574339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>1.011650</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.641636</td>\n",
       "      <td>0.576505</td>\n",
       "      <td>0.589547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>1.018782</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.640505</td>\n",
       "      <td>0.598493</td>\n",
       "      <td>0.607435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>1.038988</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.674959</td>\n",
       "      <td>0.605067</td>\n",
       "      <td>0.620872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>1.050826</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.660357</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.617928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>1.055416</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.694314</td>\n",
       "      <td>0.620139</td>\n",
       "      <td>0.636264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>1.059004</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.713470</td>\n",
       "      <td>0.628824</td>\n",
       "      <td>0.648756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>1.054862</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.700768</td>\n",
       "      <td>0.626142</td>\n",
       "      <td>0.643938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>1.074321</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.705597</td>\n",
       "      <td>0.625647</td>\n",
       "      <td>0.645423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 20:13:14,002] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 with params: {'learning_rate': 6.221860592744965e-05, 'weight_decay': 0.004, 'adam_beta1': 0.92, 'warmup_steps': 33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:56 < 05:28, 15.99 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.664400</td>\n",
       "      <td>1.867824</td>\n",
       "      <td>0.634280</td>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.262346</td>\n",
       "      <td>0.246877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.299500</td>\n",
       "      <td>1.270790</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.406841</td>\n",
       "      <td>0.407151</td>\n",
       "      <td>0.386309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.777200</td>\n",
       "      <td>1.103319</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.474195</td>\n",
       "      <td>0.465939</td>\n",
       "      <td>0.448104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>1.034183</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.477752</td>\n",
       "      <td>0.468557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>1.004436</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.534540</td>\n",
       "      <td>0.510236</td>\n",
       "      <td>0.507868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>1.004044</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.596013</td>\n",
       "      <td>0.557661</td>\n",
       "      <td>0.563668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>1.026653</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.642630</td>\n",
       "      <td>0.578725</td>\n",
       "      <td>0.593554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>1.029493</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.668006</td>\n",
       "      <td>0.611261</td>\n",
       "      <td>0.625813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>1.063238</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.680567</td>\n",
       "      <td>0.610919</td>\n",
       "      <td>0.626120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>1.106198</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.724305</td>\n",
       "      <td>0.639814</td>\n",
       "      <td>0.661283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>1.088683</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.745036</td>\n",
       "      <td>0.669261</td>\n",
       "      <td>0.687605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>1.119696</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.755442</td>\n",
       "      <td>0.683339</td>\n",
       "      <td>0.700120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>1.140229</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.775031</td>\n",
       "      <td>0.698518</td>\n",
       "      <td>0.717828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>1.149603</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.774607</td>\n",
       "      <td>0.694545</td>\n",
       "      <td>0.714573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>1.191922</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.792966</td>\n",
       "      <td>0.706702</td>\n",
       "      <td>0.731892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>1.215904</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.783394</td>\n",
       "      <td>0.705091</td>\n",
       "      <td>0.727707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>1.209140</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.785693</td>\n",
       "      <td>0.707157</td>\n",
       "      <td>0.725024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>1.230330</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784353</td>\n",
       "      <td>0.708675</td>\n",
       "      <td>0.725558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.226448</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.795396</td>\n",
       "      <td>0.713432</td>\n",
       "      <td>0.735604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.239999</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.792737</td>\n",
       "      <td>0.722431</td>\n",
       "      <td>0.740185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 20:24:12,034] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 with params: {'learning_rate': 0.00033236239862177063, 'weight_decay': 0.0, 'adam_beta1': 0.91, 'warmup_steps': 45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:20, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.333000</td>\n",
       "      <td>1.003325</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.483513</td>\n",
       "      <td>0.492872</td>\n",
       "      <td>0.475628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>1.079759</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.722475</td>\n",
       "      <td>0.635924</td>\n",
       "      <td>0.658828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>1.146371</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.800383</td>\n",
       "      <td>0.732839</td>\n",
       "      <td>0.747129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>1.264172</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.834719</td>\n",
       "      <td>0.714626</td>\n",
       "      <td>0.750067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.240007</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.801909</td>\n",
       "      <td>0.701405</td>\n",
       "      <td>0.731451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>1.325714</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.835235</td>\n",
       "      <td>0.718061</td>\n",
       "      <td>0.753269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>1.495553</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>0.714663</td>\n",
       "      <td>0.730713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.466256</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.805968</td>\n",
       "      <td>0.723509</td>\n",
       "      <td>0.742602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>1.598888</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.788865</td>\n",
       "      <td>0.698032</td>\n",
       "      <td>0.721095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.559587</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.789035</td>\n",
       "      <td>0.722463</td>\n",
       "      <td>0.730372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.641721</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.761892</td>\n",
       "      <td>0.695562</td>\n",
       "      <td>0.708161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.607311</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.801852</td>\n",
       "      <td>0.727030</td>\n",
       "      <td>0.744403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.719754</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.732658</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>0.700562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.709174</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.750837</td>\n",
       "      <td>0.715490</td>\n",
       "      <td>0.717917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.760548</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782789</td>\n",
       "      <td>0.712825</td>\n",
       "      <td>0.728745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.772953</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.789417</td>\n",
       "      <td>0.723924</td>\n",
       "      <td>0.741947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.799531</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.769198</td>\n",
       "      <td>0.698078</td>\n",
       "      <td>0.714106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.821855</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.765173</td>\n",
       "      <td>0.703637</td>\n",
       "      <td>0.715380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.868924</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.774365</td>\n",
       "      <td>0.702250</td>\n",
       "      <td>0.719401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.842628</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.739627</td>\n",
       "      <td>0.677423</td>\n",
       "      <td>0.689214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.893560</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.764410</td>\n",
       "      <td>0.708415</td>\n",
       "      <td>0.718544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.873527</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.752477</td>\n",
       "      <td>0.692777</td>\n",
       "      <td>0.706312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.837375</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.760994</td>\n",
       "      <td>0.680648</td>\n",
       "      <td>0.700175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.884791</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.774933</td>\n",
       "      <td>0.689707</td>\n",
       "      <td>0.714218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.901612</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.789429</td>\n",
       "      <td>0.695562</td>\n",
       "      <td>0.722104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.940057</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.756887</td>\n",
       "      <td>0.683006</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.940903</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.753592</td>\n",
       "      <td>0.685452</td>\n",
       "      <td>0.699546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.951135</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.757209</td>\n",
       "      <td>0.689891</td>\n",
       "      <td>0.704562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.964018</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.765391</td>\n",
       "      <td>0.685144</td>\n",
       "      <td>0.703529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.970928</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.765632</td>\n",
       "      <td>0.694213</td>\n",
       "      <td>0.709549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 20:39:34,470] Trial 18 finished with value: 0.7095491000571279 and parameters: {'learning_rate': 0.00033236239862177063, 'weight_decay': 0.0, 'adam_beta1': 0.91, 'warmup_steps': 45}. Best is trial 18 with value: 0.7095491000571279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 with params: {'learning_rate': 0.0001885868710330995, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.761800</td>\n",
       "      <td>1.106892</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.434876</td>\n",
       "      <td>0.453564</td>\n",
       "      <td>0.432211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.576723</td>\n",
       "      <td>0.540282</td>\n",
       "      <td>0.540795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>1.031646</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.715873</td>\n",
       "      <td>0.638030</td>\n",
       "      <td>0.659793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>1.088272</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786082</td>\n",
       "      <td>0.675245</td>\n",
       "      <td>0.710852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>1.159673</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.691055</td>\n",
       "      <td>0.718660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>1.247871</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.793928</td>\n",
       "      <td>0.671261</td>\n",
       "      <td>0.708902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.301323</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.789726</td>\n",
       "      <td>0.682305</td>\n",
       "      <td>0.713468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>1.321424</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.791750</td>\n",
       "      <td>0.714405</td>\n",
       "      <td>0.732323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>1.389257</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.782838</td>\n",
       "      <td>0.698515</td>\n",
       "      <td>0.720692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.436843</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.808398</td>\n",
       "      <td>0.720292</td>\n",
       "      <td>0.739990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.473243</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.766701</td>\n",
       "      <td>0.676693</td>\n",
       "      <td>0.701621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.496860</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.788388</td>\n",
       "      <td>0.744054</td>\n",
       "      <td>0.748977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.524970</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784911</td>\n",
       "      <td>0.719797</td>\n",
       "      <td>0.734744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.515320</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.807813</td>\n",
       "      <td>0.729843</td>\n",
       "      <td>0.750425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.620607</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.801749</td>\n",
       "      <td>0.710982</td>\n",
       "      <td>0.736597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.606031</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.818992</td>\n",
       "      <td>0.708768</td>\n",
       "      <td>0.740210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.635571</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.787035</td>\n",
       "      <td>0.710685</td>\n",
       "      <td>0.731746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.680702</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.769941</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.714486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.631632</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.785349</td>\n",
       "      <td>0.704790</td>\n",
       "      <td>0.727824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.688796</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.808022</td>\n",
       "      <td>0.721524</td>\n",
       "      <td>0.744397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.682885</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.775490</td>\n",
       "      <td>0.715825</td>\n",
       "      <td>0.729423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.641992</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.801608</td>\n",
       "      <td>0.712548</td>\n",
       "      <td>0.737767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.709645</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783215</td>\n",
       "      <td>0.710781</td>\n",
       "      <td>0.728557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.725098</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800023</td>\n",
       "      <td>0.709750</td>\n",
       "      <td>0.735812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.757168</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.795315</td>\n",
       "      <td>0.707030</td>\n",
       "      <td>0.731255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.802864</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.789215</td>\n",
       "      <td>0.711960</td>\n",
       "      <td>0.732525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.751409</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.793483</td>\n",
       "      <td>0.709430</td>\n",
       "      <td>0.732413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.744796</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.795251</td>\n",
       "      <td>0.709258</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.749947</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.792380</td>\n",
       "      <td>0.709330</td>\n",
       "      <td>0.730280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.750978</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.791694</td>\n",
       "      <td>0.708163</td>\n",
       "      <td>0.729378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 20:55:11,746] Trial 19 finished with value: 0.7293782518516025 and parameters: {'learning_rate': 0.0001885868710330995, 'weight_decay': 0.0, 'adam_beta1': 0.92, 'warmup_steps': 47}. Best is trial 19 with value: 0.7293782518516025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 with params: {'learning_rate': 0.00019671164081178758, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:24 < 05:12, 16.80 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.711700</td>\n",
       "      <td>1.087168</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.417537</td>\n",
       "      <td>0.446871</td>\n",
       "      <td>0.421858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.994715</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.603763</td>\n",
       "      <td>0.557557</td>\n",
       "      <td>0.564897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>1.025170</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.747009</td>\n",
       "      <td>0.657746</td>\n",
       "      <td>0.680321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>1.088808</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.762234</td>\n",
       "      <td>0.664006</td>\n",
       "      <td>0.696375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>1.176934</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.788408</td>\n",
       "      <td>0.706269</td>\n",
       "      <td>0.730901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>1.229449</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.782159</td>\n",
       "      <td>0.665636</td>\n",
       "      <td>0.700621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>1.286310</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.746222</td>\n",
       "      <td>0.669789</td>\n",
       "      <td>0.689779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>1.310420</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.795618</td>\n",
       "      <td>0.717965</td>\n",
       "      <td>0.735032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.325223</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.801403</td>\n",
       "      <td>0.715271</td>\n",
       "      <td>0.736571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.427810</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.815206</td>\n",
       "      <td>0.703819</td>\n",
       "      <td>0.731925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.478256</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.792994</td>\n",
       "      <td>0.730268</td>\n",
       "      <td>0.743337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.441892</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>0.723424</td>\n",
       "      <td>0.733363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.520550</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.781987</td>\n",
       "      <td>0.726155</td>\n",
       "      <td>0.737345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.482226</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.808398</td>\n",
       "      <td>0.730642</td>\n",
       "      <td>0.751674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.560427</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787352</td>\n",
       "      <td>0.730659</td>\n",
       "      <td>0.742880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.574390</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.797059</td>\n",
       "      <td>0.710008</td>\n",
       "      <td>0.733077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.603644</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.797407</td>\n",
       "      <td>0.727457</td>\n",
       "      <td>0.740088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.603539</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.748771</td>\n",
       "      <td>0.705388</td>\n",
       "      <td>0.714298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.617697</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.753850</td>\n",
       "      <td>0.716925</td>\n",
       "      <td>0.721700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.614041</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.792677</td>\n",
       "      <td>0.715670</td>\n",
       "      <td>0.737206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:05:37,787] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 with params: {'learning_rate': 6.692771043764605e-05, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:35 < 05:17, 16.52 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.645400</td>\n",
       "      <td>1.820614</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.296389</td>\n",
       "      <td>0.281280</td>\n",
       "      <td>0.267993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.242400</td>\n",
       "      <td>1.237769</td>\n",
       "      <td>0.742438</td>\n",
       "      <td>0.402995</td>\n",
       "      <td>0.402565</td>\n",
       "      <td>0.383434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>1.084884</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.466510</td>\n",
       "      <td>0.446404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>1.022774</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.473594</td>\n",
       "      <td>0.483103</td>\n",
       "      <td>0.472806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.999379</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.556285</td>\n",
       "      <td>0.520718</td>\n",
       "      <td>0.518452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>1.007837</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.657801</td>\n",
       "      <td>0.585006</td>\n",
       "      <td>0.602040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>1.036838</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.676877</td>\n",
       "      <td>0.610132</td>\n",
       "      <td>0.626865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>1.038494</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.683872</td>\n",
       "      <td>0.624066</td>\n",
       "      <td>0.637959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>1.077365</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.678990</td>\n",
       "      <td>0.607269</td>\n",
       "      <td>0.624735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.129463</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.731533</td>\n",
       "      <td>0.650182</td>\n",
       "      <td>0.666795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>1.112001</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.752780</td>\n",
       "      <td>0.683140</td>\n",
       "      <td>0.697595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>1.149676</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.773397</td>\n",
       "      <td>0.704195</td>\n",
       "      <td>0.717590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>1.170148</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.776677</td>\n",
       "      <td>0.698132</td>\n",
       "      <td>0.715865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>1.170907</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.763353</td>\n",
       "      <td>0.695397</td>\n",
       "      <td>0.712219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>1.221219</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.793118</td>\n",
       "      <td>0.712850</td>\n",
       "      <td>0.732759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>1.243638</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.792559</td>\n",
       "      <td>0.713469</td>\n",
       "      <td>0.732419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.240766</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.790273</td>\n",
       "      <td>0.707938</td>\n",
       "      <td>0.727618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.265032</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.774904</td>\n",
       "      <td>0.705131</td>\n",
       "      <td>0.718277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>1.250951</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.785341</td>\n",
       "      <td>0.712079</td>\n",
       "      <td>0.731014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.262926</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.781271</td>\n",
       "      <td>0.713857</td>\n",
       "      <td>0.729672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:16:14,632] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 with params: {'learning_rate': 0.00033322985870060107, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:27 < 05:13, 16.73 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323500</td>\n",
       "      <td>1.007401</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.514920</td>\n",
       "      <td>0.516108</td>\n",
       "      <td>0.504191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>1.068182</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.730961</td>\n",
       "      <td>0.679215</td>\n",
       "      <td>0.687097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>1.204379</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.758063</td>\n",
       "      <td>0.735313</td>\n",
       "      <td>0.731556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>1.236222</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.816665</td>\n",
       "      <td>0.717763</td>\n",
       "      <td>0.748263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.332075</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781751</td>\n",
       "      <td>0.710425</td>\n",
       "      <td>0.722848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.368381</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.800132</td>\n",
       "      <td>0.722290</td>\n",
       "      <td>0.736602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.448631</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.790442</td>\n",
       "      <td>0.700948</td>\n",
       "      <td>0.717226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.465040</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.780825</td>\n",
       "      <td>0.689671</td>\n",
       "      <td>0.707230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.549994</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.777309</td>\n",
       "      <td>0.711080</td>\n",
       "      <td>0.720557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.603639</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.773912</td>\n",
       "      <td>0.730199</td>\n",
       "      <td>0.729329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.593390</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>0.698279</td>\n",
       "      <td>0.726344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.603106</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.789762</td>\n",
       "      <td>0.695501</td>\n",
       "      <td>0.719815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.657041</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.779768</td>\n",
       "      <td>0.712029</td>\n",
       "      <td>0.718131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.696625</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.786800</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>0.736973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.687186</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.777463</td>\n",
       "      <td>0.711199</td>\n",
       "      <td>0.714001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.747298</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.799917</td>\n",
       "      <td>0.715979</td>\n",
       "      <td>0.732185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.809124</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.779531</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.716343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.858244</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.790189</td>\n",
       "      <td>0.698692</td>\n",
       "      <td>0.715274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.842094</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.795257</td>\n",
       "      <td>0.711092</td>\n",
       "      <td>0.726620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.866842</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.790306</td>\n",
       "      <td>0.701720</td>\n",
       "      <td>0.718618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:26:43,370] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 with params: {'learning_rate': 0.00025227866809873626, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'warmup_steps': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:18 < 05:09, 16.97 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.482800</td>\n",
       "      <td>1.028372</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.437391</td>\n",
       "      <td>0.462936</td>\n",
       "      <td>0.442513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>1.013941</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.690228</td>\n",
       "      <td>0.623191</td>\n",
       "      <td>0.639235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>1.105249</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.769195</td>\n",
       "      <td>0.676353</td>\n",
       "      <td>0.701982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.151805</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.834723</td>\n",
       "      <td>0.699904</td>\n",
       "      <td>0.741974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.203143</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.785520</td>\n",
       "      <td>0.710887</td>\n",
       "      <td>0.730607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.310153</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.808203</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>0.730582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.359257</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.801669</td>\n",
       "      <td>0.713174</td>\n",
       "      <td>0.734022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.361596</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.793151</td>\n",
       "      <td>0.727520</td>\n",
       "      <td>0.738203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.406247</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.783688</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.721136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.498919</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.800705</td>\n",
       "      <td>0.717980</td>\n",
       "      <td>0.737060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.541339</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.786433</td>\n",
       "      <td>0.720793</td>\n",
       "      <td>0.733829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.501949</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.803524</td>\n",
       "      <td>0.708891</td>\n",
       "      <td>0.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.563789</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790678</td>\n",
       "      <td>0.708270</td>\n",
       "      <td>0.729175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.525730</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.791420</td>\n",
       "      <td>0.727821</td>\n",
       "      <td>0.739701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.627199</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.798210</td>\n",
       "      <td>0.697643</td>\n",
       "      <td>0.725293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.602480</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.792886</td>\n",
       "      <td>0.704746</td>\n",
       "      <td>0.726317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.625438</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.780098</td>\n",
       "      <td>0.715274</td>\n",
       "      <td>0.729170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.631418</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.785603</td>\n",
       "      <td>0.711818</td>\n",
       "      <td>0.727866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.628705</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.785699</td>\n",
       "      <td>0.713809</td>\n",
       "      <td>0.729486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.702675</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.698179</td>\n",
       "      <td>0.722795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:37:03,324] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 with params: {'learning_rate': 0.0003159078969519084, 'weight_decay': 0.0, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:28 < 05:14, 16.71 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.433100</td>\n",
       "      <td>1.017331</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.493664</td>\n",
       "      <td>0.495554</td>\n",
       "      <td>0.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>1.063717</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.739332</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>0.678182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>1.134166</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.798712</td>\n",
       "      <td>0.735757</td>\n",
       "      <td>0.752997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>1.185715</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.804070</td>\n",
       "      <td>0.728038</td>\n",
       "      <td>0.742955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>1.300021</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.783630</td>\n",
       "      <td>0.718017</td>\n",
       "      <td>0.734415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.312599</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.798902</td>\n",
       "      <td>0.712221</td>\n",
       "      <td>0.725761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1.411966</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.771265</td>\n",
       "      <td>0.714233</td>\n",
       "      <td>0.716459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.427894</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.778986</td>\n",
       "      <td>0.705402</td>\n",
       "      <td>0.710105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.516367</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.767470</td>\n",
       "      <td>0.707711</td>\n",
       "      <td>0.719539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.543465</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.788013</td>\n",
       "      <td>0.726808</td>\n",
       "      <td>0.733974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.583214</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.769502</td>\n",
       "      <td>0.698393</td>\n",
       "      <td>0.716717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.548101</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.776338</td>\n",
       "      <td>0.720794</td>\n",
       "      <td>0.726776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.623967</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.762648</td>\n",
       "      <td>0.697992</td>\n",
       "      <td>0.710611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.599600</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.791873</td>\n",
       "      <td>0.730456</td>\n",
       "      <td>0.743072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.672953</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.781783</td>\n",
       "      <td>0.687773</td>\n",
       "      <td>0.712181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.707060</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.783176</td>\n",
       "      <td>0.684358</td>\n",
       "      <td>0.708385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.761887</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.760688</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.716475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.801130</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.773426</td>\n",
       "      <td>0.697517</td>\n",
       "      <td>0.713935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.767474</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.791664</td>\n",
       "      <td>0.705749</td>\n",
       "      <td>0.725987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.826609</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>0.709004</td>\n",
       "      <td>0.727138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:47:32,689] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 with params: {'learning_rate': 0.00048446272517392336, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:04 < 10:08, 17.24 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>0.970260</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.576787</td>\n",
       "      <td>0.568899</td>\n",
       "      <td>0.558608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>1.130399</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.755671</td>\n",
       "      <td>0.672607</td>\n",
       "      <td>0.694595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>1.256247</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.758938</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.722223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>1.372818</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.799880</td>\n",
       "      <td>0.689285</td>\n",
       "      <td>0.726349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>1.476040</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.780938</td>\n",
       "      <td>0.717914</td>\n",
       "      <td>0.727421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.439839</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.796146</td>\n",
       "      <td>0.701585</td>\n",
       "      <td>0.723265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.524400</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.780277</td>\n",
       "      <td>0.689377</td>\n",
       "      <td>0.711391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.600577</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.786636</td>\n",
       "      <td>0.710888</td>\n",
       "      <td>0.727766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.763174</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.822301</td>\n",
       "      <td>0.678056</td>\n",
       "      <td>0.718699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.764102</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.773673</td>\n",
       "      <td>0.677125</td>\n",
       "      <td>0.698395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 21:52:38,324] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 with params: {'learning_rate': 0.0002839050984979592, 'weight_decay': 0.002, 'adam_beta1': 0.92, 'warmup_steps': 48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:24 < 05:12, 16.81 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.460200</td>\n",
       "      <td>1.024715</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.466006</td>\n",
       "      <td>0.477354</td>\n",
       "      <td>0.458433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>1.050202</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.738425</td>\n",
       "      <td>0.654285</td>\n",
       "      <td>0.674106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>1.157279</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.784642</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.739725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>1.221600</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.787593</td>\n",
       "      <td>0.694904</td>\n",
       "      <td>0.719129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>1.284423</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.787736</td>\n",
       "      <td>0.704574</td>\n",
       "      <td>0.729469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>1.336457</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.777146</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>0.715129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>1.429591</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.798793</td>\n",
       "      <td>0.683921</td>\n",
       "      <td>0.716521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>1.440112</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.798932</td>\n",
       "      <td>0.704419</td>\n",
       "      <td>0.726258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.477205</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.809408</td>\n",
       "      <td>0.736848</td>\n",
       "      <td>0.755253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.548370</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.794245</td>\n",
       "      <td>0.725512</td>\n",
       "      <td>0.737981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.570685</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.795348</td>\n",
       "      <td>0.709125</td>\n",
       "      <td>0.730696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.574203</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.798745</td>\n",
       "      <td>0.714287</td>\n",
       "      <td>0.735079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.660837</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.766449</td>\n",
       "      <td>0.710047</td>\n",
       "      <td>0.719636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.600804</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792343</td>\n",
       "      <td>0.720178</td>\n",
       "      <td>0.735318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.701145</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799001</td>\n",
       "      <td>0.707670</td>\n",
       "      <td>0.727859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.717018</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.791120</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.741867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.739447</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.769794</td>\n",
       "      <td>0.720545</td>\n",
       "      <td>0.729838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.739658</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.787454</td>\n",
       "      <td>0.709088</td>\n",
       "      <td>0.731335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.741083</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.718428</td>\n",
       "      <td>0.740040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.781955</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.782796</td>\n",
       "      <td>0.705465</td>\n",
       "      <td>0.725244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 22:03:03,925] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 with params: {'learning_rate': 5.316302127492754e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:05 < 10:11, 17.18 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.809500</td>\n",
       "      <td>2.045164</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.272239</td>\n",
       "      <td>0.237578</td>\n",
       "      <td>0.221823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.490100</td>\n",
       "      <td>1.376288</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>0.354461</td>\n",
       "      <td>0.365943</td>\n",
       "      <td>0.345687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>1.158988</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.458490</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>0.426217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.634300</td>\n",
       "      <td>1.066763</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.487616</td>\n",
       "      <td>0.469025</td>\n",
       "      <td>0.460062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>1.027455</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.500912</td>\n",
       "      <td>0.496194</td>\n",
       "      <td>0.487101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>1.007323</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.575713</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.527235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>1.009930</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.591052</td>\n",
       "      <td>0.536645</td>\n",
       "      <td>0.540277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>1.009111</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.642501</td>\n",
       "      <td>0.578314</td>\n",
       "      <td>0.592713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>1.034934</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.668094</td>\n",
       "      <td>0.601686</td>\n",
       "      <td>0.615965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>1.061218</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.712476</td>\n",
       "      <td>0.621315</td>\n",
       "      <td>0.643494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 22:08:10,834] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 with params: {'learning_rate': 0.0003898152024004867, 'weight_decay': 0.006, 'adam_beta1': 0.96, 'warmup_steps': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.246300</td>\n",
       "      <td>1.042080</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.548750</td>\n",
       "      <td>0.539158</td>\n",
       "      <td>0.526853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>1.099286</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.725423</td>\n",
       "      <td>0.668886</td>\n",
       "      <td>0.677882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>1.244150</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.752234</td>\n",
       "      <td>0.718355</td>\n",
       "      <td>0.720524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>1.318205</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.793225</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.733433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.374488</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.805535</td>\n",
       "      <td>0.727654</td>\n",
       "      <td>0.746984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.445723</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.788912</td>\n",
       "      <td>0.695596</td>\n",
       "      <td>0.717655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.511199</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.748357</td>\n",
       "      <td>0.702699</td>\n",
       "      <td>0.706477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>1.554726</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.762733</td>\n",
       "      <td>0.706959</td>\n",
       "      <td>0.718601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.590509</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.797205</td>\n",
       "      <td>0.704456</td>\n",
       "      <td>0.729457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.611882</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.794802</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.726317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.650980</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790370</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.715242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.688910</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.775264</td>\n",
       "      <td>0.691392</td>\n",
       "      <td>0.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.831249</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.733959</td>\n",
       "      <td>0.683362</td>\n",
       "      <td>0.689770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.757291</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.799506</td>\n",
       "      <td>0.712375</td>\n",
       "      <td>0.733614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.791043</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.771878</td>\n",
       "      <td>0.673364</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.893552</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.763445</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.709940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.894365</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.765955</td>\n",
       "      <td>0.666184</td>\n",
       "      <td>0.694779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.862416</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.764358</td>\n",
       "      <td>0.679484</td>\n",
       "      <td>0.703064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.933489</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.801005</td>\n",
       "      <td>0.678220</td>\n",
       "      <td>0.713132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.896228</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.772697</td>\n",
       "      <td>0.673342</td>\n",
       "      <td>0.701706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.933631</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.769856</td>\n",
       "      <td>0.686302</td>\n",
       "      <td>0.709059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.949077</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.774789</td>\n",
       "      <td>0.658341</td>\n",
       "      <td>0.694177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.997743</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.775418</td>\n",
       "      <td>0.684604</td>\n",
       "      <td>0.709706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.009560</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.773411</td>\n",
       "      <td>0.668521</td>\n",
       "      <td>0.700407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.073267</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.772644</td>\n",
       "      <td>0.667975</td>\n",
       "      <td>0.699845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.030317</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.772788</td>\n",
       "      <td>0.680269</td>\n",
       "      <td>0.708102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.029632</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.769869</td>\n",
       "      <td>0.666518</td>\n",
       "      <td>0.697772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.026553</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.756649</td>\n",
       "      <td>0.661869</td>\n",
       "      <td>0.689918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.053882</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.755227</td>\n",
       "      <td>0.667805</td>\n",
       "      <td>0.693170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.061675</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.759622</td>\n",
       "      <td>0.670143</td>\n",
       "      <td>0.696046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 22:23:49,085] Trial 28 finished with value: 0.6960462624930169 and parameters: {'learning_rate': 0.0003898152024004867, 'weight_decay': 0.006, 'adam_beta1': 0.96, 'warmup_steps': 10}. Best is trial 19 with value: 0.7293782518516025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29 with params: {'learning_rate': 0.00025682128316432246, 'weight_decay': 0.007, 'adam_beta1': 0.98, 'warmup_steps': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:28, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.649500</td>\n",
       "      <td>1.094053</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.466516</td>\n",
       "      <td>0.455685</td>\n",
       "      <td>0.437316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>1.054227</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.677948</td>\n",
       "      <td>0.626052</td>\n",
       "      <td>0.634821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>1.123925</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.746800</td>\n",
       "      <td>0.679551</td>\n",
       "      <td>0.691869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>1.212044</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.787061</td>\n",
       "      <td>0.683446</td>\n",
       "      <td>0.715011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>1.271371</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.784491</td>\n",
       "      <td>0.725155</td>\n",
       "      <td>0.735671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>1.318774</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.710977</td>\n",
       "      <td>0.737665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.772217</td>\n",
       "      <td>0.718419</td>\n",
       "      <td>0.728939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.382954</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.781731</td>\n",
       "      <td>0.701331</td>\n",
       "      <td>0.720570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.424714</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.784259</td>\n",
       "      <td>0.715317</td>\n",
       "      <td>0.734680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.532331</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.762916</td>\n",
       "      <td>0.721341</td>\n",
       "      <td>0.720290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.527013</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.780460</td>\n",
       "      <td>0.704779</td>\n",
       "      <td>0.720844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.545367</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.806455</td>\n",
       "      <td>0.730908</td>\n",
       "      <td>0.749370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.595939</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.801238</td>\n",
       "      <td>0.700741</td>\n",
       "      <td>0.731205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.588271</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.768086</td>\n",
       "      <td>0.716808</td>\n",
       "      <td>0.725990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.655800</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.774718</td>\n",
       "      <td>0.712919</td>\n",
       "      <td>0.724645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.733920</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.791077</td>\n",
       "      <td>0.713599</td>\n",
       "      <td>0.733866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.690871</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.787783</td>\n",
       "      <td>0.712904</td>\n",
       "      <td>0.734574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.737653</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.784063</td>\n",
       "      <td>0.719726</td>\n",
       "      <td>0.733335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.780702</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.799382</td>\n",
       "      <td>0.717096</td>\n",
       "      <td>0.737866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.732251</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.807963</td>\n",
       "      <td>0.709630</td>\n",
       "      <td>0.738747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.829425</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.797649</td>\n",
       "      <td>0.702068</td>\n",
       "      <td>0.730873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.803639</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.803601</td>\n",
       "      <td>0.712186</td>\n",
       "      <td>0.736341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.813337</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.772795</td>\n",
       "      <td>0.718421</td>\n",
       "      <td>0.728743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.822960</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.800367</td>\n",
       "      <td>0.711160</td>\n",
       "      <td>0.735596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.810700</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.796501</td>\n",
       "      <td>0.706572</td>\n",
       "      <td>0.731662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.845665</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.787585</td>\n",
       "      <td>0.723059</td>\n",
       "      <td>0.738657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.864235</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.798123</td>\n",
       "      <td>0.704467</td>\n",
       "      <td>0.731187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.876979</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.703113</td>\n",
       "      <td>0.723910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.870065</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.794218</td>\n",
       "      <td>0.708014</td>\n",
       "      <td>0.731197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.874931</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.794106</td>\n",
       "      <td>0.708014</td>\n",
       "      <td>0.731286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 22:39:19,391] Trial 29 finished with value: 0.7312857459802197 and parameters: {'learning_rate': 0.00025682128316432246, 'weight_decay': 0.007, 'adam_beta1': 0.98, 'warmup_steps': 17}. Best is trial 29 with value: 0.7312857459802197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 with params: {'learning_rate': 0.00010867727717767609, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:32, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.494800</td>\n",
       "      <td>1.644828</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.320610</td>\n",
       "      <td>0.292672</td>\n",
       "      <td>0.278166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>1.111855</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.442625</td>\n",
       "      <td>0.437244</td>\n",
       "      <td>0.415043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>1.018669</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.527403</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.502519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>1.019922</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.647120</td>\n",
       "      <td>0.578632</td>\n",
       "      <td>0.593813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>1.072082</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.712248</td>\n",
       "      <td>0.631681</td>\n",
       "      <td>0.648080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>1.127903</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.741420</td>\n",
       "      <td>0.645769</td>\n",
       "      <td>0.665252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>1.157525</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.753410</td>\n",
       "      <td>0.668721</td>\n",
       "      <td>0.687592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>1.172029</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.799457</td>\n",
       "      <td>0.700268</td>\n",
       "      <td>0.725848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>1.209348</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.771054</td>\n",
       "      <td>0.706845</td>\n",
       "      <td>0.720330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>1.270574</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.774654</td>\n",
       "      <td>0.718415</td>\n",
       "      <td>0.725732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>1.290359</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786724</td>\n",
       "      <td>0.708996</td>\n",
       "      <td>0.725006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.311285</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.776419</td>\n",
       "      <td>0.704381</td>\n",
       "      <td>0.718261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>1.342195</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.780191</td>\n",
       "      <td>0.715879</td>\n",
       "      <td>0.725132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>1.334197</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.785874</td>\n",
       "      <td>0.715934</td>\n",
       "      <td>0.732216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.387247</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778411</td>\n",
       "      <td>0.713066</td>\n",
       "      <td>0.724122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.413357</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.791427</td>\n",
       "      <td>0.723316</td>\n",
       "      <td>0.737162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.451078</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.784261</td>\n",
       "      <td>0.726001</td>\n",
       "      <td>0.736841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.473785</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.790182</td>\n",
       "      <td>0.728426</td>\n",
       "      <td>0.740052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.470555</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.777196</td>\n",
       "      <td>0.716054</td>\n",
       "      <td>0.728422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.477232</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.798577</td>\n",
       "      <td>0.717375</td>\n",
       "      <td>0.735235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.511066</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.790282</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>0.735337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.518810</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.785302</td>\n",
       "      <td>0.723171</td>\n",
       "      <td>0.735771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.514216</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791750</td>\n",
       "      <td>0.723094</td>\n",
       "      <td>0.738106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.526906</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.797213</td>\n",
       "      <td>0.726215</td>\n",
       "      <td>0.743319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.527740</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796064</td>\n",
       "      <td>0.727043</td>\n",
       "      <td>0.743772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.531459</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790662</td>\n",
       "      <td>0.733319</td>\n",
       "      <td>0.744183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.556281</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.722567</td>\n",
       "      <td>0.733161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.539408</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.780962</td>\n",
       "      <td>0.729068</td>\n",
       "      <td>0.738345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.559318</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782666</td>\n",
       "      <td>0.724574</td>\n",
       "      <td>0.734078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.557935</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784124</td>\n",
       "      <td>0.724721</td>\n",
       "      <td>0.734823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 22:54:53,318] Trial 30 finished with value: 0.7348231007366749 and parameters: {'learning_rate': 0.00010867727717767609, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 26}. Best is trial 30 with value: 0.7348231007366749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 with params: {'learning_rate': 5.416464753898959e-05, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.945600</td>\n",
       "      <td>2.283973</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.193392</td>\n",
       "      <td>0.168950</td>\n",
       "      <td>0.154026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.692600</td>\n",
       "      <td>1.485265</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.372059</td>\n",
       "      <td>0.345579</td>\n",
       "      <td>0.326999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.023200</td>\n",
       "      <td>1.189980</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.415357</td>\n",
       "      <td>0.410271</td>\n",
       "      <td>0.393129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.068546</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.439910</td>\n",
       "      <td>0.461087</td>\n",
       "      <td>0.442363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.490700</td>\n",
       "      <td>1.031021</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.480328</td>\n",
       "      <td>0.488084</td>\n",
       "      <td>0.476257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>1.008843</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.470876</td>\n",
       "      <td>0.493291</td>\n",
       "      <td>0.477668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>1.002442</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.627180</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.563441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>1.012375</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.643569</td>\n",
       "      <td>0.574397</td>\n",
       "      <td>0.587679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>1.026069</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.666899</td>\n",
       "      <td>0.609996</td>\n",
       "      <td>0.625429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>1.051076</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.732149</td>\n",
       "      <td>0.647149</td>\n",
       "      <td>0.664724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>1.052660</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.732908</td>\n",
       "      <td>0.646551</td>\n",
       "      <td>0.665654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>1.084374</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.738369</td>\n",
       "      <td>0.665991</td>\n",
       "      <td>0.677320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.090122</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.667512</td>\n",
       "      <td>0.684212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.098951</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797513</td>\n",
       "      <td>0.706620</td>\n",
       "      <td>0.727538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>1.128300</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.796270</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.731674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>1.159199</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791261</td>\n",
       "      <td>0.707687</td>\n",
       "      <td>0.730179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>1.148781</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.802973</td>\n",
       "      <td>0.718368</td>\n",
       "      <td>0.738501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.184383</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783923</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.724184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>1.176209</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.799569</td>\n",
       "      <td>0.711164</td>\n",
       "      <td>0.735310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.198564</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786515</td>\n",
       "      <td>0.704334</td>\n",
       "      <td>0.724146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>1.202417</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.790224</td>\n",
       "      <td>0.706163</td>\n",
       "      <td>0.727106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.215690</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798094</td>\n",
       "      <td>0.708653</td>\n",
       "      <td>0.730354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>1.225385</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794026</td>\n",
       "      <td>0.707038</td>\n",
       "      <td>0.727404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>1.224840</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.789924</td>\n",
       "      <td>0.717059</td>\n",
       "      <td>0.734337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>1.240261</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.798731</td>\n",
       "      <td>0.709120</td>\n",
       "      <td>0.730835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.245069</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.800033</td>\n",
       "      <td>0.707808</td>\n",
       "      <td>0.730351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>1.252751</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781447</td>\n",
       "      <td>0.715333</td>\n",
       "      <td>0.729690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.254283</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.797509</td>\n",
       "      <td>0.716978</td>\n",
       "      <td>0.735490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.259001</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.793748</td>\n",
       "      <td>0.715809</td>\n",
       "      <td>0.733325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>1.257079</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.797149</td>\n",
       "      <td>0.716627</td>\n",
       "      <td>0.735105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:10:45,557] Trial 31 finished with value: 0.7351045121613455 and parameters: {'learning_rate': 5.416464753898959e-05, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 17}. Best is trial 31 with value: 0.7351045121613455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 with params: {'learning_rate': 0.0003098517363425933, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:07 < 05:03, 17.27 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.662100</td>\n",
       "      <td>1.151937</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.415586</td>\n",
       "      <td>0.434383</td>\n",
       "      <td>0.410713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>1.154022</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.640229</td>\n",
       "      <td>0.605613</td>\n",
       "      <td>0.608737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>1.219595</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.688662</td>\n",
       "      <td>0.678395</td>\n",
       "      <td>0.673228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>1.329433</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.791686</td>\n",
       "      <td>0.695698</td>\n",
       "      <td>0.726287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>1.358531</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.785953</td>\n",
       "      <td>0.710721</td>\n",
       "      <td>0.731228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>1.432772</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.801665</td>\n",
       "      <td>0.697701</td>\n",
       "      <td>0.730178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.553298</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.744020</td>\n",
       "      <td>0.669875</td>\n",
       "      <td>0.688887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>1.507646</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.768670</td>\n",
       "      <td>0.700788</td>\n",
       "      <td>0.712675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.572703</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.790730</td>\n",
       "      <td>0.687617</td>\n",
       "      <td>0.718563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.692550</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.753579</td>\n",
       "      <td>0.704421</td>\n",
       "      <td>0.713421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.667726</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.778482</td>\n",
       "      <td>0.722654</td>\n",
       "      <td>0.736181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.712440</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.786071</td>\n",
       "      <td>0.729989</td>\n",
       "      <td>0.743789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.746349</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.769142</td>\n",
       "      <td>0.702888</td>\n",
       "      <td>0.721078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.747360</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.792179</td>\n",
       "      <td>0.701395</td>\n",
       "      <td>0.727105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.773440</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.783158</td>\n",
       "      <td>0.681552</td>\n",
       "      <td>0.706974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.809108</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.803636</td>\n",
       "      <td>0.698021</td>\n",
       "      <td>0.727820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.839600</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.784031</td>\n",
       "      <td>0.682153</td>\n",
       "      <td>0.712435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.945266</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.765671</td>\n",
       "      <td>0.688917</td>\n",
       "      <td>0.704731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.919507</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.774122</td>\n",
       "      <td>0.704490</td>\n",
       "      <td>0.721307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.921767</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.774357</td>\n",
       "      <td>0.702398</td>\n",
       "      <td>0.718330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:20:54,562] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 with params: {'learning_rate': 5.509905487567882e-05, 'weight_decay': 0.007, 'adam_beta1': 0.99, 'warmup_steps': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:32 < 12:41, 17.24 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.919800</td>\n",
       "      <td>2.254354</td>\n",
       "      <td>0.542621</td>\n",
       "      <td>0.204683</td>\n",
       "      <td>0.190185</td>\n",
       "      <td>0.172202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.661500</td>\n",
       "      <td>1.463946</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.368954</td>\n",
       "      <td>0.347623</td>\n",
       "      <td>0.326035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>1.179035</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.420333</td>\n",
       "      <td>0.415621</td>\n",
       "      <td>0.399314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>1.062375</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.442359</td>\n",
       "      <td>0.465250</td>\n",
       "      <td>0.446336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.476600</td>\n",
       "      <td>1.028910</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.476336</td>\n",
       "      <td>0.486064</td>\n",
       "      <td>0.473979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:23:27,880] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34 with params: {'learning_rate': 2.773343681700955e-05, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:08 < 10:17, 16.99 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.295700</td>\n",
       "      <td>2.845498</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>0.087140</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.408000</td>\n",
       "      <td>2.139469</td>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.209669</td>\n",
       "      <td>0.216151</td>\n",
       "      <td>0.193331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.775200</td>\n",
       "      <td>1.681393</td>\n",
       "      <td>0.684693</td>\n",
       "      <td>0.313798</td>\n",
       "      <td>0.304520</td>\n",
       "      <td>0.287596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.345700</td>\n",
       "      <td>1.412168</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.387553</td>\n",
       "      <td>0.361975</td>\n",
       "      <td>0.346864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.056200</td>\n",
       "      <td>1.260748</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.388936</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.367212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>1.166736</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.415086</td>\n",
       "      <td>0.415080</td>\n",
       "      <td>0.397524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.711300</td>\n",
       "      <td>1.099689</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.467124</td>\n",
       "      <td>0.458791</td>\n",
       "      <td>0.447521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>1.064897</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.484871</td>\n",
       "      <td>0.478432</td>\n",
       "      <td>0.464010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>1.043329</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.478511</td>\n",
       "      <td>0.484059</td>\n",
       "      <td>0.472815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.452300</td>\n",
       "      <td>1.017947</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.470526</td>\n",
       "      <td>0.485044</td>\n",
       "      <td>0.471312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:28:37,853] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 with params: {'learning_rate': 8.753744400903284e-05, 'weight_decay': 0.003, 'adam_beta1': 0.99, 'warmup_steps': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.636000</td>\n",
       "      <td>1.837575</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.228513</td>\n",
       "      <td>0.226714</td>\n",
       "      <td>0.202685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.179500</td>\n",
       "      <td>1.190097</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.414047</td>\n",
       "      <td>0.400249</td>\n",
       "      <td>0.375184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>1.034662</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.471513</td>\n",
       "      <td>0.477358</td>\n",
       "      <td>0.461114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>1.002346</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.550471</td>\n",
       "      <td>0.517285</td>\n",
       "      <td>0.518802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>1.028895</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.629785</td>\n",
       "      <td>0.585175</td>\n",
       "      <td>0.591832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>1.062239</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.716245</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>0.651237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>1.079819</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.735030</td>\n",
       "      <td>0.656823</td>\n",
       "      <td>0.674301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>1.091550</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.742484</td>\n",
       "      <td>0.675302</td>\n",
       "      <td>0.689090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.141469</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777869</td>\n",
       "      <td>0.705992</td>\n",
       "      <td>0.722303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>1.187745</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.766623</td>\n",
       "      <td>0.711823</td>\n",
       "      <td>0.720059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>1.209427</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.786061</td>\n",
       "      <td>0.694201</td>\n",
       "      <td>0.716560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>1.220156</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.767919</td>\n",
       "      <td>0.715794</td>\n",
       "      <td>0.722634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>1.247622</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783508</td>\n",
       "      <td>0.713019</td>\n",
       "      <td>0.727648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>1.271401</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790100</td>\n",
       "      <td>0.712779</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.309842</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.706800</td>\n",
       "      <td>0.716578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>1.331999</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.720599</td>\n",
       "      <td>0.737574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>1.353300</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.788170</td>\n",
       "      <td>0.710133</td>\n",
       "      <td>0.729095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>1.381957</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786696</td>\n",
       "      <td>0.713994</td>\n",
       "      <td>0.729380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.372232</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.775565</td>\n",
       "      <td>0.709433</td>\n",
       "      <td>0.726147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.379615</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792916</td>\n",
       "      <td>0.721284</td>\n",
       "      <td>0.737166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>1.389364</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.714928</td>\n",
       "      <td>0.728870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.409132</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786241</td>\n",
       "      <td>0.722938</td>\n",
       "      <td>0.736668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.410574</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.733196</td>\n",
       "      <td>0.745152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.421380</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.792648</td>\n",
       "      <td>0.724896</td>\n",
       "      <td>0.740306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.429525</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798629</td>\n",
       "      <td>0.731193</td>\n",
       "      <td>0.747466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.435277</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791405</td>\n",
       "      <td>0.730949</td>\n",
       "      <td>0.743257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.447366</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.787328</td>\n",
       "      <td>0.723291</td>\n",
       "      <td>0.735479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.438828</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.790692</td>\n",
       "      <td>0.729333</td>\n",
       "      <td>0.742117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.453096</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.793376</td>\n",
       "      <td>0.724548</td>\n",
       "      <td>0.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.450494</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.793284</td>\n",
       "      <td>0.724548</td>\n",
       "      <td>0.738957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:44:15,752] Trial 35 finished with value: 0.7389568059039658 and parameters: {'learning_rate': 8.753744400903284e-05, 'weight_decay': 0.003, 'adam_beta1': 0.99, 'warmup_steps': 20}. Best is trial 35 with value: 0.7389568059039658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 with params: {'learning_rate': 7.900156280003128e-05, 'weight_decay': 0.003, 'adam_beta1': 0.98, 'warmup_steps': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.613100</td>\n",
       "      <td>1.783625</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.307204</td>\n",
       "      <td>0.281012</td>\n",
       "      <td>0.267434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.161600</td>\n",
       "      <td>1.188527</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.398259</td>\n",
       "      <td>0.406519</td>\n",
       "      <td>0.384117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>1.040911</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.463883</td>\n",
       "      <td>0.469620</td>\n",
       "      <td>0.458617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.496880</td>\n",
       "      <td>0.497897</td>\n",
       "      <td>0.487629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>1.005335</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.650023</td>\n",
       "      <td>0.576729</td>\n",
       "      <td>0.590078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>1.026621</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.696039</td>\n",
       "      <td>0.609666</td>\n",
       "      <td>0.632547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>1.038676</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.690724</td>\n",
       "      <td>0.629310</td>\n",
       "      <td>0.641368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>1.059059</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.772659</td>\n",
       "      <td>0.690163</td>\n",
       "      <td>0.710678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>1.120232</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.753742</td>\n",
       "      <td>0.677565</td>\n",
       "      <td>0.697538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>1.154973</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.778250</td>\n",
       "      <td>0.699172</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>1.163353</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.803072</td>\n",
       "      <td>0.711611</td>\n",
       "      <td>0.737482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>1.173946</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.790663</td>\n",
       "      <td>0.720575</td>\n",
       "      <td>0.735459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.202794</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.783688</td>\n",
       "      <td>0.712937</td>\n",
       "      <td>0.731715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>1.226057</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.796566</td>\n",
       "      <td>0.713901</td>\n",
       "      <td>0.736337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.275573</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.777883</td>\n",
       "      <td>0.706923</td>\n",
       "      <td>0.722168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>1.311850</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.706791</td>\n",
       "      <td>0.723043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.315197</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.804173</td>\n",
       "      <td>0.708711</td>\n",
       "      <td>0.733460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>1.333486</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.790172</td>\n",
       "      <td>0.718120</td>\n",
       "      <td>0.732067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>1.313005</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779054</td>\n",
       "      <td>0.708194</td>\n",
       "      <td>0.726553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.337849</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.795187</td>\n",
       "      <td>0.728044</td>\n",
       "      <td>0.743135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.331042</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.779135</td>\n",
       "      <td>0.714004</td>\n",
       "      <td>0.730377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.363968</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.794350</td>\n",
       "      <td>0.722973</td>\n",
       "      <td>0.739154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.380052</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.782634</td>\n",
       "      <td>0.719896</td>\n",
       "      <td>0.732463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.377269</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.800101</td>\n",
       "      <td>0.727228</td>\n",
       "      <td>0.744905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.384156</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.790584</td>\n",
       "      <td>0.717217</td>\n",
       "      <td>0.734644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.394503</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786793</td>\n",
       "      <td>0.725019</td>\n",
       "      <td>0.737835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.405971</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.780143</td>\n",
       "      <td>0.720647</td>\n",
       "      <td>0.731942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.398866</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.792884</td>\n",
       "      <td>0.722041</td>\n",
       "      <td>0.736332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.410176</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791497</td>\n",
       "      <td>0.721930</td>\n",
       "      <td>0.736050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.409643</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.789110</td>\n",
       "      <td>0.721691</td>\n",
       "      <td>0.734667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 23:59:57,650] Trial 36 finished with value: 0.734666875180893 and parameters: {'learning_rate': 7.900156280003128e-05, 'weight_decay': 0.003, 'adam_beta1': 0.98, 'warmup_steps': 22}. Best is trial 35 with value: 0.7389568059039658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 with params: {'learning_rate': 5.977398091589551e-05, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:17 < 10:35, 16.53 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.913100</td>\n",
       "      <td>2.216671</td>\n",
       "      <td>0.545371</td>\n",
       "      <td>0.204007</td>\n",
       "      <td>0.192548</td>\n",
       "      <td>0.172902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.600900</td>\n",
       "      <td>1.420938</td>\n",
       "      <td>0.717690</td>\n",
       "      <td>0.398115</td>\n",
       "      <td>0.366910</td>\n",
       "      <td>0.350137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.939300</td>\n",
       "      <td>1.152544</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.429548</td>\n",
       "      <td>0.424108</td>\n",
       "      <td>0.410450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>1.048143</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.473872</td>\n",
       "      <td>0.473136</td>\n",
       "      <td>0.460209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>1.021370</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.481761</td>\n",
       "      <td>0.487363</td>\n",
       "      <td>0.478659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>1.010061</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.585596</td>\n",
       "      <td>0.525005</td>\n",
       "      <td>0.532138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>1.010886</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.652834</td>\n",
       "      <td>0.576229</td>\n",
       "      <td>0.591227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>1.022041</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.694350</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>0.640188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>1.042668</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.715188</td>\n",
       "      <td>0.637491</td>\n",
       "      <td>0.656142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>1.071215</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.647210</td>\n",
       "      <td>0.659739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 00:05:16,379] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38 with params: {'learning_rate': 0.00010220732101943805, 'weight_decay': 0.003, 'adam_beta1': 0.99, 'warmup_steps': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 16:00, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.506900</td>\n",
       "      <td>1.680324</td>\n",
       "      <td>0.658112</td>\n",
       "      <td>0.278076</td>\n",
       "      <td>0.281256</td>\n",
       "      <td>0.264116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.021000</td>\n",
       "      <td>1.122050</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.454048</td>\n",
       "      <td>0.443334</td>\n",
       "      <td>0.424341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>1.021016</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.506153</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.479565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>1.012977</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.630972</td>\n",
       "      <td>0.562143</td>\n",
       "      <td>0.576711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>1.061021</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.697494</td>\n",
       "      <td>0.623061</td>\n",
       "      <td>0.638706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>1.105792</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.743730</td>\n",
       "      <td>0.648215</td>\n",
       "      <td>0.667574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>1.133121</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.747906</td>\n",
       "      <td>0.651942</td>\n",
       "      <td>0.671631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>1.147972</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.799234</td>\n",
       "      <td>0.701135</td>\n",
       "      <td>0.726151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>1.194702</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.769480</td>\n",
       "      <td>0.706655</td>\n",
       "      <td>0.720282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.240781</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.776041</td>\n",
       "      <td>0.712862</td>\n",
       "      <td>0.721507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>1.278281</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.777754</td>\n",
       "      <td>0.714446</td>\n",
       "      <td>0.723493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>1.292298</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.777989</td>\n",
       "      <td>0.719078</td>\n",
       "      <td>0.728020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.308891</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.771799</td>\n",
       "      <td>0.710380</td>\n",
       "      <td>0.717795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>1.323608</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798383</td>\n",
       "      <td>0.721154</td>\n",
       "      <td>0.737969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>1.368054</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.775804</td>\n",
       "      <td>0.716516</td>\n",
       "      <td>0.723915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.397719</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.795953</td>\n",
       "      <td>0.721933</td>\n",
       "      <td>0.737620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.433382</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797352</td>\n",
       "      <td>0.727779</td>\n",
       "      <td>0.742021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.448170</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.792709</td>\n",
       "      <td>0.729184</td>\n",
       "      <td>0.741986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.442923</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.795337</td>\n",
       "      <td>0.723203</td>\n",
       "      <td>0.740042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.446194</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799459</td>\n",
       "      <td>0.722201</td>\n",
       "      <td>0.739984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.491296</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788731</td>\n",
       "      <td>0.721891</td>\n",
       "      <td>0.734384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.500895</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790633</td>\n",
       "      <td>0.720724</td>\n",
       "      <td>0.734804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.493752</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790335</td>\n",
       "      <td>0.717785</td>\n",
       "      <td>0.734952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.493756</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.796186</td>\n",
       "      <td>0.717507</td>\n",
       "      <td>0.737243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.499524</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789540</td>\n",
       "      <td>0.723008</td>\n",
       "      <td>0.739131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.497811</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.795079</td>\n",
       "      <td>0.725487</td>\n",
       "      <td>0.741342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.527675</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.785979</td>\n",
       "      <td>0.720068</td>\n",
       "      <td>0.732888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.512347</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.789429</td>\n",
       "      <td>0.725355</td>\n",
       "      <td>0.739428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.529489</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791453</td>\n",
       "      <td>0.720989</td>\n",
       "      <td>0.736347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.529251</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788021</td>\n",
       "      <td>0.722757</td>\n",
       "      <td>0.736557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 00:21:18,658] Trial 38 finished with value: 0.7365573554528182 and parameters: {'learning_rate': 0.00010220732101943805, 'weight_decay': 0.003, 'adam_beta1': 0.99, 'warmup_steps': 17}. Best is trial 35 with value: 0.7389568059039658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39 with params: {'learning_rate': 1.9867411486560037e-05, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:33 < 05:16, 16.58 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.408400</td>\n",
       "      <td>3.041130</td>\n",
       "      <td>0.354720</td>\n",
       "      <td>0.056673</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.050397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.690800</td>\n",
       "      <td>2.447186</td>\n",
       "      <td>0.516040</td>\n",
       "      <td>0.188456</td>\n",
       "      <td>0.159896</td>\n",
       "      <td>0.142024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.141000</td>\n",
       "      <td>2.013415</td>\n",
       "      <td>0.598533</td>\n",
       "      <td>0.270064</td>\n",
       "      <td>0.230126</td>\n",
       "      <td>0.209689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.732200</td>\n",
       "      <td>1.706566</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>0.299829</td>\n",
       "      <td>0.285541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.423000</td>\n",
       "      <td>1.498194</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.359478</td>\n",
       "      <td>0.346341</td>\n",
       "      <td>0.330053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.197700</td>\n",
       "      <td>1.354627</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.378620</td>\n",
       "      <td>0.380099</td>\n",
       "      <td>0.363850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.022100</td>\n",
       "      <td>1.258080</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.380745</td>\n",
       "      <td>0.393291</td>\n",
       "      <td>0.370955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.894700</td>\n",
       "      <td>1.189439</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.416785</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>0.390836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.790800</td>\n",
       "      <td>1.145042</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.431817</td>\n",
       "      <td>0.431070</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.701800</td>\n",
       "      <td>1.104107</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.454892</td>\n",
       "      <td>0.450829</td>\n",
       "      <td>0.436162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>1.073509</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.466180</td>\n",
       "      <td>0.466493</td>\n",
       "      <td>0.453465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>1.056958</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.502731</td>\n",
       "      <td>0.478222</td>\n",
       "      <td>0.465461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.519100</td>\n",
       "      <td>1.036959</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.502253</td>\n",
       "      <td>0.478176</td>\n",
       "      <td>0.474348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>1.026891</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.476383</td>\n",
       "      <td>0.481236</td>\n",
       "      <td>0.469909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.437200</td>\n",
       "      <td>1.013993</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.488296</td>\n",
       "      <td>0.490434</td>\n",
       "      <td>0.481256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>1.009549</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.495694</td>\n",
       "      <td>0.489642</td>\n",
       "      <td>0.482816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.502197</td>\n",
       "      <td>0.494802</td>\n",
       "      <td>0.488990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>1.002202</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.508799</td>\n",
       "      <td>0.498865</td>\n",
       "      <td>0.494580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.497778</td>\n",
       "      <td>0.494574</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>1.000929</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.530858</td>\n",
       "      <td>0.505703</td>\n",
       "      <td>0.501887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 00:31:52,915] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 with params: {'learning_rate': 0.00012520413499433398, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.198600</td>\n",
       "      <td>1.345369</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.385426</td>\n",
       "      <td>0.384246</td>\n",
       "      <td>0.364829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>1.043304</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.498292</td>\n",
       "      <td>0.487701</td>\n",
       "      <td>0.482039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>1.007115</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.598359</td>\n",
       "      <td>0.552988</td>\n",
       "      <td>0.556588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>1.018855</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.689110</td>\n",
       "      <td>0.623551</td>\n",
       "      <td>0.639847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>1.061672</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.725606</td>\n",
       "      <td>0.629967</td>\n",
       "      <td>0.653327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>1.106026</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.787180</td>\n",
       "      <td>0.685027</td>\n",
       "      <td>0.710736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>1.166955</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.778921</td>\n",
       "      <td>0.694746</td>\n",
       "      <td>0.712915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>1.189559</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.788909</td>\n",
       "      <td>0.705457</td>\n",
       "      <td>0.724522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>1.251066</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781988</td>\n",
       "      <td>0.708613</td>\n",
       "      <td>0.723745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>1.309275</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.777980</td>\n",
       "      <td>0.705749</td>\n",
       "      <td>0.717081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.321656</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.800115</td>\n",
       "      <td>0.699728</td>\n",
       "      <td>0.727234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.328654</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.791365</td>\n",
       "      <td>0.720814</td>\n",
       "      <td>0.733962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.367169</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.784840</td>\n",
       "      <td>0.716300</td>\n",
       "      <td>0.729965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.382413</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.795226</td>\n",
       "      <td>0.724289</td>\n",
       "      <td>0.737354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.442961</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790868</td>\n",
       "      <td>0.723354</td>\n",
       "      <td>0.738425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.467855</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.799090</td>\n",
       "      <td>0.715726</td>\n",
       "      <td>0.737523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.488671</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.803670</td>\n",
       "      <td>0.733299</td>\n",
       "      <td>0.746302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.511041</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.782297</td>\n",
       "      <td>0.724982</td>\n",
       "      <td>0.736575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.465823</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.798282</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.743803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.495420</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791174</td>\n",
       "      <td>0.716063</td>\n",
       "      <td>0.733737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.548166</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.784595</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.745771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.542869</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.792483</td>\n",
       "      <td>0.735817</td>\n",
       "      <td>0.747457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.581267</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.789645</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.748736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.570599</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.803149</td>\n",
       "      <td>0.734667</td>\n",
       "      <td>0.751173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.602898</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.795333</td>\n",
       "      <td>0.732107</td>\n",
       "      <td>0.746477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.611316</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.794034</td>\n",
       "      <td>0.722399</td>\n",
       "      <td>0.738857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.607619</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.777957</td>\n",
       "      <td>0.728985</td>\n",
       "      <td>0.738912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.606153</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.786953</td>\n",
       "      <td>0.732148</td>\n",
       "      <td>0.742728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.610527</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.787815</td>\n",
       "      <td>0.733924</td>\n",
       "      <td>0.743914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.611090</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781476</td>\n",
       "      <td>0.732398</td>\n",
       "      <td>0.743065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 00:47:42,105] Trial 40 finished with value: 0.7430654138816002 and parameters: {'learning_rate': 0.00012520413499433398, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 31}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 with params: {'learning_rate': 0.00013410300578029893, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.97, 'warmup_steps': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.151400</td>\n",
       "      <td>1.306638</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.410568</td>\n",
       "      <td>0.397153</td>\n",
       "      <td>0.377861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>1.029786</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.486509</td>\n",
       "      <td>0.486457</td>\n",
       "      <td>0.478970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>1.008430</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.621759</td>\n",
       "      <td>0.568463</td>\n",
       "      <td>0.575114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>1.035089</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.700207</td>\n",
       "      <td>0.629022</td>\n",
       "      <td>0.644135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>1.068859</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.741795</td>\n",
       "      <td>0.646585</td>\n",
       "      <td>0.669840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>1.117513</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.790090</td>\n",
       "      <td>0.695526</td>\n",
       "      <td>0.719495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>1.193617</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.774201</td>\n",
       "      <td>0.689761</td>\n",
       "      <td>0.708613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>1.203965</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.792824</td>\n",
       "      <td>0.716205</td>\n",
       "      <td>0.730334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>1.271207</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.772180</td>\n",
       "      <td>0.704615</td>\n",
       "      <td>0.721456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>1.312541</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.789728</td>\n",
       "      <td>0.710929</td>\n",
       "      <td>0.724142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>1.342155</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.797978</td>\n",
       "      <td>0.730928</td>\n",
       "      <td>0.742965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.360377</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.789447</td>\n",
       "      <td>0.721594</td>\n",
       "      <td>0.731265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.392707</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.782264</td>\n",
       "      <td>0.714849</td>\n",
       "      <td>0.728666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.405660</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.795837</td>\n",
       "      <td>0.723836</td>\n",
       "      <td>0.742560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.475860</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.775180</td>\n",
       "      <td>0.716668</td>\n",
       "      <td>0.728366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.488603</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.809949</td>\n",
       "      <td>0.711921</td>\n",
       "      <td>0.735483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.511981</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.797279</td>\n",
       "      <td>0.705170</td>\n",
       "      <td>0.728328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.532951</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.791611</td>\n",
       "      <td>0.725219</td>\n",
       "      <td>0.738163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.507377</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.779588</td>\n",
       "      <td>0.723576</td>\n",
       "      <td>0.737160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.523455</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.808252</td>\n",
       "      <td>0.731015</td>\n",
       "      <td>0.747354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.557262</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776235</td>\n",
       "      <td>0.722178</td>\n",
       "      <td>0.732472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.567673</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.787138</td>\n",
       "      <td>0.725949</td>\n",
       "      <td>0.736544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.588503</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.794058</td>\n",
       "      <td>0.729719</td>\n",
       "      <td>0.742783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.582364</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.806978</td>\n",
       "      <td>0.719527</td>\n",
       "      <td>0.740507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.600523</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.804709</td>\n",
       "      <td>0.720965</td>\n",
       "      <td>0.739821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.623753</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.783315</td>\n",
       "      <td>0.719663</td>\n",
       "      <td>0.733336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.623775</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.776553</td>\n",
       "      <td>0.722249</td>\n",
       "      <td>0.733491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.626553</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.786185</td>\n",
       "      <td>0.723963</td>\n",
       "      <td>0.736989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.632582</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.777645</td>\n",
       "      <td>0.724185</td>\n",
       "      <td>0.735147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.630086</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.779198</td>\n",
       "      <td>0.722893</td>\n",
       "      <td>0.734963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:03:34,068] Trial 41 finished with value: 0.7349632761929046 and parameters: {'learning_rate': 0.00013410300578029893, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.97, 'warmup_steps': 35}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42 with params: {'learning_rate': 0.00025101073365780825, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:29, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.578000</td>\n",
       "      <td>1.044763</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.465320</td>\n",
       "      <td>0.468641</td>\n",
       "      <td>0.452378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>1.008945</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.663783</td>\n",
       "      <td>0.617899</td>\n",
       "      <td>0.623395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>1.080773</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.732491</td>\n",
       "      <td>0.691576</td>\n",
       "      <td>0.697091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>1.151122</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.812262</td>\n",
       "      <td>0.696714</td>\n",
       "      <td>0.732139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>1.205364</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.802080</td>\n",
       "      <td>0.718995</td>\n",
       "      <td>0.744276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>1.274610</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.790893</td>\n",
       "      <td>0.711020</td>\n",
       "      <td>0.733105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>1.369667</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.766105</td>\n",
       "      <td>0.719339</td>\n",
       "      <td>0.723136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>1.343381</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.788781</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.731993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.443368</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.798703</td>\n",
       "      <td>0.734612</td>\n",
       "      <td>0.747256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.503361</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.812538</td>\n",
       "      <td>0.722232</td>\n",
       "      <td>0.742097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.509314</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.809046</td>\n",
       "      <td>0.700965</td>\n",
       "      <td>0.730817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.569769</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.767367</td>\n",
       "      <td>0.722472</td>\n",
       "      <td>0.727538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.570421</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.784754</td>\n",
       "      <td>0.711277</td>\n",
       "      <td>0.728866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.584124</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791876</td>\n",
       "      <td>0.718673</td>\n",
       "      <td>0.731966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.624381</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.797544</td>\n",
       "      <td>0.723331</td>\n",
       "      <td>0.737737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.672752</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.798627</td>\n",
       "      <td>0.694663</td>\n",
       "      <td>0.723595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.645985</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.796131</td>\n",
       "      <td>0.719729</td>\n",
       "      <td>0.735783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.703618</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.786916</td>\n",
       "      <td>0.706077</td>\n",
       "      <td>0.721870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.668050</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785325</td>\n",
       "      <td>0.718394</td>\n",
       "      <td>0.731097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.674468</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.799536</td>\n",
       "      <td>0.722153</td>\n",
       "      <td>0.741884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.714897</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776178</td>\n",
       "      <td>0.703984</td>\n",
       "      <td>0.716346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.711876</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794173</td>\n",
       "      <td>0.716024</td>\n",
       "      <td>0.730768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.776989</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>0.723340</td>\n",
       "      <td>0.740406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.714564</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.794875</td>\n",
       "      <td>0.702230</td>\n",
       "      <td>0.725423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.773163</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.801450</td>\n",
       "      <td>0.727130</td>\n",
       "      <td>0.740944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.758544</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.799384</td>\n",
       "      <td>0.715975</td>\n",
       "      <td>0.734126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.762292</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783259</td>\n",
       "      <td>0.704142</td>\n",
       "      <td>0.720684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.744489</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.798355</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>0.733943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.755108</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.781621</td>\n",
       "      <td>0.718064</td>\n",
       "      <td>0.730526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.756062</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.780782</td>\n",
       "      <td>0.720243</td>\n",
       "      <td>0.729745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:19:05,184] Trial 42 finished with value: 0.7297447610019583 and parameters: {'learning_rate': 0.00025101073365780825, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 38}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 with params: {'learning_rate': 8.649930997204209e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:22 < 05:11, 16.88 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.545100</td>\n",
       "      <td>1.658605</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.330316</td>\n",
       "      <td>0.305572</td>\n",
       "      <td>0.288508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.051500</td>\n",
       "      <td>1.146352</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.430004</td>\n",
       "      <td>0.430963</td>\n",
       "      <td>0.410873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>1.032050</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.492306</td>\n",
       "      <td>0.491841</td>\n",
       "      <td>0.481286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.992228</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.563495</td>\n",
       "      <td>0.528845</td>\n",
       "      <td>0.529882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>1.013871</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.644660</td>\n",
       "      <td>0.595893</td>\n",
       "      <td>0.604379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>1.052479</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.684874</td>\n",
       "      <td>0.609946</td>\n",
       "      <td>0.630191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>1.064534</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.724450</td>\n",
       "      <td>0.647210</td>\n",
       "      <td>0.664724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>1.089201</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.761475</td>\n",
       "      <td>0.679729</td>\n",
       "      <td>0.697475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>1.153940</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.778934</td>\n",
       "      <td>0.695796</td>\n",
       "      <td>0.718749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>1.194259</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.782036</td>\n",
       "      <td>0.706557</td>\n",
       "      <td>0.723435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>1.181733</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.784852</td>\n",
       "      <td>0.690622</td>\n",
       "      <td>0.715157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>1.213359</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780415</td>\n",
       "      <td>0.709718</td>\n",
       "      <td>0.723497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>1.248315</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.784671</td>\n",
       "      <td>0.710787</td>\n",
       "      <td>0.728030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>1.257726</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792494</td>\n",
       "      <td>0.710676</td>\n",
       "      <td>0.735096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.326815</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.773282</td>\n",
       "      <td>0.702538</td>\n",
       "      <td>0.719538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.344899</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.766942</td>\n",
       "      <td>0.699367</td>\n",
       "      <td>0.716073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>1.353990</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.789066</td>\n",
       "      <td>0.689979</td>\n",
       "      <td>0.716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.368091</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.784280</td>\n",
       "      <td>0.707391</td>\n",
       "      <td>0.724002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.355207</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.777075</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>0.724367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.379008</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.776683</td>\n",
       "      <td>0.711073</td>\n",
       "      <td>0.723936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:29:28,370] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 with params: {'learning_rate': 0.00019272781094329593, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:30, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.006400</td>\n",
       "      <td>1.261238</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.379078</td>\n",
       "      <td>0.378595</td>\n",
       "      <td>0.361975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>1.057373</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.569102</td>\n",
       "      <td>0.538820</td>\n",
       "      <td>0.537850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>1.082224</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.652667</td>\n",
       "      <td>0.655895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>1.163652</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.737701</td>\n",
       "      <td>0.663964</td>\n",
       "      <td>0.681729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.212169</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.767862</td>\n",
       "      <td>0.679323</td>\n",
       "      <td>0.701901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>1.279940</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.710541</td>\n",
       "      <td>0.735050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>1.370212</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.787423</td>\n",
       "      <td>0.715066</td>\n",
       "      <td>0.732825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.345386</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.787776</td>\n",
       "      <td>0.708527</td>\n",
       "      <td>0.730101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1.389816</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.802019</td>\n",
       "      <td>0.708806</td>\n",
       "      <td>0.737102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.487260</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780577</td>\n",
       "      <td>0.732859</td>\n",
       "      <td>0.736124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.522983</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.799157</td>\n",
       "      <td>0.708985</td>\n",
       "      <td>0.732628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.530455</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.774426</td>\n",
       "      <td>0.722916</td>\n",
       "      <td>0.732320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.586579</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.759131</td>\n",
       "      <td>0.703254</td>\n",
       "      <td>0.712997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.585154</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.767924</td>\n",
       "      <td>0.721547</td>\n",
       "      <td>0.729670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.605248</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787484</td>\n",
       "      <td>0.721394</td>\n",
       "      <td>0.736198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.647727</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.803002</td>\n",
       "      <td>0.713198</td>\n",
       "      <td>0.738217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.684381</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.794095</td>\n",
       "      <td>0.729018</td>\n",
       "      <td>0.743205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.717918</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.784993</td>\n",
       "      <td>0.713903</td>\n",
       "      <td>0.731353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.729652</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.790233</td>\n",
       "      <td>0.710866</td>\n",
       "      <td>0.733177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.710084</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.773379</td>\n",
       "      <td>0.730349</td>\n",
       "      <td>0.738354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.724634</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.782814</td>\n",
       "      <td>0.716976</td>\n",
       "      <td>0.732408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.751410</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.777657</td>\n",
       "      <td>0.725452</td>\n",
       "      <td>0.735853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.744697</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.797498</td>\n",
       "      <td>0.710069</td>\n",
       "      <td>0.734456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.763989</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.786314</td>\n",
       "      <td>0.716803</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.739330</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.794266</td>\n",
       "      <td>0.709200</td>\n",
       "      <td>0.735158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.781761</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.780557</td>\n",
       "      <td>0.706440</td>\n",
       "      <td>0.726211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.784566</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.796622</td>\n",
       "      <td>0.711658</td>\n",
       "      <td>0.735167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.769150</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.780993</td>\n",
       "      <td>0.708692</td>\n",
       "      <td>0.726688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.794622</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.780339</td>\n",
       "      <td>0.718855</td>\n",
       "      <td>0.734604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.793547</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.781251</td>\n",
       "      <td>0.718491</td>\n",
       "      <td>0.733730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:45:00,729] Trial 44 finished with value: 0.7337300048615195 and parameters: {'learning_rate': 0.00019272781094329593, 'weight_decay': 0.002, 'adam_beta1': 0.99, 'warmup_steps': 18}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 with params: {'learning_rate': 3.5051177072717587e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:33 < 12:45, 17.14 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.076800</td>\n",
       "      <td>2.447258</td>\n",
       "      <td>0.525206</td>\n",
       "      <td>0.166784</td>\n",
       "      <td>0.159462</td>\n",
       "      <td>0.139589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.955700</td>\n",
       "      <td>1.725543</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.311560</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.287055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.342600</td>\n",
       "      <td>1.380131</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>0.397034</td>\n",
       "      <td>0.381645</td>\n",
       "      <td>0.366643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>1.214214</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.393208</td>\n",
       "      <td>0.404382</td>\n",
       "      <td>0.384301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.764100</td>\n",
       "      <td>1.124489</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.469899</td>\n",
       "      <td>0.451815</td>\n",
       "      <td>0.439936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:47:34,927] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 with params: {'learning_rate': 4.3515782249930405e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.97, 'warmup_steps': 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:39 < 05:19, 16.41 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.998400</td>\n",
       "      <td>2.314530</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>0.191262</td>\n",
       "      <td>0.178171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.771900</td>\n",
       "      <td>1.565323</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.339902</td>\n",
       "      <td>0.322779</td>\n",
       "      <td>0.301791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.146100</td>\n",
       "      <td>1.263905</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.381604</td>\n",
       "      <td>0.395149</td>\n",
       "      <td>0.375683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.809600</td>\n",
       "      <td>1.128366</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.437232</td>\n",
       "      <td>0.442448</td>\n",
       "      <td>0.427790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.606100</td>\n",
       "      <td>1.068694</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.481716</td>\n",
       "      <td>0.466425</td>\n",
       "      <td>0.453993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>1.028353</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.459622</td>\n",
       "      <td>0.477777</td>\n",
       "      <td>0.462616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>1.005507</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.508407</td>\n",
       "      <td>0.507788</td>\n",
       "      <td>0.498592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>1.003179</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.592425</td>\n",
       "      <td>0.535657</td>\n",
       "      <td>0.540511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>1.007307</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.594975</td>\n",
       "      <td>0.539734</td>\n",
       "      <td>0.546314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>1.020983</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.648165</td>\n",
       "      <td>0.587179</td>\n",
       "      <td>0.597365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>1.016836</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.681461</td>\n",
       "      <td>0.608206</td>\n",
       "      <td>0.626140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>1.036604</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.663457</td>\n",
       "      <td>0.607817</td>\n",
       "      <td>0.616041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>1.038886</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.701933</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.644450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>1.052960</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.703205</td>\n",
       "      <td>0.630392</td>\n",
       "      <td>0.647476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.080216</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.704469</td>\n",
       "      <td>0.625843</td>\n",
       "      <td>0.645174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>1.094739</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.702990</td>\n",
       "      <td>0.629433</td>\n",
       "      <td>0.646527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>1.098856</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.707206</td>\n",
       "      <td>0.626598</td>\n",
       "      <td>0.646649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.104882</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.756211</td>\n",
       "      <td>0.676223</td>\n",
       "      <td>0.696461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.100825</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.755468</td>\n",
       "      <td>0.676548</td>\n",
       "      <td>0.697693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>1.123870</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>0.692286</td>\n",
       "      <td>0.719635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 01:58:15,865] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 with params: {'learning_rate': 0.0003753823772443784, 'weight_decay': 0.01, 'adam_beta1': 0.98, 'warmup_steps': 26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:23 < 05:11, 16.85 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.417100</td>\n",
       "      <td>1.072422</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.527198</td>\n",
       "      <td>0.522332</td>\n",
       "      <td>0.509543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>1.167029</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.716385</td>\n",
       "      <td>0.648689</td>\n",
       "      <td>0.658630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>1.267442</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.766414</td>\n",
       "      <td>0.708635</td>\n",
       "      <td>0.721804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.318705</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.781957</td>\n",
       "      <td>0.720866</td>\n",
       "      <td>0.732151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>1.415661</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.802389</td>\n",
       "      <td>0.714706</td>\n",
       "      <td>0.739336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.462827</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.807553</td>\n",
       "      <td>0.680601</td>\n",
       "      <td>0.714276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>1.533412</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.781936</td>\n",
       "      <td>0.702427</td>\n",
       "      <td>0.721583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.556657</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.807604</td>\n",
       "      <td>0.703252</td>\n",
       "      <td>0.729989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>1.625016</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.760400</td>\n",
       "      <td>0.715969</td>\n",
       "      <td>0.722702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.630892</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.811944</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.737830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.645041</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.783100</td>\n",
       "      <td>0.709879</td>\n",
       "      <td>0.727883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.753792</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.797961</td>\n",
       "      <td>0.695384</td>\n",
       "      <td>0.720396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.698697</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.791979</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.730509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.687831</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.773054</td>\n",
       "      <td>0.716256</td>\n",
       "      <td>0.731763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.793513</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.794142</td>\n",
       "      <td>0.691697</td>\n",
       "      <td>0.719943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.829436</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.784005</td>\n",
       "      <td>0.681282</td>\n",
       "      <td>0.707891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.870326</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.761056</td>\n",
       "      <td>0.704991</td>\n",
       "      <td>0.717895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.831520</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.760615</td>\n",
       "      <td>0.696136</td>\n",
       "      <td>0.705044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.806197</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.791624</td>\n",
       "      <td>0.708466</td>\n",
       "      <td>0.730815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.909409</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.792308</td>\n",
       "      <td>0.708118</td>\n",
       "      <td>0.730118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 02:08:40,203] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 with params: {'learning_rate': 0.00040740464809269436, 'weight_decay': 0.01, 'adam_beta1': 0.98, 'warmup_steps': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:13 < 10:26, 16.76 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.423200</td>\n",
       "      <td>1.087546</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.492642</td>\n",
       "      <td>0.503620</td>\n",
       "      <td>0.490681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>1.127852</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.718556</td>\n",
       "      <td>0.657734</td>\n",
       "      <td>0.670288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>1.284736</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.760722</td>\n",
       "      <td>0.703979</td>\n",
       "      <td>0.715226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>1.306137</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.712338</td>\n",
       "      <td>0.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>1.374176</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.787586</td>\n",
       "      <td>0.687329</td>\n",
       "      <td>0.713692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>1.489161</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.775755</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>0.714908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>1.540925</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.742204</td>\n",
       "      <td>0.700586</td>\n",
       "      <td>0.702788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.557080</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.776154</td>\n",
       "      <td>0.696272</td>\n",
       "      <td>0.715881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.743790</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.766399</td>\n",
       "      <td>0.694772</td>\n",
       "      <td>0.713030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.764621</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.716680</td>\n",
       "      <td>0.695952</td>\n",
       "      <td>0.691337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 02:13:54,545] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 with params: {'learning_rate': 2.2037250878112672e-06, 'weight_decay': 0.002, 'adam_beta1': 0.97, 'warmup_steps': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:31 < 05:15, 16.63 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.802200</td>\n",
       "      <td>3.718979</td>\n",
       "      <td>0.203483</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.015477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.655500</td>\n",
       "      <td>3.595393</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.016688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.539800</td>\n",
       "      <td>3.485011</td>\n",
       "      <td>0.235564</td>\n",
       "      <td>0.052081</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>0.029123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.435800</td>\n",
       "      <td>3.382716</td>\n",
       "      <td>0.308891</td>\n",
       "      <td>0.044367</td>\n",
       "      <td>0.059738</td>\n",
       "      <td>0.046106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.336000</td>\n",
       "      <td>3.290022</td>\n",
       "      <td>0.346471</td>\n",
       "      <td>0.040951</td>\n",
       "      <td>0.071921</td>\n",
       "      <td>0.049736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.246100</td>\n",
       "      <td>3.206956</td>\n",
       "      <td>0.353804</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>0.075349</td>\n",
       "      <td>0.052468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.163300</td>\n",
       "      <td>3.132471</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.072345</td>\n",
       "      <td>0.080370</td>\n",
       "      <td>0.059639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.089300</td>\n",
       "      <td>3.065569</td>\n",
       "      <td>0.395050</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.088494</td>\n",
       "      <td>0.071286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.022200</td>\n",
       "      <td>3.003909</td>\n",
       "      <td>0.414299</td>\n",
       "      <td>0.095281</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>0.079246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.959600</td>\n",
       "      <td>2.947238</td>\n",
       "      <td>0.428048</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>0.103236</td>\n",
       "      <td>0.088373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.902500</td>\n",
       "      <td>2.895444</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>0.109456</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>0.092129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.850700</td>\n",
       "      <td>2.847930</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.109592</td>\n",
       "      <td>0.114645</td>\n",
       "      <td>0.099328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.802200</td>\n",
       "      <td>2.803046</td>\n",
       "      <td>0.453712</td>\n",
       "      <td>0.107315</td>\n",
       "      <td>0.118647</td>\n",
       "      <td>0.102066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.757100</td>\n",
       "      <td>2.761976</td>\n",
       "      <td>0.468378</td>\n",
       "      <td>0.109414</td>\n",
       "      <td>0.127910</td>\n",
       "      <td>0.109203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.713700</td>\n",
       "      <td>2.723819</td>\n",
       "      <td>0.472961</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>0.129247</td>\n",
       "      <td>0.109234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.676500</td>\n",
       "      <td>2.689207</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>0.129462</td>\n",
       "      <td>0.108833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.641700</td>\n",
       "      <td>2.656704</td>\n",
       "      <td>0.483043</td>\n",
       "      <td>0.106440</td>\n",
       "      <td>0.134763</td>\n",
       "      <td>0.112277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.609300</td>\n",
       "      <td>2.627206</td>\n",
       "      <td>0.487626</td>\n",
       "      <td>0.106435</td>\n",
       "      <td>0.136281</td>\n",
       "      <td>0.113263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.579400</td>\n",
       "      <td>2.600336</td>\n",
       "      <td>0.490376</td>\n",
       "      <td>0.105699</td>\n",
       "      <td>0.137481</td>\n",
       "      <td>0.113648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.551800</td>\n",
       "      <td>2.575960</td>\n",
       "      <td>0.492209</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.138776</td>\n",
       "      <td>0.114844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 02:24:27,091] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 with params: {'learning_rate': 7.09065670740699e-05, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:20 < 05:10, 16.92 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.767100</td>\n",
       "      <td>2.025421</td>\n",
       "      <td>0.577452</td>\n",
       "      <td>0.219492</td>\n",
       "      <td>0.211669</td>\n",
       "      <td>0.189214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.387200</td>\n",
       "      <td>1.295684</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.396038</td>\n",
       "      <td>0.395095</td>\n",
       "      <td>0.369298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>1.083825</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.467137</td>\n",
       "      <td>0.447687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>1.014455</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.469194</td>\n",
       "      <td>0.484584</td>\n",
       "      <td>0.469321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>1.012396</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.552038</td>\n",
       "      <td>0.517201</td>\n",
       "      <td>0.512720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>1.021441</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.644374</td>\n",
       "      <td>0.575007</td>\n",
       "      <td>0.588796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>1.027078</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.694489</td>\n",
       "      <td>0.615857</td>\n",
       "      <td>0.634220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>1.040886</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.708326</td>\n",
       "      <td>0.631357</td>\n",
       "      <td>0.651130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>1.077026</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.737350</td>\n",
       "      <td>0.654881</td>\n",
       "      <td>0.676431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>1.116915</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.777415</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.721444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>1.130655</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.801578</td>\n",
       "      <td>0.708817</td>\n",
       "      <td>0.734256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>1.147787</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.774030</td>\n",
       "      <td>0.717043</td>\n",
       "      <td>0.728290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>1.162416</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783398</td>\n",
       "      <td>0.700665</td>\n",
       "      <td>0.722683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>1.181611</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786520</td>\n",
       "      <td>0.714141</td>\n",
       "      <td>0.733692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>1.220127</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.781423</td>\n",
       "      <td>0.708774</td>\n",
       "      <td>0.726536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>1.246648</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.785063</td>\n",
       "      <td>0.718115</td>\n",
       "      <td>0.734419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>1.246758</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.797536</td>\n",
       "      <td>0.713003</td>\n",
       "      <td>0.734169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>1.281474</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.782154</td>\n",
       "      <td>0.710292</td>\n",
       "      <td>0.725308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.268727</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.784448</td>\n",
       "      <td>0.711090</td>\n",
       "      <td>0.731326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.286812</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.793752</td>\n",
       "      <td>0.716995</td>\n",
       "      <td>0.733622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 02:34:48,677] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 with params: {'learning_rate': 0.00010095801211097598, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.588800</td>\n",
       "      <td>1.745987</td>\n",
       "      <td>0.637030</td>\n",
       "      <td>0.273614</td>\n",
       "      <td>0.263885</td>\n",
       "      <td>0.246342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.070200</td>\n",
       "      <td>1.141792</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.436712</td>\n",
       "      <td>0.438658</td>\n",
       "      <td>0.413868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>1.020862</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.522187</td>\n",
       "      <td>0.492850</td>\n",
       "      <td>0.483655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>1.016845</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.586350</td>\n",
       "      <td>0.547446</td>\n",
       "      <td>0.551201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>1.055170</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.714295</td>\n",
       "      <td>0.620710</td>\n",
       "      <td>0.642311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>1.098913</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.747493</td>\n",
       "      <td>0.653466</td>\n",
       "      <td>0.674133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>1.135634</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>0.652360</td>\n",
       "      <td>0.674097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1.147120</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790518</td>\n",
       "      <td>0.702954</td>\n",
       "      <td>0.725571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>1.204458</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.776389</td>\n",
       "      <td>0.706204</td>\n",
       "      <td>0.721670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.251051</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.766697</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.716352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>1.274805</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.782306</td>\n",
       "      <td>0.708110</td>\n",
       "      <td>0.722114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>1.302146</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.771631</td>\n",
       "      <td>0.707573</td>\n",
       "      <td>0.720088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>1.332466</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.770305</td>\n",
       "      <td>0.711526</td>\n",
       "      <td>0.720781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.333224</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.716238</td>\n",
       "      <td>0.739944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.382016</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.772882</td>\n",
       "      <td>0.710286</td>\n",
       "      <td>0.721507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>1.411814</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.802392</td>\n",
       "      <td>0.717815</td>\n",
       "      <td>0.739790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.424039</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780855</td>\n",
       "      <td>0.707587</td>\n",
       "      <td>0.723677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.457337</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.791245</td>\n",
       "      <td>0.720580</td>\n",
       "      <td>0.736283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.455856</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.764398</td>\n",
       "      <td>0.709623</td>\n",
       "      <td>0.722503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.463781</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.708898</td>\n",
       "      <td>0.727913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.486148</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788834</td>\n",
       "      <td>0.732344</td>\n",
       "      <td>0.742510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.501941</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.789098</td>\n",
       "      <td>0.722282</td>\n",
       "      <td>0.736938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.502475</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.732216</td>\n",
       "      <td>0.742365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.509171</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800223</td>\n",
       "      <td>0.719934</td>\n",
       "      <td>0.738442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.524251</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.794905</td>\n",
       "      <td>0.727202</td>\n",
       "      <td>0.742201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.524568</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.784260</td>\n",
       "      <td>0.730319</td>\n",
       "      <td>0.740551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.544005</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.790935</td>\n",
       "      <td>0.726553</td>\n",
       "      <td>0.739067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.527145</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.729691</td>\n",
       "      <td>0.739986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.542946</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.789928</td>\n",
       "      <td>0.727747</td>\n",
       "      <td>0.739887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.542403</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.791247</td>\n",
       "      <td>0.727509</td>\n",
       "      <td>0.740472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 02:50:25,958] Trial 51 finished with value: 0.7404721838627663 and parameters: {'learning_rate': 0.00010095801211097598, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 38}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52 with params: {'learning_rate': 0.00011320008962884753, 'weight_decay': 0.008, 'adam_beta1': 0.99, 'warmup_steps': 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:30 < 05:15, 16.66 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.505500</td>\n",
       "      <td>1.638037</td>\n",
       "      <td>0.663611</td>\n",
       "      <td>0.308901</td>\n",
       "      <td>0.306864</td>\n",
       "      <td>0.287860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.963900</td>\n",
       "      <td>1.108444</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.435648</td>\n",
       "      <td>0.449010</td>\n",
       "      <td>0.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>1.017499</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.529946</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.508146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>1.029009</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.665928</td>\n",
       "      <td>0.586114</td>\n",
       "      <td>0.605044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>1.079664</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.700354</td>\n",
       "      <td>0.627215</td>\n",
       "      <td>0.641945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>1.141605</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.737497</td>\n",
       "      <td>0.652895</td>\n",
       "      <td>0.670008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.178661</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.787374</td>\n",
       "      <td>0.688337</td>\n",
       "      <td>0.713871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>1.183417</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.801492</td>\n",
       "      <td>0.703194</td>\n",
       "      <td>0.728743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>1.218223</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>0.704767</td>\n",
       "      <td>0.717585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.277234</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.776045</td>\n",
       "      <td>0.714089</td>\n",
       "      <td>0.721217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>1.297508</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.785445</td>\n",
       "      <td>0.708736</td>\n",
       "      <td>0.723212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.333412</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.772676</td>\n",
       "      <td>0.703949</td>\n",
       "      <td>0.715344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.351461</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.777905</td>\n",
       "      <td>0.718664</td>\n",
       "      <td>0.727560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.356915</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791424</td>\n",
       "      <td>0.727132</td>\n",
       "      <td>0.742232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.431328</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784974</td>\n",
       "      <td>0.719176</td>\n",
       "      <td>0.731353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.437870</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.797842</td>\n",
       "      <td>0.717727</td>\n",
       "      <td>0.736481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.477249</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.792479</td>\n",
       "      <td>0.714862</td>\n",
       "      <td>0.731702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.505397</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.787947</td>\n",
       "      <td>0.729327</td>\n",
       "      <td>0.739438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.486042</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.771053</td>\n",
       "      <td>0.715585</td>\n",
       "      <td>0.724720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.501750</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.793558</td>\n",
       "      <td>0.716437</td>\n",
       "      <td>0.732558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:00:57,161] Trial 52 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53 with params: {'learning_rate': 5.443255711584556e-05, 'weight_decay': 0.01, 'adam_beta1': 0.96, 'warmup_steps': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:08 < 10:18, 16.99 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.781000</td>\n",
       "      <td>2.030293</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.262147</td>\n",
       "      <td>0.239436</td>\n",
       "      <td>0.223698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.471500</td>\n",
       "      <td>1.363388</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.382093</td>\n",
       "      <td>0.380942</td>\n",
       "      <td>0.363536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>1.144690</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.450864</td>\n",
       "      <td>0.440666</td>\n",
       "      <td>0.427487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.615100</td>\n",
       "      <td>1.054428</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.474088</td>\n",
       "      <td>0.465639</td>\n",
       "      <td>0.455474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>1.019332</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.487965</td>\n",
       "      <td>0.488208</td>\n",
       "      <td>0.478090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>1.005204</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.551408</td>\n",
       "      <td>0.517362</td>\n",
       "      <td>0.519157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.241200</td>\n",
       "      <td>0.998764</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.586123</td>\n",
       "      <td>0.538450</td>\n",
       "      <td>0.541696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>1.007763</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.648456</td>\n",
       "      <td>0.586916</td>\n",
       "      <td>0.598286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>1.028021</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.660647</td>\n",
       "      <td>0.599110</td>\n",
       "      <td>0.613839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>1.056236</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.705587</td>\n",
       "      <td>0.622671</td>\n",
       "      <td>0.641449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:06:07,242] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54 with params: {'learning_rate': 8.383415806965337e-05, 'weight_decay': 0.006, 'adam_beta1': 0.96, 'warmup_steps': 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:26 < 05:13, 16.76 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.501600</td>\n",
       "      <td>1.634399</td>\n",
       "      <td>0.682860</td>\n",
       "      <td>0.314744</td>\n",
       "      <td>0.305342</td>\n",
       "      <td>0.290488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.045200</td>\n",
       "      <td>1.141020</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.443953</td>\n",
       "      <td>0.443028</td>\n",
       "      <td>0.425998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>1.031367</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.472524</td>\n",
       "      <td>0.490440</td>\n",
       "      <td>0.475759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.991879</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.540135</td>\n",
       "      <td>0.514001</td>\n",
       "      <td>0.510031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>1.004774</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.640628</td>\n",
       "      <td>0.595526</td>\n",
       "      <td>0.603747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>1.039051</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.688217</td>\n",
       "      <td>0.607796</td>\n",
       "      <td>0.629397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>1.055076</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.706172</td>\n",
       "      <td>0.640556</td>\n",
       "      <td>0.656202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>1.082590</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.769994</td>\n",
       "      <td>0.685272</td>\n",
       "      <td>0.705069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>1.132110</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.783278</td>\n",
       "      <td>0.700676</td>\n",
       "      <td>0.722655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>1.183919</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.778530</td>\n",
       "      <td>0.701344</td>\n",
       "      <td>0.720711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>1.174711</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.789359</td>\n",
       "      <td>0.693832</td>\n",
       "      <td>0.718976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>1.205318</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.792170</td>\n",
       "      <td>0.719392</td>\n",
       "      <td>0.732803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>1.223759</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.781075</td>\n",
       "      <td>0.713664</td>\n",
       "      <td>0.729560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>1.244691</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.780852</td>\n",
       "      <td>0.700048</td>\n",
       "      <td>0.722593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>1.315010</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.761389</td>\n",
       "      <td>0.695682</td>\n",
       "      <td>0.710372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>1.334143</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785582</td>\n",
       "      <td>0.718479</td>\n",
       "      <td>0.733975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.333464</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.792065</td>\n",
       "      <td>0.702077</td>\n",
       "      <td>0.726739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>1.361254</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.780428</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.731382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.339262</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778627</td>\n",
       "      <td>0.707795</td>\n",
       "      <td>0.724661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.349975</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.775327</td>\n",
       "      <td>0.723585</td>\n",
       "      <td>0.730527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:16:34,767] Trial 54 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 55 with params: {'learning_rate': 2.2793208043763986e-06, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:10 < 10:21, 16.88 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.797400</td>\n",
       "      <td>3.711179</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.031131</td>\n",
       "      <td>0.014988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.645800</td>\n",
       "      <td>3.583209</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.015411</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.016834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.526100</td>\n",
       "      <td>3.468997</td>\n",
       "      <td>0.242896</td>\n",
       "      <td>0.050634</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.031224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.418400</td>\n",
       "      <td>3.362981</td>\n",
       "      <td>0.316224</td>\n",
       "      <td>0.042533</td>\n",
       "      <td>0.062020</td>\n",
       "      <td>0.046377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.315300</td>\n",
       "      <td>3.268545</td>\n",
       "      <td>0.348304</td>\n",
       "      <td>0.059587</td>\n",
       "      <td>0.072614</td>\n",
       "      <td>0.049726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.222900</td>\n",
       "      <td>3.183723</td>\n",
       "      <td>0.356554</td>\n",
       "      <td>0.070244</td>\n",
       "      <td>0.076405</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.138500</td>\n",
       "      <td>3.107968</td>\n",
       "      <td>0.378552</td>\n",
       "      <td>0.097543</td>\n",
       "      <td>0.083805</td>\n",
       "      <td>0.065184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.063100</td>\n",
       "      <td>3.039802</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.099123</td>\n",
       "      <td>0.091522</td>\n",
       "      <td>0.074706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.994700</td>\n",
       "      <td>2.976801</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>0.099271</td>\n",
       "      <td>0.083854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.930900</td>\n",
       "      <td>2.919061</td>\n",
       "      <td>0.434464</td>\n",
       "      <td>0.111115</td>\n",
       "      <td>0.107380</td>\n",
       "      <td>0.092327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:21:47,266] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 56 with params: {'learning_rate': 0.00010023894871972397, 'weight_decay': 0.005, 'adam_beta1': 0.99, 'warmup_steps': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:14 < 10:29, 16.68 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.620900</td>\n",
       "      <td>1.778612</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>0.268386</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>0.232873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.091500</td>\n",
       "      <td>1.148964</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.406970</td>\n",
       "      <td>0.431778</td>\n",
       "      <td>0.405699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>1.021044</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.522898</td>\n",
       "      <td>0.493276</td>\n",
       "      <td>0.484316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>1.013410</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.586343</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>0.551096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>1.055514</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.678021</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.623363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>1.092970</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.741302</td>\n",
       "      <td>0.644685</td>\n",
       "      <td>0.668079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>1.134158</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.740934</td>\n",
       "      <td>0.654399</td>\n",
       "      <td>0.674070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>1.139983</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781975</td>\n",
       "      <td>0.701511</td>\n",
       "      <td>0.721271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>1.201372</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.775985</td>\n",
       "      <td>0.705008</td>\n",
       "      <td>0.721015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>1.247103</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.772829</td>\n",
       "      <td>0.709185</td>\n",
       "      <td>0.717097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:27:03,187] Trial 56 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 57 with params: {'learning_rate': 9.590754651274163e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.382500</td>\n",
       "      <td>1.512476</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.323292</td>\n",
       "      <td>0.330486</td>\n",
       "      <td>0.310392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>1.094639</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.436752</td>\n",
       "      <td>0.455440</td>\n",
       "      <td>0.437803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480600</td>\n",
       "      <td>1.010205</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.469252</td>\n",
       "      <td>0.490394</td>\n",
       "      <td>0.475104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.600113</td>\n",
       "      <td>0.539145</td>\n",
       "      <td>0.550020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>1.016086</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.653506</td>\n",
       "      <td>0.597281</td>\n",
       "      <td>0.608142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>1.061665</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.716699</td>\n",
       "      <td>0.611138</td>\n",
       "      <td>0.639142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>1.083814</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.765279</td>\n",
       "      <td>0.673369</td>\n",
       "      <td>0.693448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>1.114638</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.803819</td>\n",
       "      <td>0.713752</td>\n",
       "      <td>0.734574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>1.166025</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.785687</td>\n",
       "      <td>0.707156</td>\n",
       "      <td>0.727865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>1.211129</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.801178</td>\n",
       "      <td>0.708220</td>\n",
       "      <td>0.730732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>1.213730</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790352</td>\n",
       "      <td>0.700257</td>\n",
       "      <td>0.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>1.235553</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.789930</td>\n",
       "      <td>0.723878</td>\n",
       "      <td>0.735058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>1.270850</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.776758</td>\n",
       "      <td>0.715340</td>\n",
       "      <td>0.727329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1.277144</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.810462</td>\n",
       "      <td>0.721530</td>\n",
       "      <td>0.745644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>1.364704</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.771051</td>\n",
       "      <td>0.700434</td>\n",
       "      <td>0.714682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.375862</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785450</td>\n",
       "      <td>0.726740</td>\n",
       "      <td>0.740182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.378671</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.800602</td>\n",
       "      <td>0.711059</td>\n",
       "      <td>0.733749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.417058</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.780203</td>\n",
       "      <td>0.720197</td>\n",
       "      <td>0.729538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.391289</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.784019</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.729930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.409551</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.780542</td>\n",
       "      <td>0.717063</td>\n",
       "      <td>0.729533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.428485</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.772364</td>\n",
       "      <td>0.733019</td>\n",
       "      <td>0.735040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.452319</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.787575</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.733897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.471171</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.773681</td>\n",
       "      <td>0.733641</td>\n",
       "      <td>0.737829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.480196</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794279</td>\n",
       "      <td>0.738760</td>\n",
       "      <td>0.747767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.474446</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.797423</td>\n",
       "      <td>0.727217</td>\n",
       "      <td>0.742293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.487088</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783924</td>\n",
       "      <td>0.731288</td>\n",
       "      <td>0.739469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.495110</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.773594</td>\n",
       "      <td>0.733090</td>\n",
       "      <td>0.738043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.488447</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789086</td>\n",
       "      <td>0.734848</td>\n",
       "      <td>0.743863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.495641</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.776279</td>\n",
       "      <td>0.731328</td>\n",
       "      <td>0.736862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.496751</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777360</td>\n",
       "      <td>0.733328</td>\n",
       "      <td>0.739349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Fri Jan 10 23:13:59 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-16 03:42:44,152] Trial 57 finished with value: 0.739348905242017 and parameters: {'learning_rate': 9.590754651274163e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 33}. Best is trial 40 with value: 0.7430654138816002.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 58 with params: {'learning_rate': 0.00012545757026986983, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 24}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>1.307781</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.389932</td>\n",
       "      <td>0.391141</td>\n",
       "      <td>0.369108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>1.028921</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.489229</td>\n",
       "      <td>0.483762</td>\n",
       "      <td>0.478377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.992548</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.601014</td>\n",
       "      <td>0.566347</td>\n",
       "      <td>0.566910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>1.015873</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.673979</td>\n",
       "      <td>0.613545</td>\n",
       "      <td>0.628138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>1.055042</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.713954</td>\n",
       "      <td>0.638747</td>\n",
       "      <td>0.654591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>1.119583</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.788189</td>\n",
       "      <td>0.678667</td>\n",
       "      <td>0.706923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>1.176871</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.819221</td>\n",
       "      <td>0.696357</td>\n",
       "      <td>0.727095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.200353</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.798605</td>\n",
       "      <td>0.716193</td>\n",
       "      <td>0.734584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.250070</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.774151</td>\n",
       "      <td>0.711743</td>\n",
       "      <td>0.726133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.322959</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.802643</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.734648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>1.306732</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.794463</td>\n",
       "      <td>0.700050</td>\n",
       "      <td>0.725762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>1.337630</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.774016</td>\n",
       "      <td>0.724006</td>\n",
       "      <td>0.726072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.366239</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.767260</td>\n",
       "      <td>0.721233</td>\n",
       "      <td>0.726329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.369410</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.804162</td>\n",
       "      <td>0.731040</td>\n",
       "      <td>0.746887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.482330</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.781884</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.732744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.504267</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.806148</td>\n",
       "      <td>0.717154</td>\n",
       "      <td>0.740863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.475864</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.807445</td>\n",
       "      <td>0.732839</td>\n",
       "      <td>0.750892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.511202</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.792092</td>\n",
       "      <td>0.729192</td>\n",
       "      <td>0.741377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.463170</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.790335</td>\n",
       "      <td>0.744227</td>\n",
       "      <td>0.752686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.486086</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.800413</td>\n",
       "      <td>0.733515</td>\n",
       "      <td>0.748304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.548344</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.775154</td>\n",
       "      <td>0.732739</td>\n",
       "      <td>0.741098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.527588</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797324</td>\n",
       "      <td>0.733063</td>\n",
       "      <td>0.746821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.566550</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.781046</td>\n",
       "      <td>0.731550</td>\n",
       "      <td>0.742759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.547455</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.796609</td>\n",
       "      <td>0.732122</td>\n",
       "      <td>0.748112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.567505</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.807557</td>\n",
       "      <td>0.731944</td>\n",
       "      <td>0.750050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.592114</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.788713</td>\n",
       "      <td>0.739151</td>\n",
       "      <td>0.747841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.593779</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791649</td>\n",
       "      <td>0.732024</td>\n",
       "      <td>0.746242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.587544</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.804268</td>\n",
       "      <td>0.738475</td>\n",
       "      <td>0.752599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.599363</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.798518</td>\n",
       "      <td>0.730393</td>\n",
       "      <td>0.746484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.600633</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.784212</td>\n",
       "      <td>0.726692</td>\n",
       "      <td>0.740929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 03:58:07,636] Trial 58 finished with value: 0.7409288034163989 and parameters: {'learning_rate': 0.00012545757026986983, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 24}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 59 with params: {'learning_rate': 5.813441783758918e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:37 < 13:05, 16.70 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.734500</td>\n",
       "      <td>1.952807</td>\n",
       "      <td>0.618698</td>\n",
       "      <td>0.286308</td>\n",
       "      <td>0.249673</td>\n",
       "      <td>0.232647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.390400</td>\n",
       "      <td>1.316074</td>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.375642</td>\n",
       "      <td>0.385096</td>\n",
       "      <td>0.366928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.840800</td>\n",
       "      <td>1.123234</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.447800</td>\n",
       "      <td>0.447194</td>\n",
       "      <td>0.431255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>1.040895</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.482938</td>\n",
       "      <td>0.476005</td>\n",
       "      <td>0.466822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>1.010764</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.494817</td>\n",
       "      <td>0.500349</td>\n",
       "      <td>0.487489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:00:46,028] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 with params: {'learning_rate': 0.0003514535590125288, 'weight_decay': 0.01, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:13 < 05:06, 17.13 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.275800</td>\n",
       "      <td>1.017260</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.505983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>1.067336</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.708734</td>\n",
       "      <td>0.657381</td>\n",
       "      <td>0.663482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>1.220345</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.760871</td>\n",
       "      <td>0.728069</td>\n",
       "      <td>0.728407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>1.235497</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.805167</td>\n",
       "      <td>0.712453</td>\n",
       "      <td>0.738390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>1.311132</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.812455</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.724361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.332148</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>0.703931</td>\n",
       "      <td>0.734746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>1.423090</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.808447</td>\n",
       "      <td>0.719886</td>\n",
       "      <td>0.745898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>1.475518</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.785587</td>\n",
       "      <td>0.707751</td>\n",
       "      <td>0.724384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>1.546923</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.771791</td>\n",
       "      <td>0.693835</td>\n",
       "      <td>0.713724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.530536</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.764877</td>\n",
       "      <td>0.713191</td>\n",
       "      <td>0.724103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.631098</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791555</td>\n",
       "      <td>0.696811</td>\n",
       "      <td>0.723048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.637598</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.771843</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.712985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.742046</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.777201</td>\n",
       "      <td>0.699749</td>\n",
       "      <td>0.719048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.676675</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.762555</td>\n",
       "      <td>0.698027</td>\n",
       "      <td>0.714151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.853203</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.774079</td>\n",
       "      <td>0.705853</td>\n",
       "      <td>0.720737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.886253</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.774243</td>\n",
       "      <td>0.695332</td>\n",
       "      <td>0.713640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.858835</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.776333</td>\n",
       "      <td>0.696037</td>\n",
       "      <td>0.718831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.936488</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.751232</td>\n",
       "      <td>0.688984</td>\n",
       "      <td>0.700818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.892073</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.692372</td>\n",
       "      <td>0.713080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.929249</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.781767</td>\n",
       "      <td>0.691136</td>\n",
       "      <td>0.714085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:11:00,197] Trial 60 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 61 with params: {'learning_rate': 0.00010215436306208406, 'weight_decay': 0.002, 'adam_beta1': 0.98, 'warmup_steps': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:39 < 05:19, 16.42 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.355900</td>\n",
       "      <td>1.522834</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.332046</td>\n",
       "      <td>0.326018</td>\n",
       "      <td>0.306020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.902300</td>\n",
       "      <td>1.090860</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.458931</td>\n",
       "      <td>0.463525</td>\n",
       "      <td>0.448425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>1.002166</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.486820</td>\n",
       "      <td>0.492377</td>\n",
       "      <td>0.482296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.995731</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.641520</td>\n",
       "      <td>0.575083</td>\n",
       "      <td>0.590850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>1.029016</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.616926</td>\n",
       "      <td>0.631492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>1.060580</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.760674</td>\n",
       "      <td>0.663166</td>\n",
       "      <td>0.687641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>1.108843</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.789437</td>\n",
       "      <td>0.705987</td>\n",
       "      <td>0.726028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>1.130782</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.806327</td>\n",
       "      <td>0.701847</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>1.191602</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.778942</td>\n",
       "      <td>0.698264</td>\n",
       "      <td>0.721858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>1.235317</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.793657</td>\n",
       "      <td>0.704875</td>\n",
       "      <td>0.727261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>1.247147</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.801097</td>\n",
       "      <td>0.699282</td>\n",
       "      <td>0.723297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>1.292052</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>0.714965</td>\n",
       "      <td>0.725537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>1.319762</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783224</td>\n",
       "      <td>0.706269</td>\n",
       "      <td>0.725140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>1.319003</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.807634</td>\n",
       "      <td>0.717036</td>\n",
       "      <td>0.744031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.380903</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.776087</td>\n",
       "      <td>0.715871</td>\n",
       "      <td>0.727275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.416790</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.791373</td>\n",
       "      <td>0.723597</td>\n",
       "      <td>0.739423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.425214</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.804748</td>\n",
       "      <td>0.719629</td>\n",
       "      <td>0.741219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.446592</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.787311</td>\n",
       "      <td>0.733206</td>\n",
       "      <td>0.742260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.427425</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.792346</td>\n",
       "      <td>0.716104</td>\n",
       "      <td>0.733282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.460718</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.795132</td>\n",
       "      <td>0.710218</td>\n",
       "      <td>0.732238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:21:40,792] Trial 61 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 62 with params: {'learning_rate': 0.00038950536460528504, 'weight_decay': 0.007, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:13 < 10:26, 16.76 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.240200</td>\n",
       "      <td>1.000669</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.561803</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.540996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167900</td>\n",
       "      <td>1.081595</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.700677</td>\n",
       "      <td>0.646374</td>\n",
       "      <td>0.658596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>1.223456</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.759193</td>\n",
       "      <td>0.723703</td>\n",
       "      <td>0.724333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>1.297212</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.806301</td>\n",
       "      <td>0.681514</td>\n",
       "      <td>0.716855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.345515</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.777575</td>\n",
       "      <td>0.698443</td>\n",
       "      <td>0.711889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>1.393977</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.809236</td>\n",
       "      <td>0.704254</td>\n",
       "      <td>0.732556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>1.490005</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.778507</td>\n",
       "      <td>0.689332</td>\n",
       "      <td>0.715536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.485947</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.758214</td>\n",
       "      <td>0.707273</td>\n",
       "      <td>0.713504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.588515</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>0.704552</td>\n",
       "      <td>0.727244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.633088</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.756414</td>\n",
       "      <td>0.705636</td>\n",
       "      <td>0.711919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:26:54,919] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 63 with params: {'learning_rate': 4.467180133544001e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:24, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.969500</td>\n",
       "      <td>2.279404</td>\n",
       "      <td>0.567369</td>\n",
       "      <td>0.233364</td>\n",
       "      <td>0.201578</td>\n",
       "      <td>0.186840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.733800</td>\n",
       "      <td>1.537291</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.343875</td>\n",
       "      <td>0.329594</td>\n",
       "      <td>0.309297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.113600</td>\n",
       "      <td>1.246871</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.402384</td>\n",
       "      <td>0.400637</td>\n",
       "      <td>0.384573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>1.116431</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.449845</td>\n",
       "      <td>0.448130</td>\n",
       "      <td>0.435160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.470610</td>\n",
       "      <td>0.466426</td>\n",
       "      <td>0.452637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>1.024903</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.478707</td>\n",
       "      <td>0.486018</td>\n",
       "      <td>0.475374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>1.003590</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.506803</td>\n",
       "      <td>0.505783</td>\n",
       "      <td>0.496448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>1.003328</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.588499</td>\n",
       "      <td>0.533990</td>\n",
       "      <td>0.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>1.008886</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.619547</td>\n",
       "      <td>0.549734</td>\n",
       "      <td>0.561110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>1.022890</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.647108</td>\n",
       "      <td>0.587179</td>\n",
       "      <td>0.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>1.019733</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.682398</td>\n",
       "      <td>0.608976</td>\n",
       "      <td>0.626490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>1.039506</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.687583</td>\n",
       "      <td>0.618128</td>\n",
       "      <td>0.631874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>1.042509</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.700393</td>\n",
       "      <td>0.626102</td>\n",
       "      <td>0.644239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>1.057311</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.710476</td>\n",
       "      <td>0.634359</td>\n",
       "      <td>0.653557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>1.086220</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.703351</td>\n",
       "      <td>0.624207</td>\n",
       "      <td>0.643757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>1.100944</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.702439</td>\n",
       "      <td>0.628797</td>\n",
       "      <td>0.645912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>1.104753</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.769887</td>\n",
       "      <td>0.677092</td>\n",
       "      <td>0.701160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>1.110707</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.777308</td>\n",
       "      <td>0.684968</td>\n",
       "      <td>0.709308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>1.106277</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.780759</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.713410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>1.129833</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791345</td>\n",
       "      <td>0.698020</td>\n",
       "      <td>0.725078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>1.135284</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794037</td>\n",
       "      <td>0.703272</td>\n",
       "      <td>0.725514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>1.152477</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.791254</td>\n",
       "      <td>0.697688</td>\n",
       "      <td>0.724448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>1.155470</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.771666</td>\n",
       "      <td>0.693335</td>\n",
       "      <td>0.712267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>1.158758</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.780321</td>\n",
       "      <td>0.698252</td>\n",
       "      <td>0.716499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>1.179237</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.795419</td>\n",
       "      <td>0.703317</td>\n",
       "      <td>0.726748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>1.177776</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.795284</td>\n",
       "      <td>0.702885</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>1.179513</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790100</td>\n",
       "      <td>0.715077</td>\n",
       "      <td>0.731486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>1.183312</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.715005</td>\n",
       "      <td>0.734121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>1.183566</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.790926</td>\n",
       "      <td>0.712836</td>\n",
       "      <td>0.730514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>1.183784</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791202</td>\n",
       "      <td>0.712733</td>\n",
       "      <td>0.730470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:42:20,977] Trial 63 finished with value: 0.7304699924097497 and parameters: {'learning_rate': 4.467180133544001e-05, 'weight_decay': 0.01, 'adam_beta1': 0.97, 'warmup_steps': 25}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64 with params: {'learning_rate': 1.6488779238415127e-06, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:38 < 05:19, 16.45 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.820700</td>\n",
       "      <td>3.750179</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.032173</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>0.013540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.699100</td>\n",
       "      <td>3.649930</td>\n",
       "      <td>0.212649</td>\n",
       "      <td>0.018428</td>\n",
       "      <td>0.030945</td>\n",
       "      <td>0.016257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.606900</td>\n",
       "      <td>3.563762</td>\n",
       "      <td>0.209899</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>0.029827</td>\n",
       "      <td>0.017776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.525700</td>\n",
       "      <td>3.484789</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.051315</td>\n",
       "      <td>0.037350</td>\n",
       "      <td>0.028418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.449100</td>\n",
       "      <td>3.411254</td>\n",
       "      <td>0.293309</td>\n",
       "      <td>0.045591</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.043483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.378600</td>\n",
       "      <td>3.342406</td>\n",
       "      <td>0.328139</td>\n",
       "      <td>0.042515</td>\n",
       "      <td>0.066030</td>\n",
       "      <td>0.048516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.311700</td>\n",
       "      <td>3.280226</td>\n",
       "      <td>0.349221</td>\n",
       "      <td>0.060623</td>\n",
       "      <td>0.072978</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.250600</td>\n",
       "      <td>3.223618</td>\n",
       "      <td>0.352887</td>\n",
       "      <td>0.074812</td>\n",
       "      <td>0.074986</td>\n",
       "      <td>0.052203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.195000</td>\n",
       "      <td>3.171898</td>\n",
       "      <td>0.359303</td>\n",
       "      <td>0.073678</td>\n",
       "      <td>0.077051</td>\n",
       "      <td>0.054693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.142400</td>\n",
       "      <td>3.124766</td>\n",
       "      <td>0.366636</td>\n",
       "      <td>0.073143</td>\n",
       "      <td>0.079376</td>\n",
       "      <td>0.058125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.094800</td>\n",
       "      <td>3.081558</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>0.099044</td>\n",
       "      <td>0.085723</td>\n",
       "      <td>0.067883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.051400</td>\n",
       "      <td>3.041941</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.090526</td>\n",
       "      <td>0.073534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.011000</td>\n",
       "      <td>3.004979</td>\n",
       "      <td>0.413382</td>\n",
       "      <td>0.098472</td>\n",
       "      <td>0.094040</td>\n",
       "      <td>0.077513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.973500</td>\n",
       "      <td>2.971325</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.116047</td>\n",
       "      <td>0.100143</td>\n",
       "      <td>0.085267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.937700</td>\n",
       "      <td>2.940209</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>0.104866</td>\n",
       "      <td>0.090391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.907000</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>0.434464</td>\n",
       "      <td>0.111463</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.092176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.878900</td>\n",
       "      <td>2.885321</td>\n",
       "      <td>0.437214</td>\n",
       "      <td>0.109759</td>\n",
       "      <td>0.109563</td>\n",
       "      <td>0.094412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.852200</td>\n",
       "      <td>2.861212</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.109429</td>\n",
       "      <td>0.111647</td>\n",
       "      <td>0.096348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>2.839518</td>\n",
       "      <td>0.447296</td>\n",
       "      <td>0.108973</td>\n",
       "      <td>0.115629</td>\n",
       "      <td>0.099950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.805600</td>\n",
       "      <td>2.819620</td>\n",
       "      <td>0.450046</td>\n",
       "      <td>0.108937</td>\n",
       "      <td>0.117259</td>\n",
       "      <td>0.101167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 04:53:00,286] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 65 with params: {'learning_rate': 0.00025146973771310706, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:45, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.816600</td>\n",
       "      <td>1.190332</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.421090</td>\n",
       "      <td>0.414161</td>\n",
       "      <td>0.399957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>1.113561</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.640160</td>\n",
       "      <td>0.589320</td>\n",
       "      <td>0.602031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>1.152823</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.688272</td>\n",
       "      <td>0.669798</td>\n",
       "      <td>0.667025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.255220</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.747718</td>\n",
       "      <td>0.662337</td>\n",
       "      <td>0.681050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>1.297097</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.785601</td>\n",
       "      <td>0.708973</td>\n",
       "      <td>0.730993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>1.366392</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.771318</td>\n",
       "      <td>0.674151</td>\n",
       "      <td>0.702531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>1.395397</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.774291</td>\n",
       "      <td>0.701931</td>\n",
       "      <td>0.722656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.468507</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.756104</td>\n",
       "      <td>0.663187</td>\n",
       "      <td>0.690876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>1.529112</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.722679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.581938</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.764986</td>\n",
       "      <td>0.718124</td>\n",
       "      <td>0.724148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.616978</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.775855</td>\n",
       "      <td>0.708191</td>\n",
       "      <td>0.723274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.582597</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.769047</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.731704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.660552</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.764545</td>\n",
       "      <td>0.725644</td>\n",
       "      <td>0.733234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.685004</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.753839</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>0.719648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.719734</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.791852</td>\n",
       "      <td>0.712718</td>\n",
       "      <td>0.733493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.774013</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.773681</td>\n",
       "      <td>0.719072</td>\n",
       "      <td>0.731201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.754867</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.785910</td>\n",
       "      <td>0.728457</td>\n",
       "      <td>0.742587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.842045</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.761770</td>\n",
       "      <td>0.703126</td>\n",
       "      <td>0.716185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.807388</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.764290</td>\n",
       "      <td>0.715366</td>\n",
       "      <td>0.724870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.828045</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.773485</td>\n",
       "      <td>0.717445</td>\n",
       "      <td>0.732113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.839454</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.778447</td>\n",
       "      <td>0.711465</td>\n",
       "      <td>0.729263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.872193</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.762797</td>\n",
       "      <td>0.718099</td>\n",
       "      <td>0.726487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.865010</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.776509</td>\n",
       "      <td>0.717987</td>\n",
       "      <td>0.733383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.872490</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.774964</td>\n",
       "      <td>0.707242</td>\n",
       "      <td>0.724682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.869180</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.777006</td>\n",
       "      <td>0.707217</td>\n",
       "      <td>0.725089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.859444</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.746052</td>\n",
       "      <td>0.719451</td>\n",
       "      <td>0.720477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.883658</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.773056</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>0.728176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.870608</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.716766</td>\n",
       "      <td>0.730196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.883384</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.773834</td>\n",
       "      <td>0.720769</td>\n",
       "      <td>0.732754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.877966</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.772357</td>\n",
       "      <td>0.720315</td>\n",
       "      <td>0.731775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 05:08:47,693] Trial 65 finished with value: 0.7317753029315681 and parameters: {'learning_rate': 0.00025146973771310706, 'weight_decay': 0.004, 'adam_beta1': 0.99, 'warmup_steps': 21}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 66 with params: {'learning_rate': 2.0788074719357774e-05, 'weight_decay': 0.007, 'adam_beta1': 0.98, 'warmup_steps': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:05 < 05:02, 17.34 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.364500</td>\n",
       "      <td>2.949123</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.097129</td>\n",
       "      <td>0.097978</td>\n",
       "      <td>0.081741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.570800</td>\n",
       "      <td>2.316668</td>\n",
       "      <td>0.555454</td>\n",
       "      <td>0.214246</td>\n",
       "      <td>0.189350</td>\n",
       "      <td>0.175662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.004500</td>\n",
       "      <td>1.893938</td>\n",
       "      <td>0.627864</td>\n",
       "      <td>0.283170</td>\n",
       "      <td>0.257123</td>\n",
       "      <td>0.237146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.605400</td>\n",
       "      <td>1.607852</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.334042</td>\n",
       "      <td>0.315716</td>\n",
       "      <td>0.298838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.313300</td>\n",
       "      <td>1.424544</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.363643</td>\n",
       "      <td>0.365747</td>\n",
       "      <td>0.347313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.104100</td>\n",
       "      <td>1.298201</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.405228</td>\n",
       "      <td>0.397671</td>\n",
       "      <td>0.385114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>1.215180</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.438890</td>\n",
       "      <td>0.416954</td>\n",
       "      <td>0.401448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>1.157691</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.427553</td>\n",
       "      <td>0.431035</td>\n",
       "      <td>0.412936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.727900</td>\n",
       "      <td>1.116269</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.465444</td>\n",
       "      <td>0.451358</td>\n",
       "      <td>0.442270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>1.083020</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.461560</td>\n",
       "      <td>0.455691</td>\n",
       "      <td>0.444654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>1.059179</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.488162</td>\n",
       "      <td>0.476738</td>\n",
       "      <td>0.463156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.521300</td>\n",
       "      <td>1.043964</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.499591</td>\n",
       "      <td>0.481341</td>\n",
       "      <td>0.471340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>1.028113</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.503302</td>\n",
       "      <td>0.481742</td>\n",
       "      <td>0.476964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>1.017693</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.496837</td>\n",
       "      <td>0.486093</td>\n",
       "      <td>0.479693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>1.009896</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.504643</td>\n",
       "      <td>0.494225</td>\n",
       "      <td>0.489988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>1.007244</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.514641</td>\n",
       "      <td>0.493397</td>\n",
       "      <td>0.491236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>1.005633</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.511312</td>\n",
       "      <td>0.495945</td>\n",
       "      <td>0.491533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>1.001869</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.532240</td>\n",
       "      <td>0.498233</td>\n",
       "      <td>0.495986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.997022</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.548179</td>\n",
       "      <td>0.514296</td>\n",
       "      <td>0.515835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>1.002552</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.545179</td>\n",
       "      <td>0.518761</td>\n",
       "      <td>0.518594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 05:18:54,242] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 67 with params: {'learning_rate': 0.00019438727075479905, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:50, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.773300</td>\n",
       "      <td>1.124635</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.437195</td>\n",
       "      <td>0.449210</td>\n",
       "      <td>0.429570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>1.012292</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.620995</td>\n",
       "      <td>0.566325</td>\n",
       "      <td>0.571393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>1.045737</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.697952</td>\n",
       "      <td>0.653710</td>\n",
       "      <td>0.656127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>1.102102</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.776556</td>\n",
       "      <td>0.692618</td>\n",
       "      <td>0.714026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>1.168948</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.813927</td>\n",
       "      <td>0.720394</td>\n",
       "      <td>0.748517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>1.208174</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790956</td>\n",
       "      <td>0.687378</td>\n",
       "      <td>0.713782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.279282</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.794433</td>\n",
       "      <td>0.691191</td>\n",
       "      <td>0.718242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.289861</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.791664</td>\n",
       "      <td>0.718285</td>\n",
       "      <td>0.735551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>1.334578</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.819297</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>0.750593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.445414</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.782961</td>\n",
       "      <td>0.733043</td>\n",
       "      <td>0.741290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.484239</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.801927</td>\n",
       "      <td>0.701036</td>\n",
       "      <td>0.726626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.465705</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.727540</td>\n",
       "      <td>0.738474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.552213</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.768181</td>\n",
       "      <td>0.709023</td>\n",
       "      <td>0.723630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.508632</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.728450</td>\n",
       "      <td>0.744002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.499360</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.800926</td>\n",
       "      <td>0.733101</td>\n",
       "      <td>0.747680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.554167</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.771301</td>\n",
       "      <td>0.707567</td>\n",
       "      <td>0.718610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.598066</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.807369</td>\n",
       "      <td>0.722444</td>\n",
       "      <td>0.744350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.578532</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.719145</td>\n",
       "      <td>0.727541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.600160</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786538</td>\n",
       "      <td>0.735070</td>\n",
       "      <td>0.743464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.557458</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.771046</td>\n",
       "      <td>0.735320</td>\n",
       "      <td>0.739735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.651307</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794166</td>\n",
       "      <td>0.729341</td>\n",
       "      <td>0.744626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.655659</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799231</td>\n",
       "      <td>0.729211</td>\n",
       "      <td>0.748294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.667468</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.803921</td>\n",
       "      <td>0.734980</td>\n",
       "      <td>0.752584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.705830</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.817449</td>\n",
       "      <td>0.728123</td>\n",
       "      <td>0.753236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.696524</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797930</td>\n",
       "      <td>0.729400</td>\n",
       "      <td>0.745526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.722082</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.798180</td>\n",
       "      <td>0.728279</td>\n",
       "      <td>0.744515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.704441</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788622</td>\n",
       "      <td>0.727317</td>\n",
       "      <td>0.742196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.702685</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791770</td>\n",
       "      <td>0.730750</td>\n",
       "      <td>0.745131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.707104</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.796490</td>\n",
       "      <td>0.732182</td>\n",
       "      <td>0.746772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.717189</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.789393</td>\n",
       "      <td>0.727003</td>\n",
       "      <td>0.742315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 05:34:46,872] Trial 67 finished with value: 0.7423151367486238 and parameters: {'learning_rate': 0.00019438727075479905, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 22}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 68 with params: {'learning_rate': 0.00016632116544549325, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.902300</td>\n",
       "      <td>1.180301</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.437083</td>\n",
       "      <td>0.431716</td>\n",
       "      <td>0.415688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>1.005094</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.576632</td>\n",
       "      <td>0.529977</td>\n",
       "      <td>0.528594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>1.010256</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.682815</td>\n",
       "      <td>0.642254</td>\n",
       "      <td>0.646435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>1.092733</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.738932</td>\n",
       "      <td>0.642593</td>\n",
       "      <td>0.668849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>1.107854</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.801879</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>0.729709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>1.171889</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.797289</td>\n",
       "      <td>0.691396</td>\n",
       "      <td>0.721611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>1.258666</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.799093</td>\n",
       "      <td>0.693042</td>\n",
       "      <td>0.721427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>1.284186</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.801272</td>\n",
       "      <td>0.710939</td>\n",
       "      <td>0.732003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>1.306130</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.789232</td>\n",
       "      <td>0.718479</td>\n",
       "      <td>0.734438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>1.378989</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.793744</td>\n",
       "      <td>0.711468</td>\n",
       "      <td>0.729949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>1.403257</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.712604</td>\n",
       "      <td>0.734794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.441522</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.790202</td>\n",
       "      <td>0.731908</td>\n",
       "      <td>0.739872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.457370</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.812753</td>\n",
       "      <td>0.710279</td>\n",
       "      <td>0.739277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.486589</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796035</td>\n",
       "      <td>0.719979</td>\n",
       "      <td>0.741829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.528415</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800608</td>\n",
       "      <td>0.707319</td>\n",
       "      <td>0.731017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.552021</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.806023</td>\n",
       "      <td>0.698037</td>\n",
       "      <td>0.727930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.605439</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.808881</td>\n",
       "      <td>0.718235</td>\n",
       "      <td>0.742081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.589899</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.790672</td>\n",
       "      <td>0.716045</td>\n",
       "      <td>0.733342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.562774</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791337</td>\n",
       "      <td>0.727883</td>\n",
       "      <td>0.739972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.573486</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.805154</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>0.751201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.597583</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.797823</td>\n",
       "      <td>0.726293</td>\n",
       "      <td>0.740578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.582839</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.805021</td>\n",
       "      <td>0.729148</td>\n",
       "      <td>0.749011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.648824</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.793233</td>\n",
       "      <td>0.728127</td>\n",
       "      <td>0.743082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.667341</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.799260</td>\n",
       "      <td>0.726410</td>\n",
       "      <td>0.744358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.664459</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.805543</td>\n",
       "      <td>0.731775</td>\n",
       "      <td>0.750891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.672408</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.797612</td>\n",
       "      <td>0.726648</td>\n",
       "      <td>0.744646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.674951</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.797012</td>\n",
       "      <td>0.719058</td>\n",
       "      <td>0.738766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.665617</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.795424</td>\n",
       "      <td>0.730612</td>\n",
       "      <td>0.746032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.694658</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789507</td>\n",
       "      <td>0.713929</td>\n",
       "      <td>0.732347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.697232</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790259</td>\n",
       "      <td>0.712727</td>\n",
       "      <td>0.732368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 05:50:32,913] Trial 68 finished with value: 0.732368432206416 and parameters: {'learning_rate': 0.00016632116544549325, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.96, 'warmup_steps': 24}. Best is trial 40 with value: 0.7430654138816002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 69 with params: {'learning_rate': 0.0001139981084024823, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'warmup_steps': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:25, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.076600</td>\n",
       "      <td>1.324254</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.388081</td>\n",
       "      <td>0.390044</td>\n",
       "      <td>0.368354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>1.044935</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.480717</td>\n",
       "      <td>0.481779</td>\n",
       "      <td>0.472853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.989576</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.607262</td>\n",
       "      <td>0.553545</td>\n",
       "      <td>0.557178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>1.033192</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.644782</td>\n",
       "      <td>0.577951</td>\n",
       "      <td>0.595918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>1.037899</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.685478</td>\n",
       "      <td>0.620476</td>\n",
       "      <td>0.635824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>1.105759</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.745716</td>\n",
       "      <td>0.654588</td>\n",
       "      <td>0.677499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>1.156808</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.802412</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.726708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>1.174409</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.801585</td>\n",
       "      <td>0.719676</td>\n",
       "      <td>0.737780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>1.213120</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.785194</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.724045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.288796</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.803360</td>\n",
       "      <td>0.707857</td>\n",
       "      <td>0.729726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>1.272889</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.758128</td>\n",
       "      <td>0.703817</td>\n",
       "      <td>0.713530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>1.303739</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.780682</td>\n",
       "      <td>0.735687</td>\n",
       "      <td>0.740273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.335280</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.778376</td>\n",
       "      <td>0.728419</td>\n",
       "      <td>0.734517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.348165</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.802882</td>\n",
       "      <td>0.728115</td>\n",
       "      <td>0.747087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.414305</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791239</td>\n",
       "      <td>0.732856</td>\n",
       "      <td>0.743611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.417954</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.721373</td>\n",
       "      <td>0.741216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.410210</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.807858</td>\n",
       "      <td>0.728137</td>\n",
       "      <td>0.745165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.467070</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780988</td>\n",
       "      <td>0.735833</td>\n",
       "      <td>0.739296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.413490</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.795036</td>\n",
       "      <td>0.740251</td>\n",
       "      <td>0.750838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.455774</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.730245</td>\n",
       "      <td>0.745010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.470948</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.780144</td>\n",
       "      <td>0.733848</td>\n",
       "      <td>0.741995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.471781</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.783671</td>\n",
       "      <td>0.731122</td>\n",
       "      <td>0.740032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.503208</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.784876</td>\n",
       "      <td>0.736871</td>\n",
       "      <td>0.745076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.503909</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.803206</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.756703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.526842</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.741850</td>\n",
       "      <td>0.760497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.530500</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798300</td>\n",
       "      <td>0.739863</td>\n",
       "      <td>0.751372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.535741</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.738309</td>\n",
       "      <td>0.751165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.530875</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.799394</td>\n",
       "      <td>0.740568</td>\n",
       "      <td>0.752229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.537686</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.735080</td>\n",
       "      <td>0.748291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.539928</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797940</td>\n",
       "      <td>0.733483</td>\n",
       "      <td>0.747360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:06:00,671] Trial 69 finished with value: 0.747359569286359 and parameters: {'learning_rate': 0.0001139981084024823, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'warmup_steps': 5}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 70 with params: {'learning_rate': 2.0941712066636755e-05, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:06 < 10:13, 17.11 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.300500</td>\n",
       "      <td>2.846300</td>\n",
       "      <td>0.452796</td>\n",
       "      <td>0.107515</td>\n",
       "      <td>0.116377</td>\n",
       "      <td>0.098526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.467600</td>\n",
       "      <td>2.219648</td>\n",
       "      <td>0.582951</td>\n",
       "      <td>0.245155</td>\n",
       "      <td>0.211588</td>\n",
       "      <td>0.196084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.912700</td>\n",
       "      <td>1.818554</td>\n",
       "      <td>0.652612</td>\n",
       "      <td>0.318552</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.257873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.532700</td>\n",
       "      <td>1.556419</td>\n",
       "      <td>0.702108</td>\n",
       "      <td>0.334637</td>\n",
       "      <td>0.326553</td>\n",
       "      <td>0.308987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.258100</td>\n",
       "      <td>1.386071</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.355464</td>\n",
       "      <td>0.359359</td>\n",
       "      <td>0.338169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.062100</td>\n",
       "      <td>1.271247</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>0.404744</td>\n",
       "      <td>0.382574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>1.192128</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.410574</td>\n",
       "      <td>0.426497</td>\n",
       "      <td>0.406327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.796600</td>\n",
       "      <td>1.138462</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.448567</td>\n",
       "      <td>0.445401</td>\n",
       "      <td>0.427264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>1.104691</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.465281</td>\n",
       "      <td>0.453196</td>\n",
       "      <td>0.442589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.626700</td>\n",
       "      <td>1.077439</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.515853</td>\n",
       "      <td>0.472685</td>\n",
       "      <td>0.466401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:11:08,536] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 71 with params: {'learning_rate': 0.00021636896534386668, 'weight_decay': 0.007, 'adam_beta1': 0.96, 'warmup_steps': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:57, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.689500</td>\n",
       "      <td>1.095908</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.432550</td>\n",
       "      <td>0.456091</td>\n",
       "      <td>0.432769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>1.029000</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.646253</td>\n",
       "      <td>0.590570</td>\n",
       "      <td>0.597135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>1.077280</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.726149</td>\n",
       "      <td>0.668338</td>\n",
       "      <td>0.676337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.139346</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.806316</td>\n",
       "      <td>0.726891</td>\n",
       "      <td>0.748417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>1.183948</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.799810</td>\n",
       "      <td>0.731178</td>\n",
       "      <td>0.750132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>1.243808</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.803640</td>\n",
       "      <td>0.691243</td>\n",
       "      <td>0.725345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.321829</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.793485</td>\n",
       "      <td>0.693402</td>\n",
       "      <td>0.717246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.340225</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.795265</td>\n",
       "      <td>0.709057</td>\n",
       "      <td>0.728572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.396519</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.796843</td>\n",
       "      <td>0.710560</td>\n",
       "      <td>0.734038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.452866</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.773540</td>\n",
       "      <td>0.741467</td>\n",
       "      <td>0.742495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.504316</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.793791</td>\n",
       "      <td>0.696304</td>\n",
       "      <td>0.721567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.489128</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.814430</td>\n",
       "      <td>0.735858</td>\n",
       "      <td>0.754259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.599692</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.780464</td>\n",
       "      <td>0.702030</td>\n",
       "      <td>0.723928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.532739</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.790273</td>\n",
       "      <td>0.723033</td>\n",
       "      <td>0.737402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.567422</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.779721</td>\n",
       "      <td>0.702826</td>\n",
       "      <td>0.717915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.672556</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.802155</td>\n",
       "      <td>0.676828</td>\n",
       "      <td>0.712421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.612748</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.800460</td>\n",
       "      <td>0.725616</td>\n",
       "      <td>0.745764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.610497</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.767107</td>\n",
       "      <td>0.720561</td>\n",
       "      <td>0.725871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.665628</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786246</td>\n",
       "      <td>0.708782</td>\n",
       "      <td>0.731185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.624197</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.787086</td>\n",
       "      <td>0.713374</td>\n",
       "      <td>0.732216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.705199</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.788956</td>\n",
       "      <td>0.707679</td>\n",
       "      <td>0.728807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.660569</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.796696</td>\n",
       "      <td>0.710181</td>\n",
       "      <td>0.732489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.710798</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778474</td>\n",
       "      <td>0.730914</td>\n",
       "      <td>0.738825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.719988</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.786362</td>\n",
       "      <td>0.712196</td>\n",
       "      <td>0.734110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.716726</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.796174</td>\n",
       "      <td>0.701596</td>\n",
       "      <td>0.729205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.714532</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.787955</td>\n",
       "      <td>0.708774</td>\n",
       "      <td>0.729591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.746576</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790221</td>\n",
       "      <td>0.712542</td>\n",
       "      <td>0.732934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.722561</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.787966</td>\n",
       "      <td>0.711734</td>\n",
       "      <td>0.730383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.752141</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783645</td>\n",
       "      <td>0.705268</td>\n",
       "      <td>0.724576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.742941</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.782513</td>\n",
       "      <td>0.707275</td>\n",
       "      <td>0.725930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/jovyan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--recall/11f90e583db35601050aed380d48e83202a896976b9608432fba9244fb447f24 (last modified on Fri Jan 10 23:14:00 2025) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.\n",
      "[I 2025-03-16 06:27:07,628] Trial 71 finished with value: 0.7259298116320362 and parameters: {'learning_rate': 0.00021636896534386668, 'weight_decay': 0.007, 'adam_beta1': 0.96, 'warmup_steps': 22}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 with params: {'learning_rate': 2.3402871877686744e-06, 'weight_decay': 0.004, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:36 < 13:05, 16.71 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.793700</td>\n",
       "      <td>3.705907</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.014880</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.014939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.639200</td>\n",
       "      <td>3.574933</td>\n",
       "      <td>0.209899</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>0.029827</td>\n",
       "      <td>0.017615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.516700</td>\n",
       "      <td>3.458071</td>\n",
       "      <td>0.252979</td>\n",
       "      <td>0.051116</td>\n",
       "      <td>0.042739</td>\n",
       "      <td>0.034545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.406300</td>\n",
       "      <td>3.349581</td>\n",
       "      <td>0.326306</td>\n",
       "      <td>0.042781</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>0.048294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.300900</td>\n",
       "      <td>3.253550</td>\n",
       "      <td>0.351971</td>\n",
       "      <td>0.060158</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>0.051430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:29:45,846] Trial 72 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 73 with params: {'learning_rate': 6.912248600811238e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.91, 'warmup_steps': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:09 < 10:19, 16.95 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.526300</td>\n",
       "      <td>1.736819</td>\n",
       "      <td>0.674610</td>\n",
       "      <td>0.322842</td>\n",
       "      <td>0.298040</td>\n",
       "      <td>0.286638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.171400</td>\n",
       "      <td>1.203966</td>\n",
       "      <td>0.750687</td>\n",
       "      <td>0.407656</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.395890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>1.067865</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.477432</td>\n",
       "      <td>0.477260</td>\n",
       "      <td>0.463809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>1.010209</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.472522</td>\n",
       "      <td>0.484125</td>\n",
       "      <td>0.472459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>0.991575</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>0.528635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>1.007327</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.655329</td>\n",
       "      <td>0.584142</td>\n",
       "      <td>0.598625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>1.037617</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.670250</td>\n",
       "      <td>0.603116</td>\n",
       "      <td>0.619374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>1.042392</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.691516</td>\n",
       "      <td>0.624479</td>\n",
       "      <td>0.640757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>1.081000</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.677784</td>\n",
       "      <td>0.607695</td>\n",
       "      <td>0.624267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>1.131648</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.731081</td>\n",
       "      <td>0.647998</td>\n",
       "      <td>0.664983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:34:56,714] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 74 with params: {'learning_rate': 9.24846444289197e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9400000000000001, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:35 < 12:55, 16.92 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.330600</td>\n",
       "      <td>1.500503</td>\n",
       "      <td>0.714024</td>\n",
       "      <td>0.337522</td>\n",
       "      <td>0.349381</td>\n",
       "      <td>0.329462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>1.098837</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.440024</td>\n",
       "      <td>0.457932</td>\n",
       "      <td>0.441183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>1.013243</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.466297</td>\n",
       "      <td>0.490610</td>\n",
       "      <td>0.472227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>1.001850</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.579641</td>\n",
       "      <td>0.538351</td>\n",
       "      <td>0.544047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>1.015240</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.640750</td>\n",
       "      <td>0.593330</td>\n",
       "      <td>0.603755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:37:32,878] Trial 74 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 75 with params: {'learning_rate': 8.836929764547837e-05, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:40, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.297300</td>\n",
       "      <td>1.507881</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.383491</td>\n",
       "      <td>0.353862</td>\n",
       "      <td>0.338662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>1.112540</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.438805</td>\n",
       "      <td>0.449291</td>\n",
       "      <td>0.430184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>1.017690</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.462421</td>\n",
       "      <td>0.480284</td>\n",
       "      <td>0.465540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>1.001048</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.577373</td>\n",
       "      <td>0.543602</td>\n",
       "      <td>0.546164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>1.012321</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.633704</td>\n",
       "      <td>0.593675</td>\n",
       "      <td>0.600108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>1.039469</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.718118</td>\n",
       "      <td>0.621357</td>\n",
       "      <td>0.647486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>1.080780</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.756281</td>\n",
       "      <td>0.661316</td>\n",
       "      <td>0.685598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>1.106290</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.772564</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.698273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.142771</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786211</td>\n",
       "      <td>0.682892</td>\n",
       "      <td>0.712632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>1.189117</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>0.690130</td>\n",
       "      <td>0.715291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>1.176481</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788090</td>\n",
       "      <td>0.713861</td>\n",
       "      <td>0.732305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.212689</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.795884</td>\n",
       "      <td>0.728340</td>\n",
       "      <td>0.741911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.216034</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.713968</td>\n",
       "      <td>0.732315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.245783</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.804579</td>\n",
       "      <td>0.716093</td>\n",
       "      <td>0.742393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1.309349</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.783668</td>\n",
       "      <td>0.712867</td>\n",
       "      <td>0.730886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.310452</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.781699</td>\n",
       "      <td>0.712067</td>\n",
       "      <td>0.732002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.325051</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.793306</td>\n",
       "      <td>0.707202</td>\n",
       "      <td>0.730224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.346137</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.761281</td>\n",
       "      <td>0.716218</td>\n",
       "      <td>0.723402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.326560</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.716213</td>\n",
       "      <td>0.732490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.359584</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.787838</td>\n",
       "      <td>0.724382</td>\n",
       "      <td>0.740242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.358041</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.787740</td>\n",
       "      <td>0.727334</td>\n",
       "      <td>0.743566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.389994</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.789652</td>\n",
       "      <td>0.720784</td>\n",
       "      <td>0.736872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.416986</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.769467</td>\n",
       "      <td>0.726109</td>\n",
       "      <td>0.733922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.423953</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.778312</td>\n",
       "      <td>0.728354</td>\n",
       "      <td>0.740026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.417653</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.805011</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>0.744862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.425740</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.780404</td>\n",
       "      <td>0.717470</td>\n",
       "      <td>0.734269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.436342</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781620</td>\n",
       "      <td>0.721577</td>\n",
       "      <td>0.733959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.431889</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.791915</td>\n",
       "      <td>0.721447</td>\n",
       "      <td>0.740299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.437441</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791440</td>\n",
       "      <td>0.725382</td>\n",
       "      <td>0.742413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.439178</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.788073</td>\n",
       "      <td>0.724158</td>\n",
       "      <td>0.740527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 06:53:15,293] Trial 75 finished with value: 0.7405269141452502 and parameters: {'learning_rate': 8.836929764547837e-05, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 0}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 76 with params: {'learning_rate': 0.00029577722607030635, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:14 < 05:07, 17.08 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.300500</td>\n",
       "      <td>1.011738</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.496173</td>\n",
       "      <td>0.494196</td>\n",
       "      <td>0.484293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>1.050381</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.710508</td>\n",
       "      <td>0.644767</td>\n",
       "      <td>0.661314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>1.141284</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.770666</td>\n",
       "      <td>0.680240</td>\n",
       "      <td>0.702543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>1.187694</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.810839</td>\n",
       "      <td>0.707886</td>\n",
       "      <td>0.738380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>1.296576</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.709253</td>\n",
       "      <td>0.731768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>1.335168</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.802950</td>\n",
       "      <td>0.699131</td>\n",
       "      <td>0.730108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>1.435912</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.777743</td>\n",
       "      <td>0.678002</td>\n",
       "      <td>0.707109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.440594</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.795057</td>\n",
       "      <td>0.705166</td>\n",
       "      <td>0.729222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.553899</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.786308</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.716394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.607038</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.703329</td>\n",
       "      <td>0.725385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.631466</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.782405</td>\n",
       "      <td>0.690636</td>\n",
       "      <td>0.715579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.659559</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.785789</td>\n",
       "      <td>0.699945</td>\n",
       "      <td>0.721918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.579315</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.703460</td>\n",
       "      <td>0.712794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.567904</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.775355</td>\n",
       "      <td>0.705497</td>\n",
       "      <td>0.717603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.668465</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>0.709721</td>\n",
       "      <td>0.740630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.708708</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.778791</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>0.717609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.702173</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.696956</td>\n",
       "      <td>0.728099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.735590</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.801643</td>\n",
       "      <td>0.709238</td>\n",
       "      <td>0.733680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.782969</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.811271</td>\n",
       "      <td>0.697003</td>\n",
       "      <td>0.728627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.749975</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.783681</td>\n",
       "      <td>0.690078</td>\n",
       "      <td>0.710752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:03:30,937] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 77 with params: {'learning_rate': 0.00011388436281762621, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:02 < 10:06, 17.32 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.082000</td>\n",
       "      <td>1.329453</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.408384</td>\n",
       "      <td>0.389482</td>\n",
       "      <td>0.371323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>1.046065</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.452666</td>\n",
       "      <td>0.472068</td>\n",
       "      <td>0.453179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>1.000833</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.545307</td>\n",
       "      <td>0.550557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>1.032346</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.640601</td>\n",
       "      <td>0.586575</td>\n",
       "      <td>0.600547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>1.048577</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.702087</td>\n",
       "      <td>0.631246</td>\n",
       "      <td>0.645982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>1.093805</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.768821</td>\n",
       "      <td>0.668416</td>\n",
       "      <td>0.695141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>1.171937</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783034</td>\n",
       "      <td>0.691291</td>\n",
       "      <td>0.712429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>1.179129</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.788613</td>\n",
       "      <td>0.698932</td>\n",
       "      <td>0.720616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>1.223631</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.781344</td>\n",
       "      <td>0.691303</td>\n",
       "      <td>0.714090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>1.289599</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>0.697691</td>\n",
       "      <td>0.718399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:08:35,169] Trial 77 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 78 with params: {'learning_rate': 0.00010745549908189871, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 09:55 < 04:58, 17.62 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.123700</td>\n",
       "      <td>1.361896</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.381264</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.363054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.768700</td>\n",
       "      <td>1.059717</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.466032</td>\n",
       "      <td>0.466416</td>\n",
       "      <td>0.446613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.995924</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.575515</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.526999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>1.025779</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.611109</td>\n",
       "      <td>0.559471</td>\n",
       "      <td>0.570760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>1.038948</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.697832</td>\n",
       "      <td>0.626986</td>\n",
       "      <td>0.643624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>1.081834</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.754490</td>\n",
       "      <td>0.657272</td>\n",
       "      <td>0.683271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>1.153255</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.755410</td>\n",
       "      <td>0.669745</td>\n",
       "      <td>0.689094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>1.170017</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.789982</td>\n",
       "      <td>0.712113</td>\n",
       "      <td>0.728721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>1.209115</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.789225</td>\n",
       "      <td>0.698564</td>\n",
       "      <td>0.722442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.265434</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786166</td>\n",
       "      <td>0.696280</td>\n",
       "      <td>0.721084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.253273</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.757836</td>\n",
       "      <td>0.721319</td>\n",
       "      <td>0.725513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.281606</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.768465</td>\n",
       "      <td>0.732785</td>\n",
       "      <td>0.734090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.306953</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.779535</td>\n",
       "      <td>0.730858</td>\n",
       "      <td>0.737849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.335673</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.731168</td>\n",
       "      <td>0.746787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>1.382548</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788326</td>\n",
       "      <td>0.722132</td>\n",
       "      <td>0.735592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.393404</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.798174</td>\n",
       "      <td>0.708198</td>\n",
       "      <td>0.733374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.399051</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.793430</td>\n",
       "      <td>0.727753</td>\n",
       "      <td>0.739204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.443759</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779196</td>\n",
       "      <td>0.742401</td>\n",
       "      <td>0.743152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.389786</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.779540</td>\n",
       "      <td>0.741060</td>\n",
       "      <td>0.747555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.432277</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.770772</td>\n",
       "      <td>0.719082</td>\n",
       "      <td>0.730635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:18:36,700] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 79 with params: {'learning_rate': 7.85993259542217e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:32, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.406800</td>\n",
       "      <td>1.609597</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.333930</td>\n",
       "      <td>0.322939</td>\n",
       "      <td>0.305427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>1.149055</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.450993</td>\n",
       "      <td>0.434383</td>\n",
       "      <td>0.422292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>1.041125</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.470798</td>\n",
       "      <td>0.483867</td>\n",
       "      <td>0.471359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>1.000508</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.542646</td>\n",
       "      <td>0.509990</td>\n",
       "      <td>0.508245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.998175</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.645122</td>\n",
       "      <td>0.582118</td>\n",
       "      <td>0.597464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>1.020649</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.691160</td>\n",
       "      <td>0.619438</td>\n",
       "      <td>0.637922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>1.055343</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.685075</td>\n",
       "      <td>0.617965</td>\n",
       "      <td>0.633814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>1.072145</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.775929</td>\n",
       "      <td>0.676902</td>\n",
       "      <td>0.701081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>1.119327</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.759422</td>\n",
       "      <td>0.653577</td>\n",
       "      <td>0.681874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>1.175144</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.765457</td>\n",
       "      <td>0.684856</td>\n",
       "      <td>0.704719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>1.175587</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.690029</td>\n",
       "      <td>0.704042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>1.201303</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786132</td>\n",
       "      <td>0.720832</td>\n",
       "      <td>0.730135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>1.207679</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.770369</td>\n",
       "      <td>0.706441</td>\n",
       "      <td>0.718245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>1.221566</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.788267</td>\n",
       "      <td>0.712042</td>\n",
       "      <td>0.731088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.287038</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783853</td>\n",
       "      <td>0.714915</td>\n",
       "      <td>0.728662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.289181</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.786666</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.735663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1.286940</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.709887</td>\n",
       "      <td>0.728232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>1.333345</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.723962</td>\n",
       "      <td>0.737655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>1.306051</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.724245</td>\n",
       "      <td>0.736243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.312315</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.775388</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.731986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.333811</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.772653</td>\n",
       "      <td>0.720985</td>\n",
       "      <td>0.729437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.358645</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.785324</td>\n",
       "      <td>0.728079</td>\n",
       "      <td>0.737563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.375538</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.773412</td>\n",
       "      <td>0.729137</td>\n",
       "      <td>0.734319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.387147</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.769956</td>\n",
       "      <td>0.724337</td>\n",
       "      <td>0.730064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.385115</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.795534</td>\n",
       "      <td>0.727270</td>\n",
       "      <td>0.740891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.401295</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.779253</td>\n",
       "      <td>0.724718</td>\n",
       "      <td>0.735868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.404960</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.725050</td>\n",
       "      <td>0.738306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.402725</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.785191</td>\n",
       "      <td>0.725658</td>\n",
       "      <td>0.738819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.409817</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.778164</td>\n",
       "      <td>0.723673</td>\n",
       "      <td>0.732469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.408171</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.776985</td>\n",
       "      <td>0.722994</td>\n",
       "      <td>0.731695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:34:10,823] Trial 79 finished with value: 0.7316952041227047 and parameters: {'learning_rate': 7.85993259542217e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 4}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 80 with params: {'learning_rate': 1.2466463833276121e-05, 'weight_decay': 0.01, 'adam_beta1': 0.99, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:15 < 10:32, 16.61 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.571600</td>\n",
       "      <td>3.299789</td>\n",
       "      <td>0.325390</td>\n",
       "      <td>0.043293</td>\n",
       "      <td>0.064437</td>\n",
       "      <td>0.047354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.045300</td>\n",
       "      <td>2.850230</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.109106</td>\n",
       "      <td>0.110829</td>\n",
       "      <td>0.095011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.620200</td>\n",
       "      <td>2.481766</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.149207</td>\n",
       "      <td>0.152172</td>\n",
       "      <td>0.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.271500</td>\n",
       "      <td>2.192612</td>\n",
       "      <td>0.574702</td>\n",
       "      <td>0.210240</td>\n",
       "      <td>0.207623</td>\n",
       "      <td>0.189448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.982700</td>\n",
       "      <td>1.964375</td>\n",
       "      <td>0.609533</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.234669</td>\n",
       "      <td>0.212703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.753000</td>\n",
       "      <td>1.780689</td>\n",
       "      <td>0.659028</td>\n",
       "      <td>0.296091</td>\n",
       "      <td>0.282174</td>\n",
       "      <td>0.266229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.560200</td>\n",
       "      <td>1.635730</td>\n",
       "      <td>0.687443</td>\n",
       "      <td>0.361186</td>\n",
       "      <td>0.316147</td>\n",
       "      <td>0.303860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.405900</td>\n",
       "      <td>1.520021</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.374464</td>\n",
       "      <td>0.343865</td>\n",
       "      <td>0.330223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.277300</td>\n",
       "      <td>1.433430</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.365806</td>\n",
       "      <td>0.361647</td>\n",
       "      <td>0.342379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>1.360029</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.391473</td>\n",
       "      <td>0.382557</td>\n",
       "      <td>0.367589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:39:27,970] Trial 80 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 81 with params: {'learning_rate': 0.00020915505374898168, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:55, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.578600</td>\n",
       "      <td>1.076607</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.425031</td>\n",
       "      <td>0.462275</td>\n",
       "      <td>0.434701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359600</td>\n",
       "      <td>1.006755</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.668389</td>\n",
       "      <td>0.598698</td>\n",
       "      <td>0.614408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>1.081282</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.714756</td>\n",
       "      <td>0.656117</td>\n",
       "      <td>0.669136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.132778</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.783630</td>\n",
       "      <td>0.685883</td>\n",
       "      <td>0.715577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>1.183590</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.818853</td>\n",
       "      <td>0.725201</td>\n",
       "      <td>0.751720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>1.235040</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798427</td>\n",
       "      <td>0.694776</td>\n",
       "      <td>0.724805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>1.295991</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.818174</td>\n",
       "      <td>0.698132</td>\n",
       "      <td>0.733448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>1.348690</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.806382</td>\n",
       "      <td>0.730527</td>\n",
       "      <td>0.747535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.389354</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782607</td>\n",
       "      <td>0.705045</td>\n",
       "      <td>0.726022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.448485</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.791256</td>\n",
       "      <td>0.702392</td>\n",
       "      <td>0.727263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.498501</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.796920</td>\n",
       "      <td>0.709327</td>\n",
       "      <td>0.731883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.461538</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.800426</td>\n",
       "      <td>0.737384</td>\n",
       "      <td>0.748853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.531209</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.774780</td>\n",
       "      <td>0.712343</td>\n",
       "      <td>0.727478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.505169</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797307</td>\n",
       "      <td>0.709158</td>\n",
       "      <td>0.729108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.585354</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.797653</td>\n",
       "      <td>0.703817</td>\n",
       "      <td>0.725050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.660121</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786278</td>\n",
       "      <td>0.711315</td>\n",
       "      <td>0.731017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.680997</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.780039</td>\n",
       "      <td>0.711364</td>\n",
       "      <td>0.725105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.697854</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.800267</td>\n",
       "      <td>0.706143</td>\n",
       "      <td>0.733088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.670111</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.777325</td>\n",
       "      <td>0.708328</td>\n",
       "      <td>0.725434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.694964</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787095</td>\n",
       "      <td>0.713025</td>\n",
       "      <td>0.734385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.715375</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.784220</td>\n",
       "      <td>0.713142</td>\n",
       "      <td>0.732834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.721283</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797345</td>\n",
       "      <td>0.708544</td>\n",
       "      <td>0.735105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.748199</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>0.721812</td>\n",
       "      <td>0.737211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.786870</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.799997</td>\n",
       "      <td>0.708322</td>\n",
       "      <td>0.737012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.765109</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.732369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.794238</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.795033</td>\n",
       "      <td>0.717292</td>\n",
       "      <td>0.738628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.788028</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.792773</td>\n",
       "      <td>0.710678</td>\n",
       "      <td>0.732554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.779197</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.794290</td>\n",
       "      <td>0.707632</td>\n",
       "      <td>0.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.803811</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.792979</td>\n",
       "      <td>0.709523</td>\n",
       "      <td>0.732410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.805733</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790750</td>\n",
       "      <td>0.710789</td>\n",
       "      <td>0.732909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 07:55:25,701] Trial 81 finished with value: 0.7329089318240765 and parameters: {'learning_rate': 0.00020915505374898168, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 2}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 82 with params: {'learning_rate': 0.00015504972457644627, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.980800</td>\n",
       "      <td>1.214561</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.410195</td>\n",
       "      <td>0.413352</td>\n",
       "      <td>0.397885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>1.002907</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.535465</td>\n",
       "      <td>0.509158</td>\n",
       "      <td>0.504375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.998348</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.637640</td>\n",
       "      <td>0.610442</td>\n",
       "      <td>0.612331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>1.073203</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.746880</td>\n",
       "      <td>0.641426</td>\n",
       "      <td>0.669688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>1.094693</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788856</td>\n",
       "      <td>0.690132</td>\n",
       "      <td>0.719239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>1.164420</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.693785</td>\n",
       "      <td>0.721087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.259862</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.798228</td>\n",
       "      <td>0.700577</td>\n",
       "      <td>0.725115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>1.262525</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.806325</td>\n",
       "      <td>0.733317</td>\n",
       "      <td>0.750726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.329290</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780783</td>\n",
       "      <td>0.706544</td>\n",
       "      <td>0.725317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.370487</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.783922</td>\n",
       "      <td>0.715879</td>\n",
       "      <td>0.728890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.390592</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.812861</td>\n",
       "      <td>0.705246</td>\n",
       "      <td>0.732469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.433313</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.808331</td>\n",
       "      <td>0.732964</td>\n",
       "      <td>0.749139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.460986</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.777167</td>\n",
       "      <td>0.715530</td>\n",
       "      <td>0.724595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.471693</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.813170</td>\n",
       "      <td>0.728578</td>\n",
       "      <td>0.750666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.491544</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.797486</td>\n",
       "      <td>0.733969</td>\n",
       "      <td>0.747257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.553856</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.803384</td>\n",
       "      <td>0.704842</td>\n",
       "      <td>0.731021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.593962</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.810524</td>\n",
       "      <td>0.696981</td>\n",
       "      <td>0.727168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.606869</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.790581</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.725539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.556330</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.726752</td>\n",
       "      <td>0.739834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.610380</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.798539</td>\n",
       "      <td>0.719440</td>\n",
       "      <td>0.738450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.631111</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.790239</td>\n",
       "      <td>0.721104</td>\n",
       "      <td>0.734159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.605781</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.799527</td>\n",
       "      <td>0.721753</td>\n",
       "      <td>0.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.652962</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.798422</td>\n",
       "      <td>0.721658</td>\n",
       "      <td>0.740433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.658188</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.805176</td>\n",
       "      <td>0.725586</td>\n",
       "      <td>0.747419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.674076</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.804820</td>\n",
       "      <td>0.720126</td>\n",
       "      <td>0.741874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.698449</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.793704</td>\n",
       "      <td>0.717312</td>\n",
       "      <td>0.736406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.693151</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.796344</td>\n",
       "      <td>0.721360</td>\n",
       "      <td>0.739676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.686995</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.794179</td>\n",
       "      <td>0.723969</td>\n",
       "      <td>0.741261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.693643</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.792821</td>\n",
       "      <td>0.723920</td>\n",
       "      <td>0.740642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.700450</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.790157</td>\n",
       "      <td>0.719677</td>\n",
       "      <td>0.736823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 08:11:03,366] Trial 82 finished with value: 0.7368234468273828 and parameters: {'learning_rate': 0.00015504972457644627, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 32}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83 with params: {'learning_rate': 7.919910992358981e-05, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:37 < 13:06, 16.69 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.545300</td>\n",
       "      <td>1.687372</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>0.318145</td>\n",
       "      <td>0.300345</td>\n",
       "      <td>0.286430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.099600</td>\n",
       "      <td>1.164156</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.442707</td>\n",
       "      <td>0.431336</td>\n",
       "      <td>0.417884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>1.041329</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.468999</td>\n",
       "      <td>0.484847</td>\n",
       "      <td>0.469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.992181</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.515892</td>\n",
       "      <td>0.504258</td>\n",
       "      <td>0.493518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.998684</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.621199</td>\n",
       "      <td>0.578290</td>\n",
       "      <td>0.584643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 08:13:41,803] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 with params: {'learning_rate': 8.991175211632078e-05, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:27, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.422200</td>\n",
       "      <td>1.561575</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>0.323953</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.974800</td>\n",
       "      <td>1.113776</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.447289</td>\n",
       "      <td>0.451440</td>\n",
       "      <td>0.434955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>1.019519</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.466237</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.470657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.541805</td>\n",
       "      <td>0.518954</td>\n",
       "      <td>0.516552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>1.007396</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.670085</td>\n",
       "      <td>0.601192</td>\n",
       "      <td>0.617886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>1.048468</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.688301</td>\n",
       "      <td>0.607328</td>\n",
       "      <td>0.629140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.065503</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.757522</td>\n",
       "      <td>0.663748</td>\n",
       "      <td>0.686995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>1.097739</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.799442</td>\n",
       "      <td>0.706065</td>\n",
       "      <td>0.727988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>1.149879</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.783890</td>\n",
       "      <td>0.702286</td>\n",
       "      <td>0.724012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>1.198551</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.785053</td>\n",
       "      <td>0.709532</td>\n",
       "      <td>0.724997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>1.198518</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.804276</td>\n",
       "      <td>0.707132</td>\n",
       "      <td>0.728974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.219510</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.779541</td>\n",
       "      <td>0.716125</td>\n",
       "      <td>0.726753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.245763</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.779843</td>\n",
       "      <td>0.710487</td>\n",
       "      <td>0.726180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>1.252658</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.796369</td>\n",
       "      <td>0.719021</td>\n",
       "      <td>0.739985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.332270</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.772240</td>\n",
       "      <td>0.702424</td>\n",
       "      <td>0.715225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>1.350441</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.780541</td>\n",
       "      <td>0.723053</td>\n",
       "      <td>0.736248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.359592</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.795196</td>\n",
       "      <td>0.706696</td>\n",
       "      <td>0.731675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.385021</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.782249</td>\n",
       "      <td>0.714978</td>\n",
       "      <td>0.727979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.376841</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.785889</td>\n",
       "      <td>0.713633</td>\n",
       "      <td>0.730766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.390776</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.775191</td>\n",
       "      <td>0.714922</td>\n",
       "      <td>0.727864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.395282</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.782064</td>\n",
       "      <td>0.737106</td>\n",
       "      <td>0.743936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.423340</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.784150</td>\n",
       "      <td>0.712104</td>\n",
       "      <td>0.727997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.444394</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777256</td>\n",
       "      <td>0.728810</td>\n",
       "      <td>0.737789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.452395</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.789815</td>\n",
       "      <td>0.737218</td>\n",
       "      <td>0.745784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.449109</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796074</td>\n",
       "      <td>0.725775</td>\n",
       "      <td>0.741680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.465854</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.778125</td>\n",
       "      <td>0.734151</td>\n",
       "      <td>0.741333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.467577</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.768157</td>\n",
       "      <td>0.729490</td>\n",
       "      <td>0.733099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.463669</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.780450</td>\n",
       "      <td>0.733142</td>\n",
       "      <td>0.742219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.470801</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.771121</td>\n",
       "      <td>0.727994</td>\n",
       "      <td>0.733999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.471463</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.773129</td>\n",
       "      <td>0.728461</td>\n",
       "      <td>0.735542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 08:29:11,108] Trial 84 finished with value: 0.7355420594615302 and parameters: {'learning_rate': 8.991175211632078e-05, 'weight_decay': 0.008, 'adam_beta1': 0.96, 'warmup_steps': 27}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85 with params: {'learning_rate': 0.0001264099551930982, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:19, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.125500</td>\n",
       "      <td>1.307273</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>0.391016</td>\n",
       "      <td>0.368593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>1.032786</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.485559</td>\n",
       "      <td>0.482106</td>\n",
       "      <td>0.476411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.995602</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.595854</td>\n",
       "      <td>0.558524</td>\n",
       "      <td>0.558356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>1.015727</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.695029</td>\n",
       "      <td>0.624377</td>\n",
       "      <td>0.641109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>1.051717</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.711406</td>\n",
       "      <td>0.643151</td>\n",
       "      <td>0.659750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>1.120684</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782205</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.701937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>1.171419</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790234</td>\n",
       "      <td>0.687273</td>\n",
       "      <td>0.710361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.196008</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>0.714258</td>\n",
       "      <td>0.728369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>1.244685</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>0.710198</td>\n",
       "      <td>0.724724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>1.309951</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.809953</td>\n",
       "      <td>0.705538</td>\n",
       "      <td>0.730298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.318790</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782161</td>\n",
       "      <td>0.705138</td>\n",
       "      <td>0.724852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>1.319089</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.782803</td>\n",
       "      <td>0.714721</td>\n",
       "      <td>0.722339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.399130</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.777966</td>\n",
       "      <td>0.707337</td>\n",
       "      <td>0.718232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.367150</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.803718</td>\n",
       "      <td>0.724071</td>\n",
       "      <td>0.744670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.468865</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783632</td>\n",
       "      <td>0.730645</td>\n",
       "      <td>0.740099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.492912</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.800491</td>\n",
       "      <td>0.714704</td>\n",
       "      <td>0.734035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.474948</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.799956</td>\n",
       "      <td>0.720101</td>\n",
       "      <td>0.738338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.490577</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782508</td>\n",
       "      <td>0.721447</td>\n",
       "      <td>0.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.449545</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.796253</td>\n",
       "      <td>0.739870</td>\n",
       "      <td>0.752118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.471867</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.807796</td>\n",
       "      <td>0.733864</td>\n",
       "      <td>0.749261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.527657</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.772886</td>\n",
       "      <td>0.731355</td>\n",
       "      <td>0.737807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.508448</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.812579</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.753594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.554544</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.800828</td>\n",
       "      <td>0.732730</td>\n",
       "      <td>0.747441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.547803</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.789617</td>\n",
       "      <td>0.730732</td>\n",
       "      <td>0.743533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.552462</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.808523</td>\n",
       "      <td>0.727110</td>\n",
       "      <td>0.746354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.585005</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.779303</td>\n",
       "      <td>0.724577</td>\n",
       "      <td>0.735482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.579919</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.792907</td>\n",
       "      <td>0.722482</td>\n",
       "      <td>0.736606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.570743</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.795692</td>\n",
       "      <td>0.727602</td>\n",
       "      <td>0.742385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.583796</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.726279</td>\n",
       "      <td>0.739499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.585603</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.794910</td>\n",
       "      <td>0.728279</td>\n",
       "      <td>0.742017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 08:44:32,899] Trial 85 finished with value: 0.7420168179200104 and parameters: {'learning_rate': 0.0001264099551930982, 'weight_decay': 0.008, 'adam_beta1': 0.9500000000000001, 'warmup_steps': 34}. Best is trial 69 with value: 0.747359569286359.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 86 with params: {'learning_rate': 4.3182046483014284e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'warmup_steps': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:32 < 12:41, 17.24 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.895800</td>\n",
       "      <td>2.200905</td>\n",
       "      <td>0.584785</td>\n",
       "      <td>0.252032</td>\n",
       "      <td>0.213951</td>\n",
       "      <td>0.201107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>1.512172</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.324023</td>\n",
       "      <td>0.338909</td>\n",
       "      <td>0.315649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.241814</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.400151</td>\n",
       "      <td>0.405429</td>\n",
       "      <td>0.383243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.786700</td>\n",
       "      <td>1.117231</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.448881</td>\n",
       "      <td>0.447720</td>\n",
       "      <td>0.435292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.593500</td>\n",
       "      <td>1.062446</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.504893</td>\n",
       "      <td>0.477667</td>\n",
       "      <td>0.464891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 08:47:06,187] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87 with params: {'learning_rate': 0.0001253937577800803, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.086000</td>\n",
       "      <td>1.295802</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.379736</td>\n",
       "      <td>0.391369</td>\n",
       "      <td>0.366518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>1.036750</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.484626</td>\n",
       "      <td>0.481966</td>\n",
       "      <td>0.476319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.994189</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.599031</td>\n",
       "      <td>0.560153</td>\n",
       "      <td>0.561480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>1.026439</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.673704</td>\n",
       "      <td>0.605059</td>\n",
       "      <td>0.625081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.045203</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.710523</td>\n",
       "      <td>0.630810</td>\n",
       "      <td>0.652275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.131787</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.776880</td>\n",
       "      <td>0.665999</td>\n",
       "      <td>0.696990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>1.180471</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.702998</td>\n",
       "      <td>0.727815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>1.190124</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.796101</td>\n",
       "      <td>0.719515</td>\n",
       "      <td>0.735370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>1.253859</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783396</td>\n",
       "      <td>0.709318</td>\n",
       "      <td>0.727885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.319392</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.813905</td>\n",
       "      <td>0.708619</td>\n",
       "      <td>0.734130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>1.316008</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>0.695836</td>\n",
       "      <td>0.715564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>1.331429</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.776145</td>\n",
       "      <td>0.737716</td>\n",
       "      <td>0.739340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.390080</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.779889</td>\n",
       "      <td>0.709114</td>\n",
       "      <td>0.725261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.368234</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.807194</td>\n",
       "      <td>0.723363</td>\n",
       "      <td>0.745226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.452366</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.791692</td>\n",
       "      <td>0.720744</td>\n",
       "      <td>0.733895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.475629</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800779</td>\n",
       "      <td>0.714727</td>\n",
       "      <td>0.735975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.465686</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.805492</td>\n",
       "      <td>0.718389</td>\n",
       "      <td>0.738870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.494421</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788892</td>\n",
       "      <td>0.729489</td>\n",
       "      <td>0.739285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.447988</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.784147</td>\n",
       "      <td>0.737120</td>\n",
       "      <td>0.744789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.505016</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.793489</td>\n",
       "      <td>0.731103</td>\n",
       "      <td>0.744657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.523339</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.784550</td>\n",
       "      <td>0.731615</td>\n",
       "      <td>0.742685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.512356</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.811112</td>\n",
       "      <td>0.735346</td>\n",
       "      <td>0.753439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.550535</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.800599</td>\n",
       "      <td>0.739459</td>\n",
       "      <td>0.754466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.551091</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.793056</td>\n",
       "      <td>0.725653</td>\n",
       "      <td>0.743043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.560888</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.811023</td>\n",
       "      <td>0.721274</td>\n",
       "      <td>0.746107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.578291</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.794726</td>\n",
       "      <td>0.741132</td>\n",
       "      <td>0.752950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.574217</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.816709</td>\n",
       "      <td>0.741834</td>\n",
       "      <td>0.760076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.565589</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.816285</td>\n",
       "      <td>0.742810</td>\n",
       "      <td>0.760240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.580239</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.816494</td>\n",
       "      <td>0.742084</td>\n",
       "      <td>0.759925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.582892</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.809683</td>\n",
       "      <td>0.739584</td>\n",
       "      <td>0.754525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:02:55,274] Trial 87 finished with value: 0.7545251683099805 and parameters: {'learning_rate': 0.0001253937577800803, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 31}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88 with params: {'learning_rate': 7.285832479135637e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.93, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:13 < 10:26, 16.76 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>1.730481</td>\n",
       "      <td>0.677360</td>\n",
       "      <td>0.320117</td>\n",
       "      <td>0.300401</td>\n",
       "      <td>0.288569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.150100</td>\n",
       "      <td>1.193208</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.415504</td>\n",
       "      <td>0.429928</td>\n",
       "      <td>0.411601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>1.062914</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.466910</td>\n",
       "      <td>0.480725</td>\n",
       "      <td>0.463120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>1.005151</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.496023</td>\n",
       "      <td>0.496715</td>\n",
       "      <td>0.486683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.994576</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.597096</td>\n",
       "      <td>0.554781</td>\n",
       "      <td>0.561641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>1.013137</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.646596</td>\n",
       "      <td>0.589841</td>\n",
       "      <td>0.605364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>1.042878</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.690018</td>\n",
       "      <td>0.619644</td>\n",
       "      <td>0.636305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>1.051933</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.697623</td>\n",
       "      <td>0.626406</td>\n",
       "      <td>0.645273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>1.092906</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.701065</td>\n",
       "      <td>0.628062</td>\n",
       "      <td>0.645847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>1.145472</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.756251</td>\n",
       "      <td>0.677598</td>\n",
       "      <td>0.697273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:08:09,565] Trial 88 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 89 with params: {'learning_rate': 8.431777839855785e-05, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.394000</td>\n",
       "      <td>1.567836</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.327688</td>\n",
       "      <td>0.310883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>1.127972</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.450814</td>\n",
       "      <td>0.454446</td>\n",
       "      <td>0.438116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.468305</td>\n",
       "      <td>0.483926</td>\n",
       "      <td>0.470432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.994725</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.538960</td>\n",
       "      <td>0.516903</td>\n",
       "      <td>0.512595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.646839</td>\n",
       "      <td>0.586933</td>\n",
       "      <td>0.601153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>1.030891</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.689055</td>\n",
       "      <td>0.607697</td>\n",
       "      <td>0.629151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>1.060822</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.732401</td>\n",
       "      <td>0.647056</td>\n",
       "      <td>0.667574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>1.084352</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.793530</td>\n",
       "      <td>0.689212</td>\n",
       "      <td>0.713220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>1.130344</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.787066</td>\n",
       "      <td>0.686220</td>\n",
       "      <td>0.713835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>1.191742</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>0.692147</td>\n",
       "      <td>0.716156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>1.183503</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.781046</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.710296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>1.214986</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796262</td>\n",
       "      <td>0.726527</td>\n",
       "      <td>0.738604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>1.227303</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785917</td>\n",
       "      <td>0.709128</td>\n",
       "      <td>0.727766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.248943</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>0.716096</td>\n",
       "      <td>0.736224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.314522</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.781745</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.726567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>1.323624</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791406</td>\n",
       "      <td>0.723124</td>\n",
       "      <td>0.741998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1.317103</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797874</td>\n",
       "      <td>0.713771</td>\n",
       "      <td>0.736492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>1.351891</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.778569</td>\n",
       "      <td>0.720489</td>\n",
       "      <td>0.731355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>1.332795</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.783201</td>\n",
       "      <td>0.722033</td>\n",
       "      <td>0.734911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.351580</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.774249</td>\n",
       "      <td>0.733454</td>\n",
       "      <td>0.736871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.360477</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.770116</td>\n",
       "      <td>0.730462</td>\n",
       "      <td>0.735928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.385713</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>0.733402</td>\n",
       "      <td>0.739839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.402835</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.777249</td>\n",
       "      <td>0.733002</td>\n",
       "      <td>0.737906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.415549</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.782413</td>\n",
       "      <td>0.734703</td>\n",
       "      <td>0.743273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.419828</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.800105</td>\n",
       "      <td>0.735163</td>\n",
       "      <td>0.749177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.431173</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.774854</td>\n",
       "      <td>0.741892</td>\n",
       "      <td>0.745499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.429416</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.785329</td>\n",
       "      <td>0.734785</td>\n",
       "      <td>0.745729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.432007</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786050</td>\n",
       "      <td>0.734096</td>\n",
       "      <td>0.745927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.439911</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.784252</td>\n",
       "      <td>0.732435</td>\n",
       "      <td>0.743334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.437895</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.783536</td>\n",
       "      <td>0.734435</td>\n",
       "      <td>0.745153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:23:49,977] Trial 89 finished with value: 0.7451531507333542 and parameters: {'learning_rate': 8.431777839855785e-05, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 21}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 with params: {'learning_rate': 5.401664528764546e-05, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.92, 'warmup_steps': 23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:12 < 10:24, 16.81 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.754900</td>\n",
       "      <td>1.994746</td>\n",
       "      <td>0.614115</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.246085</td>\n",
       "      <td>0.228890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.443700</td>\n",
       "      <td>1.353418</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.406029</td>\n",
       "      <td>0.395887</td>\n",
       "      <td>0.383399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>1.147084</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.440854</td>\n",
       "      <td>0.439545</td>\n",
       "      <td>0.423754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>1.059630</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.487328</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>0.464734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>1.020573</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.500685</td>\n",
       "      <td>0.495378</td>\n",
       "      <td>0.488019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>1.003232</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.572879</td>\n",
       "      <td>0.523615</td>\n",
       "      <td>0.526522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>1.011438</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.590013</td>\n",
       "      <td>0.542493</td>\n",
       "      <td>0.546418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>1.010658</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.648964</td>\n",
       "      <td>0.593351</td>\n",
       "      <td>0.606852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>1.040004</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.663281</td>\n",
       "      <td>0.602435</td>\n",
       "      <td>0.615895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>1.065115</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.705587</td>\n",
       "      <td>0.621770</td>\n",
       "      <td>0.643041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:29:03,403] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91 with params: {'learning_rate': 0.0001785436549414426, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:31, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.782900</td>\n",
       "      <td>1.125075</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.425896</td>\n",
       "      <td>0.452436</td>\n",
       "      <td>0.426811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.998610</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.567263</td>\n",
       "      <td>0.523990</td>\n",
       "      <td>0.523773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>1.034347</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.724429</td>\n",
       "      <td>0.638294</td>\n",
       "      <td>0.659814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>1.083582</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776956</td>\n",
       "      <td>0.664339</td>\n",
       "      <td>0.700063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>1.149392</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.796642</td>\n",
       "      <td>0.702280</td>\n",
       "      <td>0.732162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>1.201053</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.807958</td>\n",
       "      <td>0.685940</td>\n",
       "      <td>0.718973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.269351</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.790955</td>\n",
       "      <td>0.690552</td>\n",
       "      <td>0.716427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.282758</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.801852</td>\n",
       "      <td>0.737290</td>\n",
       "      <td>0.749677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>1.340380</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.792621</td>\n",
       "      <td>0.713997</td>\n",
       "      <td>0.733451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.388960</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.713662</td>\n",
       "      <td>0.728925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.471124</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.771983</td>\n",
       "      <td>0.723174</td>\n",
       "      <td>0.734333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.459940</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.768168</td>\n",
       "      <td>0.720963</td>\n",
       "      <td>0.730952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.508474</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.700007</td>\n",
       "      <td>0.719314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.503533</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.811773</td>\n",
       "      <td>0.717416</td>\n",
       "      <td>0.745037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.551888</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798517</td>\n",
       "      <td>0.718933</td>\n",
       "      <td>0.742181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.590767</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.813478</td>\n",
       "      <td>0.710511</td>\n",
       "      <td>0.738326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.598384</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.806831</td>\n",
       "      <td>0.738187</td>\n",
       "      <td>0.755837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.626049</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.783193</td>\n",
       "      <td>0.709938</td>\n",
       "      <td>0.728034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.639534</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.785364</td>\n",
       "      <td>0.721218</td>\n",
       "      <td>0.737698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.658482</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.811032</td>\n",
       "      <td>0.732603</td>\n",
       "      <td>0.751428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.665678</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.797853</td>\n",
       "      <td>0.732649</td>\n",
       "      <td>0.748779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.656440</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794457</td>\n",
       "      <td>0.723025</td>\n",
       "      <td>0.742524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.673467</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.775325</td>\n",
       "      <td>0.715316</td>\n",
       "      <td>0.731078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.694883</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.792073</td>\n",
       "      <td>0.703804</td>\n",
       "      <td>0.728844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.728059</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.823707</td>\n",
       "      <td>0.718839</td>\n",
       "      <td>0.748167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.728788</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.793147</td>\n",
       "      <td>0.727416</td>\n",
       "      <td>0.744208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.751078</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.793558</td>\n",
       "      <td>0.726420</td>\n",
       "      <td>0.743559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.745417</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.802538</td>\n",
       "      <td>0.729263</td>\n",
       "      <td>0.746514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.745111</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791857</td>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.743938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.748798</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.793101</td>\n",
       "      <td>0.726534</td>\n",
       "      <td>0.743534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:44:37,556] Trial 91 finished with value: 0.7435341247193272 and parameters: {'learning_rate': 0.0001785436549414426, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 36}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 92 with params: {'learning_rate': 0.00025249939621007966, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:19, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.518600</td>\n",
       "      <td>1.030146</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.462997</td>\n",
       "      <td>0.464405</td>\n",
       "      <td>0.445113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.700129</td>\n",
       "      <td>0.621192</td>\n",
       "      <td>0.639089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>1.098224</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.677706</td>\n",
       "      <td>0.690823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.143204</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.805296</td>\n",
       "      <td>0.706589</td>\n",
       "      <td>0.736578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.214207</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783818</td>\n",
       "      <td>0.708109</td>\n",
       "      <td>0.729578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.290832</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789624</td>\n",
       "      <td>0.699853</td>\n",
       "      <td>0.723957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>1.372360</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.800196</td>\n",
       "      <td>0.702638</td>\n",
       "      <td>0.726690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.387321</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.798458</td>\n",
       "      <td>0.716768</td>\n",
       "      <td>0.734314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.457664</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.772308</td>\n",
       "      <td>0.732825</td>\n",
       "      <td>0.737235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.477017</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.814312</td>\n",
       "      <td>0.727857</td>\n",
       "      <td>0.748179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.487530</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.786905</td>\n",
       "      <td>0.704949</td>\n",
       "      <td>0.723768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.513993</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.796436</td>\n",
       "      <td>0.726033</td>\n",
       "      <td>0.739220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.604962</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.795687</td>\n",
       "      <td>0.697798</td>\n",
       "      <td>0.724271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.636063</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.782046</td>\n",
       "      <td>0.707391</td>\n",
       "      <td>0.726114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.636312</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.776373</td>\n",
       "      <td>0.707397</td>\n",
       "      <td>0.722345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.691705</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.791902</td>\n",
       "      <td>0.702339</td>\n",
       "      <td>0.727743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.702559</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.786529</td>\n",
       "      <td>0.736930</td>\n",
       "      <td>0.741395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.754194</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778810</td>\n",
       "      <td>0.723775</td>\n",
       "      <td>0.733812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.774243</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.777354</td>\n",
       "      <td>0.724195</td>\n",
       "      <td>0.732533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.739396</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.810630</td>\n",
       "      <td>0.714141</td>\n",
       "      <td>0.739709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.741069</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.803845</td>\n",
       "      <td>0.718867</td>\n",
       "      <td>0.740599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.771984</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.795037</td>\n",
       "      <td>0.723210</td>\n",
       "      <td>0.737980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.778587</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.795508</td>\n",
       "      <td>0.733712</td>\n",
       "      <td>0.745339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.784346</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.806173</td>\n",
       "      <td>0.728441</td>\n",
       "      <td>0.747009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.828453</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.807605</td>\n",
       "      <td>0.709852</td>\n",
       "      <td>0.736617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.816761</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.806566</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.739722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.841422</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.797930</td>\n",
       "      <td>0.717190</td>\n",
       "      <td>0.736050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.814159</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.799451</td>\n",
       "      <td>0.721077</td>\n",
       "      <td>0.738561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.825953</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.793445</td>\n",
       "      <td>0.721141</td>\n",
       "      <td>0.735798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.829661</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.797901</td>\n",
       "      <td>0.720881</td>\n",
       "      <td>0.737703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 09:59:59,422] Trial 92 finished with value: 0.7377027844793805 and parameters: {'learning_rate': 0.00025249939621007966, 'weight_decay': 0.008, 'adam_beta1': 0.93, 'warmup_steps': 33}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 93 with params: {'learning_rate': 0.0002798432432643174, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:31, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.446900</td>\n",
       "      <td>1.020198</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.456235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>1.029150</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.737414</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.660378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>1.129745</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.801352</td>\n",
       "      <td>0.728513</td>\n",
       "      <td>0.746462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>1.184451</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.809144</td>\n",
       "      <td>0.695451</td>\n",
       "      <td>0.727079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.259562</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.805503</td>\n",
       "      <td>0.697913</td>\n",
       "      <td>0.730307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>1.315022</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.785845</td>\n",
       "      <td>0.698148</td>\n",
       "      <td>0.720848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>1.375414</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.786362</td>\n",
       "      <td>0.708568</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.393820</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.789576</td>\n",
       "      <td>0.722726</td>\n",
       "      <td>0.736858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.454928</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.809859</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>0.751513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.508336</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.805760</td>\n",
       "      <td>0.724741</td>\n",
       "      <td>0.741797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.563975</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.787295</td>\n",
       "      <td>0.722516</td>\n",
       "      <td>0.737827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.535346</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.798996</td>\n",
       "      <td>0.717803</td>\n",
       "      <td>0.742053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.590342</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.771295</td>\n",
       "      <td>0.711884</td>\n",
       "      <td>0.724259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.509815</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.804687</td>\n",
       "      <td>0.712331</td>\n",
       "      <td>0.738740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.669974</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.815325</td>\n",
       "      <td>0.707076</td>\n",
       "      <td>0.738893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.679713</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799972</td>\n",
       "      <td>0.712434</td>\n",
       "      <td>0.733243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.687664</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.796035</td>\n",
       "      <td>0.715693</td>\n",
       "      <td>0.734986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.710178</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.779707</td>\n",
       "      <td>0.718979</td>\n",
       "      <td>0.727969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.715955</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.778469</td>\n",
       "      <td>0.715054</td>\n",
       "      <td>0.728058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.704608</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.807746</td>\n",
       "      <td>0.731488</td>\n",
       "      <td>0.752054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.766108</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800264</td>\n",
       "      <td>0.718918</td>\n",
       "      <td>0.740411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.732376</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.810124</td>\n",
       "      <td>0.720522</td>\n",
       "      <td>0.743612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.823206</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.793686</td>\n",
       "      <td>0.704791</td>\n",
       "      <td>0.728837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.777681</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791771</td>\n",
       "      <td>0.725612</td>\n",
       "      <td>0.741838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.828649</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.801983</td>\n",
       "      <td>0.732133</td>\n",
       "      <td>0.749895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.840183</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.800212</td>\n",
       "      <td>0.726384</td>\n",
       "      <td>0.744698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.844428</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791185</td>\n",
       "      <td>0.730249</td>\n",
       "      <td>0.741912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.846187</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.800982</td>\n",
       "      <td>0.716108</td>\n",
       "      <td>0.737621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.849647</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.793737</td>\n",
       "      <td>0.710816</td>\n",
       "      <td>0.731474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.850948</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.795624</td>\n",
       "      <td>0.715134</td>\n",
       "      <td>0.734674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 10:15:33,398] Trial 93 finished with value: 0.7346740102785173 and parameters: {'learning_rate': 0.0002798432432643174, 'weight_decay': 0.01, 'adam_beta1': 0.91, 'warmup_steps': 43}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 94 with params: {'learning_rate': 0.00019762279855014495, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:30, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.660700</td>\n",
       "      <td>1.079506</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.435245</td>\n",
       "      <td>0.450782</td>\n",
       "      <td>0.429216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.624918</td>\n",
       "      <td>0.582424</td>\n",
       "      <td>0.587270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>1.040181</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.724125</td>\n",
       "      <td>0.643961</td>\n",
       "      <td>0.661885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>1.111995</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.779243</td>\n",
       "      <td>0.673266</td>\n",
       "      <td>0.706918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>1.172810</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.787934</td>\n",
       "      <td>0.709948</td>\n",
       "      <td>0.731637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>1.242729</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.811645</td>\n",
       "      <td>0.686371</td>\n",
       "      <td>0.720644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.821389</td>\n",
       "      <td>0.699842</td>\n",
       "      <td>0.732927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.294848</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.795891</td>\n",
       "      <td>0.720953</td>\n",
       "      <td>0.735898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>1.368158</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.811917</td>\n",
       "      <td>0.706980</td>\n",
       "      <td>0.736393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.414175</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.794040</td>\n",
       "      <td>0.710647</td>\n",
       "      <td>0.728418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.481635</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.786204</td>\n",
       "      <td>0.700017</td>\n",
       "      <td>0.722854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.487563</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.779781</td>\n",
       "      <td>0.722835</td>\n",
       "      <td>0.729603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.498854</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.773443</td>\n",
       "      <td>0.707534</td>\n",
       "      <td>0.723165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.507539</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.800864</td>\n",
       "      <td>0.727306</td>\n",
       "      <td>0.745454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.549639</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.799951</td>\n",
       "      <td>0.722597</td>\n",
       "      <td>0.738870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.660133</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.794826</td>\n",
       "      <td>0.695087</td>\n",
       "      <td>0.722997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.622060</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.802345</td>\n",
       "      <td>0.734748</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.650016</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.800629</td>\n",
       "      <td>0.708376</td>\n",
       "      <td>0.734092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.632902</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786355</td>\n",
       "      <td>0.720400</td>\n",
       "      <td>0.736215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.643463</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790836</td>\n",
       "      <td>0.723097</td>\n",
       "      <td>0.742118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.648063</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788853</td>\n",
       "      <td>0.723649</td>\n",
       "      <td>0.741489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.648058</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.811270</td>\n",
       "      <td>0.720082</td>\n",
       "      <td>0.745793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.635077</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.803709</td>\n",
       "      <td>0.718824</td>\n",
       "      <td>0.743981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.655950</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.791877</td>\n",
       "      <td>0.714034</td>\n",
       "      <td>0.736931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.677410</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.811869</td>\n",
       "      <td>0.712719</td>\n",
       "      <td>0.741151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.711270</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.810045</td>\n",
       "      <td>0.707637</td>\n",
       "      <td>0.736903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.713605</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.818145</td>\n",
       "      <td>0.722695</td>\n",
       "      <td>0.746867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.712124</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.798435</td>\n",
       "      <td>0.726951</td>\n",
       "      <td>0.743964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.715332</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.725278</td>\n",
       "      <td>0.743502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.719445</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.806412</td>\n",
       "      <td>0.725253</td>\n",
       "      <td>0.745836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 10:31:05,405] Trial 94 finished with value: 0.7458363684318783 and parameters: {'learning_rate': 0.00019762279855014495, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 27}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95 with params: {'learning_rate': 0.0004124376536120012, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:26 < 05:13, 16.74 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.153200</td>\n",
       "      <td>0.996783</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.539742</td>\n",
       "      <td>0.547583</td>\n",
       "      <td>0.535325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>1.113957</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.730606</td>\n",
       "      <td>0.668361</td>\n",
       "      <td>0.681407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>1.201008</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.763559</td>\n",
       "      <td>0.719148</td>\n",
       "      <td>0.724961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>1.286943</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.814428</td>\n",
       "      <td>0.709248</td>\n",
       "      <td>0.740514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.386710</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.787318</td>\n",
       "      <td>0.705264</td>\n",
       "      <td>0.723906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>1.397362</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.791344</td>\n",
       "      <td>0.724981</td>\n",
       "      <td>0.741554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.579736</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.781329</td>\n",
       "      <td>0.708812</td>\n",
       "      <td>0.721581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.572365</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.768636</td>\n",
       "      <td>0.712337</td>\n",
       "      <td>0.723323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.640229</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.805748</td>\n",
       "      <td>0.711240</td>\n",
       "      <td>0.739022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.607968</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.788791</td>\n",
       "      <td>0.699614</td>\n",
       "      <td>0.723255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.744300</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.771564</td>\n",
       "      <td>0.685628</td>\n",
       "      <td>0.702852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.794753</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.693156</td>\n",
       "      <td>0.720995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.780371</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.745442</td>\n",
       "      <td>0.680906</td>\n",
       "      <td>0.690397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.748045</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.819914</td>\n",
       "      <td>0.709240</td>\n",
       "      <td>0.743622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.947553</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.761423</td>\n",
       "      <td>0.692981</td>\n",
       "      <td>0.707881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.852592</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.764983</td>\n",
       "      <td>0.676158</td>\n",
       "      <td>0.702046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.879719</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.763059</td>\n",
       "      <td>0.704851</td>\n",
       "      <td>0.718752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.928563</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.776947</td>\n",
       "      <td>0.691445</td>\n",
       "      <td>0.714169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.963830</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.783148</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.712272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.961074</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.761129</td>\n",
       "      <td>0.686804</td>\n",
       "      <td>0.703573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 10:41:33,490] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 96 with params: {'learning_rate': 8.970210996083779e-05, 'weight_decay': 0.007, 'adam_beta1': 0.92, 'warmup_steps': 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:12 < 10:25, 16.79 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.356800</td>\n",
       "      <td>1.521881</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>0.343298</td>\n",
       "      <td>0.340915</td>\n",
       "      <td>0.324356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.940300</td>\n",
       "      <td>1.110990</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.450859</td>\n",
       "      <td>0.456351</td>\n",
       "      <td>0.440086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>1.021999</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.469984</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.475618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>1.000199</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.562554</td>\n",
       "      <td>0.532312</td>\n",
       "      <td>0.532249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>1.006847</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.644001</td>\n",
       "      <td>0.595173</td>\n",
       "      <td>0.606529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>1.041414</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.686305</td>\n",
       "      <td>0.605675</td>\n",
       "      <td>0.627579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>1.071859</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.739779</td>\n",
       "      <td>0.647599</td>\n",
       "      <td>0.670538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>1.096630</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.821595</td>\n",
       "      <td>0.706307</td>\n",
       "      <td>0.734609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>1.145681</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.789821</td>\n",
       "      <td>0.689744</td>\n",
       "      <td>0.716991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>1.208534</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.788903</td>\n",
       "      <td>0.699511</td>\n",
       "      <td>0.723039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 10:46:47,326] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 97 with params: {'learning_rate': 3.4845689761788494e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:35 < 12:59, 16.83 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.049600</td>\n",
       "      <td>2.418319</td>\n",
       "      <td>0.523373</td>\n",
       "      <td>0.188770</td>\n",
       "      <td>0.158132</td>\n",
       "      <td>0.139432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.935100</td>\n",
       "      <td>1.713585</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.311751</td>\n",
       "      <td>0.308381</td>\n",
       "      <td>0.290509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.333500</td>\n",
       "      <td>1.373857</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.376640</td>\n",
       "      <td>0.359947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>1.210205</td>\n",
       "      <td>0.751604</td>\n",
       "      <td>0.399432</td>\n",
       "      <td>0.410283</td>\n",
       "      <td>0.389561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.762800</td>\n",
       "      <td>1.121523</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.476387</td>\n",
       "      <td>0.451593</td>\n",
       "      <td>0.441936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 10:49:24,403] Trial 97 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98 with params: {'learning_rate': 0.00022179095726477724, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:30, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.579700</td>\n",
       "      <td>1.050529</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>0.435242</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.431063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.654095</td>\n",
       "      <td>0.600997</td>\n",
       "      <td>0.611231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>1.063185</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.743447</td>\n",
       "      <td>0.661397</td>\n",
       "      <td>0.683646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>1.124063</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.780830</td>\n",
       "      <td>0.671743</td>\n",
       "      <td>0.706802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>1.181932</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.797703</td>\n",
       "      <td>0.719527</td>\n",
       "      <td>0.740495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>1.234265</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.809460</td>\n",
       "      <td>0.695386</td>\n",
       "      <td>0.727435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>1.295508</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.812835</td>\n",
       "      <td>0.706990</td>\n",
       "      <td>0.734767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>1.332692</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.820847</td>\n",
       "      <td>0.735032</td>\n",
       "      <td>0.756307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.345756</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776069</td>\n",
       "      <td>0.735954</td>\n",
       "      <td>0.741724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.378236</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.819436</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.751827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.472208</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.813396</td>\n",
       "      <td>0.717121</td>\n",
       "      <td>0.742144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.423745</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>0.733955</td>\n",
       "      <td>0.751592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.484263</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.796384</td>\n",
       "      <td>0.728774</td>\n",
       "      <td>0.747075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.460898</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.799916</td>\n",
       "      <td>0.736589</td>\n",
       "      <td>0.753611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.542235</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.800427</td>\n",
       "      <td>0.728059</td>\n",
       "      <td>0.748390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.555818</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>0.718167</td>\n",
       "      <td>0.736910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.615128</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.804904</td>\n",
       "      <td>0.719306</td>\n",
       "      <td>0.742476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.640818</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.802367</td>\n",
       "      <td>0.714040</td>\n",
       "      <td>0.738348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.633101</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.806936</td>\n",
       "      <td>0.731945</td>\n",
       "      <td>0.752516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.691662</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.714085</td>\n",
       "      <td>0.741913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.674283</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.801642</td>\n",
       "      <td>0.712456</td>\n",
       "      <td>0.737091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.657843</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.799231</td>\n",
       "      <td>0.729858</td>\n",
       "      <td>0.747071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.672302</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.810077</td>\n",
       "      <td>0.734475</td>\n",
       "      <td>0.753173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.706168</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.727021</td>\n",
       "      <td>0.748788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.710309</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.807493</td>\n",
       "      <td>0.725525</td>\n",
       "      <td>0.746612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.716473</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.805354</td>\n",
       "      <td>0.720667</td>\n",
       "      <td>0.744073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.736948</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.809732</td>\n",
       "      <td>0.722711</td>\n",
       "      <td>0.745774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.710292</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.805806</td>\n",
       "      <td>0.727157</td>\n",
       "      <td>0.747596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.728227</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.800865</td>\n",
       "      <td>0.728494</td>\n",
       "      <td>0.745523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.733822</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.726527</td>\n",
       "      <td>0.744071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 11:04:56,444] Trial 98 finished with value: 0.744070793918236 and parameters: {'learning_rate': 0.00022179095726477724, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 31}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 99 with params: {'learning_rate': 9.621525134665849e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.284000</td>\n",
       "      <td>1.454194</td>\n",
       "      <td>0.724106</td>\n",
       "      <td>0.347730</td>\n",
       "      <td>0.357266</td>\n",
       "      <td>0.337066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>1.092684</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.465234</td>\n",
       "      <td>0.461555</td>\n",
       "      <td>0.446246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>1.007627</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.468647</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.476333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>1.011947</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.609580</td>\n",
       "      <td>0.548525</td>\n",
       "      <td>0.558136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>1.017417</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.684694</td>\n",
       "      <td>0.615676</td>\n",
       "      <td>0.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>1.053399</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.687275</td>\n",
       "      <td>0.606188</td>\n",
       "      <td>0.628115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>1.091167</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.768513</td>\n",
       "      <td>0.668692</td>\n",
       "      <td>0.694738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>1.118863</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.811666</td>\n",
       "      <td>0.715506</td>\n",
       "      <td>0.739480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>1.168545</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.799597</td>\n",
       "      <td>0.690048</td>\n",
       "      <td>0.722210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>1.225199</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.800968</td>\n",
       "      <td>0.703458</td>\n",
       "      <td>0.732130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.213941</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.775134</td>\n",
       "      <td>0.718686</td>\n",
       "      <td>0.730759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.252535</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.800567</td>\n",
       "      <td>0.750250</td>\n",
       "      <td>0.753914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.278804</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.788709</td>\n",
       "      <td>0.715731</td>\n",
       "      <td>0.731489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.285056</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.787449</td>\n",
       "      <td>0.723267</td>\n",
       "      <td>0.741129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.357406</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780890</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.737253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.357971</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.798218</td>\n",
       "      <td>0.735789</td>\n",
       "      <td>0.749448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.363832</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.795751</td>\n",
       "      <td>0.718570</td>\n",
       "      <td>0.734919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.393348</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.777846</td>\n",
       "      <td>0.732223</td>\n",
       "      <td>0.736159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.363224</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.770007</td>\n",
       "      <td>0.720597</td>\n",
       "      <td>0.729244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.420356</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.786314</td>\n",
       "      <td>0.738086</td>\n",
       "      <td>0.746484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.407947</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.784592</td>\n",
       "      <td>0.742830</td>\n",
       "      <td>0.747374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.432540</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.787377</td>\n",
       "      <td>0.734506</td>\n",
       "      <td>0.744721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.452167</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.789850</td>\n",
       "      <td>0.741754</td>\n",
       "      <td>0.749360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.458139</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.779683</td>\n",
       "      <td>0.729910</td>\n",
       "      <td>0.738492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.454908</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.732137</td>\n",
       "      <td>0.746182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.465846</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.788253</td>\n",
       "      <td>0.739630</td>\n",
       "      <td>0.748854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.475483</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783787</td>\n",
       "      <td>0.741775</td>\n",
       "      <td>0.746874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.476767</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791699</td>\n",
       "      <td>0.745121</td>\n",
       "      <td>0.753564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.481196</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.784320</td>\n",
       "      <td>0.742811</td>\n",
       "      <td>0.747935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.482828</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783262</td>\n",
       "      <td>0.741803</td>\n",
       "      <td>0.746842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 11:20:45,399] Trial 99 finished with value: 0.7468424668030823 and parameters: {'learning_rate': 9.621525134665849e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 33}. Best is trial 87 with value: 0.7545251683099805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 with params: {'learning_rate': 0.00026326183503997335, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:26 < 05:13, 16.75 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.444700</td>\n",
       "      <td>1.023367</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.470896</td>\n",
       "      <td>0.474784</td>\n",
       "      <td>0.458267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>1.027561</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.709766</td>\n",
       "      <td>0.626121</td>\n",
       "      <td>0.646624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>1.109384</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.753812</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>0.696852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>1.202097</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.810267</td>\n",
       "      <td>0.692666</td>\n",
       "      <td>0.727531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>1.265803</td>\n",
       "      <td>0.802933</td>\n",
       "      <td>0.810728</td>\n",
       "      <td>0.714261</td>\n",
       "      <td>0.743046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>1.280068</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.813930</td>\n",
       "      <td>0.726439</td>\n",
       "      <td>0.752483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>1.392788</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.793661</td>\n",
       "      <td>0.700277</td>\n",
       "      <td>0.726344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.394402</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.795883</td>\n",
       "      <td>0.732184</td>\n",
       "      <td>0.746394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.455548</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.779254</td>\n",
       "      <td>0.729756</td>\n",
       "      <td>0.739187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.471101</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.796726</td>\n",
       "      <td>0.724830</td>\n",
       "      <td>0.741566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.581398</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.769477</td>\n",
       "      <td>0.725092</td>\n",
       "      <td>0.730355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.548893</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.782778</td>\n",
       "      <td>0.733650</td>\n",
       "      <td>0.744118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.552203</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784104</td>\n",
       "      <td>0.694058</td>\n",
       "      <td>0.721191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.565352</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.780992</td>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.733633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.781065</td>\n",
       "      <td>0.700225</td>\n",
       "      <td>0.722444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.626397</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.787904</td>\n",
       "      <td>0.699673</td>\n",
       "      <td>0.722912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.645442</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.775357</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.719370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.762913</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.771344</td>\n",
       "      <td>0.698728</td>\n",
       "      <td>0.718457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.751164</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.770044</td>\n",
       "      <td>0.712509</td>\n",
       "      <td>0.726578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.753315</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.795382</td>\n",
       "      <td>0.701321</td>\n",
       "      <td>0.728257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 11:31:13,110] Trial 100 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 101 with params: {'learning_rate': 0.00010218895538078328, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:32, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.251300</td>\n",
       "      <td>1.417891</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.368436</td>\n",
       "      <td>0.366286</td>\n",
       "      <td>0.341517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>1.082425</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.455463</td>\n",
       "      <td>0.462431</td>\n",
       "      <td>0.442617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>1.004512</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.555639</td>\n",
       "      <td>0.518153</td>\n",
       "      <td>0.514995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>1.017374</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.647803</td>\n",
       "      <td>0.578166</td>\n",
       "      <td>0.597814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>1.027956</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.618063</td>\n",
       "      <td>0.637252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>1.070933</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.710924</td>\n",
       "      <td>0.624744</td>\n",
       "      <td>0.650483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>1.109393</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.774606</td>\n",
       "      <td>0.682293</td>\n",
       "      <td>0.706236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>1.142758</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.805598</td>\n",
       "      <td>0.709391</td>\n",
       "      <td>0.732277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>1.178429</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.797319</td>\n",
       "      <td>0.694007</td>\n",
       "      <td>0.722822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>1.238728</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.805281</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.735131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>1.220223</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777818</td>\n",
       "      <td>0.713792</td>\n",
       "      <td>0.729242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>1.257764</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.807275</td>\n",
       "      <td>0.749315</td>\n",
       "      <td>0.757873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.291024</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.797884</td>\n",
       "      <td>0.725924</td>\n",
       "      <td>0.741976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.289384</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.799197</td>\n",
       "      <td>0.725188</td>\n",
       "      <td>0.746056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.352341</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.790968</td>\n",
       "      <td>0.728502</td>\n",
       "      <td>0.744472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.368149</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.792046</td>\n",
       "      <td>0.734447</td>\n",
       "      <td>0.750507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.391297</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.800631</td>\n",
       "      <td>0.734243</td>\n",
       "      <td>0.748341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.425959</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.781214</td>\n",
       "      <td>0.742142</td>\n",
       "      <td>0.744136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.388806</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.786637</td>\n",
       "      <td>0.734661</td>\n",
       "      <td>0.746308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.434141</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787186</td>\n",
       "      <td>0.731902</td>\n",
       "      <td>0.744705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.422805</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.787844</td>\n",
       "      <td>0.735448</td>\n",
       "      <td>0.748423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.442378</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.783220</td>\n",
       "      <td>0.729465</td>\n",
       "      <td>0.741991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.470176</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.767896</td>\n",
       "      <td>0.728539</td>\n",
       "      <td>0.736893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.474475</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783636</td>\n",
       "      <td>0.730512</td>\n",
       "      <td>0.744145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.476326</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.805289</td>\n",
       "      <td>0.736187</td>\n",
       "      <td>0.753044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.501616</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.791944</td>\n",
       "      <td>0.743620</td>\n",
       "      <td>0.754308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.502968</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>0.745643</td>\n",
       "      <td>0.757385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.502028</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799443</td>\n",
       "      <td>0.740906</td>\n",
       "      <td>0.753970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.508258</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.794726</td>\n",
       "      <td>0.741253</td>\n",
       "      <td>0.752873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.509766</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797745</td>\n",
       "      <td>0.747919</td>\n",
       "      <td>0.758038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 11:46:47,462] Trial 101 finished with value: 0.7580381076828203 and parameters: {'learning_rate': 0.00010218895538078328, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 42}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 102 with params: {'learning_rate': 0.00024303206858875134, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:25, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.527500</td>\n",
       "      <td>1.033849</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.439862</td>\n",
       "      <td>0.455737</td>\n",
       "      <td>0.434290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.998491</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.649648</td>\n",
       "      <td>0.606398</td>\n",
       "      <td>0.612449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>1.082780</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.759557</td>\n",
       "      <td>0.671713</td>\n",
       "      <td>0.694357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>1.153161</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.796178</td>\n",
       "      <td>0.685711</td>\n",
       "      <td>0.719765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>1.239615</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.804397</td>\n",
       "      <td>0.708587</td>\n",
       "      <td>0.733167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>1.290254</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.798797</td>\n",
       "      <td>0.678787</td>\n",
       "      <td>0.710841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.336750</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799026</td>\n",
       "      <td>0.706038</td>\n",
       "      <td>0.730924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>1.343703</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.800509</td>\n",
       "      <td>0.729245</td>\n",
       "      <td>0.745404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.450001</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.802975</td>\n",
       "      <td>0.722484</td>\n",
       "      <td>0.742353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.472208</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.719429</td>\n",
       "      <td>0.743440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.505325</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792434</td>\n",
       "      <td>0.738301</td>\n",
       "      <td>0.749212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.503239</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786209</td>\n",
       "      <td>0.720757</td>\n",
       "      <td>0.735839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.523697</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.803148</td>\n",
       "      <td>0.715680</td>\n",
       "      <td>0.736672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.437473</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.810598</td>\n",
       "      <td>0.731120</td>\n",
       "      <td>0.750355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.580839</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>0.727242</td>\n",
       "      <td>0.747051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.556995</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.794463</td>\n",
       "      <td>0.712478</td>\n",
       "      <td>0.735481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.612949</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.787837</td>\n",
       "      <td>0.715609</td>\n",
       "      <td>0.732345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.640194</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.811774</td>\n",
       "      <td>0.712136</td>\n",
       "      <td>0.736650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.627375</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.801420</td>\n",
       "      <td>0.718572</td>\n",
       "      <td>0.740142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.648589</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.806697</td>\n",
       "      <td>0.725726</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.698140</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.800897</td>\n",
       "      <td>0.707260</td>\n",
       "      <td>0.733747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.691070</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.813418</td>\n",
       "      <td>0.721810</td>\n",
       "      <td>0.747018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.727141</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.808902</td>\n",
       "      <td>0.724182</td>\n",
       "      <td>0.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.739815</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.808886</td>\n",
       "      <td>0.724302</td>\n",
       "      <td>0.746491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.754649</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.806299</td>\n",
       "      <td>0.725205</td>\n",
       "      <td>0.745777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.776562</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>0.723155</td>\n",
       "      <td>0.748701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.742226</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.812372</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.747168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.759580</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.806843</td>\n",
       "      <td>0.722604</td>\n",
       "      <td>0.744688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.770574</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.805125</td>\n",
       "      <td>0.722527</td>\n",
       "      <td>0.744880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.770747</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.804474</td>\n",
       "      <td>0.722527</td>\n",
       "      <td>0.744558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:02:15,244] Trial 102 finished with value: 0.7445584528490106 and parameters: {'learning_rate': 0.00024303206858875134, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 38}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 103 with params: {'learning_rate': 7.379909560782258e-05, 'weight_decay': 0.005, 'adam_beta1': 0.9, 'warmup_steps': 39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:41 < 13:28, 16.24 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.523900</td>\n",
       "      <td>1.699540</td>\n",
       "      <td>0.681027</td>\n",
       "      <td>0.312070</td>\n",
       "      <td>0.309164</td>\n",
       "      <td>0.289607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.124800</td>\n",
       "      <td>1.185884</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.422866</td>\n",
       "      <td>0.434872</td>\n",
       "      <td>0.417491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>1.059815</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.474181</td>\n",
       "      <td>0.481231</td>\n",
       "      <td>0.463082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.415100</td>\n",
       "      <td>1.007991</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.506081</td>\n",
       "      <td>0.502727</td>\n",
       "      <td>0.496671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.992563</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.623327</td>\n",
       "      <td>0.571505</td>\n",
       "      <td>0.583197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:04:57,910] Trial 103 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 104 with params: {'learning_rate': 0.00025631562696183294, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.459200</td>\n",
       "      <td>1.027214</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.465460</td>\n",
       "      <td>0.472995</td>\n",
       "      <td>0.454879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>1.015119</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.686938</td>\n",
       "      <td>0.623727</td>\n",
       "      <td>0.638254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>1.096897</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.751518</td>\n",
       "      <td>0.678176</td>\n",
       "      <td>0.696479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>1.167523</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.733185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.279468</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.781993</td>\n",
       "      <td>0.699189</td>\n",
       "      <td>0.721486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.302098</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.791836</td>\n",
       "      <td>0.703334</td>\n",
       "      <td>0.728698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.388248</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.781692</td>\n",
       "      <td>0.706298</td>\n",
       "      <td>0.723145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.436103</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.816443</td>\n",
       "      <td>0.737692</td>\n",
       "      <td>0.760072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.479326</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787548</td>\n",
       "      <td>0.719927</td>\n",
       "      <td>0.735494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.483114</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.722811</td>\n",
       "      <td>0.737121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.550474</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.766677</td>\n",
       "      <td>0.735640</td>\n",
       "      <td>0.733944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.585148</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.792604</td>\n",
       "      <td>0.711377</td>\n",
       "      <td>0.731781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.576985</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.779265</td>\n",
       "      <td>0.720859</td>\n",
       "      <td>0.732805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.574041</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.799119</td>\n",
       "      <td>0.720891</td>\n",
       "      <td>0.740927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.626048</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.798935</td>\n",
       "      <td>0.710085</td>\n",
       "      <td>0.733340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.695658</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.760777</td>\n",
       "      <td>0.718687</td>\n",
       "      <td>0.723767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.692413</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.774693</td>\n",
       "      <td>0.728225</td>\n",
       "      <td>0.731896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.705038</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.791332</td>\n",
       "      <td>0.704968</td>\n",
       "      <td>0.725993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.717062</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.781507</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.732134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.742325</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.796624</td>\n",
       "      <td>0.713996</td>\n",
       "      <td>0.734931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.769762</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.778345</td>\n",
       "      <td>0.715968</td>\n",
       "      <td>0.730380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.821869</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.795165</td>\n",
       "      <td>0.696534</td>\n",
       "      <td>0.724686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.820473</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.786352</td>\n",
       "      <td>0.702178</td>\n",
       "      <td>0.725987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.783557</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.807447</td>\n",
       "      <td>0.708629</td>\n",
       "      <td>0.736237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.828015</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.802001</td>\n",
       "      <td>0.711749</td>\n",
       "      <td>0.736248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.817224</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.804528</td>\n",
       "      <td>0.715659</td>\n",
       "      <td>0.740701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.856348</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.802164</td>\n",
       "      <td>0.702702</td>\n",
       "      <td>0.732499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.830168</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.788554</td>\n",
       "      <td>0.713564</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.838859</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.787589</td>\n",
       "      <td>0.713151</td>\n",
       "      <td>0.731837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.836697</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.790475</td>\n",
       "      <td>0.716520</td>\n",
       "      <td>0.735681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:20:35,924] Trial 104 finished with value: 0.7356811974104959 and parameters: {'learning_rate': 0.00025631562696183294, 'weight_decay': 0.009000000000000001, 'adam_beta1': 0.9, 'warmup_steps': 26}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 105 with params: {'learning_rate': 8.855441380475776e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:32, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.375600</td>\n",
       "      <td>1.533528</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.367117</td>\n",
       "      <td>0.350204</td>\n",
       "      <td>0.335612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.953600</td>\n",
       "      <td>1.123244</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.442207</td>\n",
       "      <td>0.451652</td>\n",
       "      <td>0.432246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>1.027614</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.469611</td>\n",
       "      <td>0.490621</td>\n",
       "      <td>0.475530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>1.005554</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.588348</td>\n",
       "      <td>0.535626</td>\n",
       "      <td>0.540686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>1.009932</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.650871</td>\n",
       "      <td>0.596503</td>\n",
       "      <td>0.609805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>1.036875</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.686572</td>\n",
       "      <td>0.609529</td>\n",
       "      <td>0.631236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>1.073570</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.741892</td>\n",
       "      <td>0.651743</td>\n",
       "      <td>0.674138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>1.092337</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.813440</td>\n",
       "      <td>0.700068</td>\n",
       "      <td>0.731670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>1.144837</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.798299</td>\n",
       "      <td>0.682338</td>\n",
       "      <td>0.714506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>1.196302</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.796678</td>\n",
       "      <td>0.697251</td>\n",
       "      <td>0.726811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>1.177175</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.786584</td>\n",
       "      <td>0.706891</td>\n",
       "      <td>0.728188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>1.226278</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.802787</td>\n",
       "      <td>0.739793</td>\n",
       "      <td>0.752862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.241109</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.705696</td>\n",
       "      <td>0.725499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>1.256121</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.807470</td>\n",
       "      <td>0.708999</td>\n",
       "      <td>0.738508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>1.311531</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.784736</td>\n",
       "      <td>0.731546</td>\n",
       "      <td>0.744006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.327164</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786947</td>\n",
       "      <td>0.714166</td>\n",
       "      <td>0.736024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.349969</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.788210</td>\n",
       "      <td>0.707962</td>\n",
       "      <td>0.730397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.363085</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.772522</td>\n",
       "      <td>0.717070</td>\n",
       "      <td>0.728879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.342031</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.789487</td>\n",
       "      <td>0.733744</td>\n",
       "      <td>0.750099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.381501</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.794557</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.745214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.385532</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.789647</td>\n",
       "      <td>0.724250</td>\n",
       "      <td>0.741108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.409127</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785249</td>\n",
       "      <td>0.718870</td>\n",
       "      <td>0.734098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.425755</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.766892</td>\n",
       "      <td>0.719931</td>\n",
       "      <td>0.729451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.441404</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.777414</td>\n",
       "      <td>0.734373</td>\n",
       "      <td>0.743314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>1.437758</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.803090</td>\n",
       "      <td>0.728532</td>\n",
       "      <td>0.746859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.452204</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786012</td>\n",
       "      <td>0.731625</td>\n",
       "      <td>0.744432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.452327</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.786987</td>\n",
       "      <td>0.740020</td>\n",
       "      <td>0.750237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.451252</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.792797</td>\n",
       "      <td>0.738471</td>\n",
       "      <td>0.751272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.455629</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.786808</td>\n",
       "      <td>0.741115</td>\n",
       "      <td>0.752156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.458320</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.787330</td>\n",
       "      <td>0.740900</td>\n",
       "      <td>0.751804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:36:10,527] Trial 105 finished with value: 0.7518035569469373 and parameters: {'learning_rate': 8.855441380475776e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 42}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106 with params: {'learning_rate': 2.8009890789134495e-05, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:27 < 12:18, 17.76 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.196400</td>\n",
       "      <td>2.638755</td>\n",
       "      <td>0.485793</td>\n",
       "      <td>0.103856</td>\n",
       "      <td>0.135388</td>\n",
       "      <td>0.111481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.190800</td>\n",
       "      <td>1.941070</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>0.270340</td>\n",
       "      <td>0.248286</td>\n",
       "      <td>0.226817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.589900</td>\n",
       "      <td>1.552370</td>\n",
       "      <td>0.706691</td>\n",
       "      <td>0.351450</td>\n",
       "      <td>0.334729</td>\n",
       "      <td>0.316294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.213500</td>\n",
       "      <td>1.337207</td>\n",
       "      <td>0.731439</td>\n",
       "      <td>0.365635</td>\n",
       "      <td>0.379580</td>\n",
       "      <td>0.359520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.963400</td>\n",
       "      <td>1.216431</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.396817</td>\n",
       "      <td>0.407624</td>\n",
       "      <td>0.388056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:38:39,446] Trial 106 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 107 with params: {'learning_rate': 6.46317968978187e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:30, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.629000</td>\n",
       "      <td>1.824155</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.290091</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.254138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.254700</td>\n",
       "      <td>1.246247</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.402842</td>\n",
       "      <td>0.406547</td>\n",
       "      <td>0.385753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.744200</td>\n",
       "      <td>1.088126</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.456466</td>\n",
       "      <td>0.467873</td>\n",
       "      <td>0.446275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>1.027558</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.480230</td>\n",
       "      <td>0.479726</td>\n",
       "      <td>0.471760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.997759</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.578794</td>\n",
       "      <td>0.522568</td>\n",
       "      <td>0.525765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>1.002187</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.624253</td>\n",
       "      <td>0.570210</td>\n",
       "      <td>0.581059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>1.030690</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.675512</td>\n",
       "      <td>0.604941</td>\n",
       "      <td>0.622294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>1.031091</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.672539</td>\n",
       "      <td>0.610595</td>\n",
       "      <td>0.626002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>1.068186</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.683180</td>\n",
       "      <td>0.611101</td>\n",
       "      <td>0.626562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>1.117634</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.739543</td>\n",
       "      <td>0.652383</td>\n",
       "      <td>0.672937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>1.098567</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.755210</td>\n",
       "      <td>0.674050</td>\n",
       "      <td>0.692522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>1.130862</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.771666</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>0.710237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>1.149179</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.686291</td>\n",
       "      <td>0.700159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>1.157549</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.772129</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.711952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>1.199456</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.796972</td>\n",
       "      <td>0.712340</td>\n",
       "      <td>0.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>1.224696</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783355</td>\n",
       "      <td>0.704662</td>\n",
       "      <td>0.726184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>1.216525</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.784632</td>\n",
       "      <td>0.708081</td>\n",
       "      <td>0.725890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.238801</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783621</td>\n",
       "      <td>0.709155</td>\n",
       "      <td>0.726858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>1.231266</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.791896</td>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.733608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.247037</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791505</td>\n",
       "      <td>0.722718</td>\n",
       "      <td>0.738184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>1.262173</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.789210</td>\n",
       "      <td>0.736708</td>\n",
       "      <td>0.745358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>1.291886</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790761</td>\n",
       "      <td>0.725544</td>\n",
       "      <td>0.740886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.294113</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.790188</td>\n",
       "      <td>0.727838</td>\n",
       "      <td>0.742959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>1.306884</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.715509</td>\n",
       "      <td>0.736437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>1.318750</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788420</td>\n",
       "      <td>0.708052</td>\n",
       "      <td>0.726689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>1.323273</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.792911</td>\n",
       "      <td>0.714785</td>\n",
       "      <td>0.734632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.321008</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.791109</td>\n",
       "      <td>0.727533</td>\n",
       "      <td>0.741435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.322055</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.792582</td>\n",
       "      <td>0.715824</td>\n",
       "      <td>0.735823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.326130</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.791259</td>\n",
       "      <td>0.724315</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>1.326356</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790898</td>\n",
       "      <td>0.724211</td>\n",
       "      <td>0.740566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:54:11,749] Trial 107 finished with value: 0.740566443666514 and parameters: {'learning_rate': 6.46317968978187e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 38}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 108 with params: {'learning_rate': 0.00011423850364123761, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:14, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.162300</td>\n",
       "      <td>1.341676</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.382853</td>\n",
       "      <td>0.386392</td>\n",
       "      <td>0.363145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.740300</td>\n",
       "      <td>1.057674</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.468312</td>\n",
       "      <td>0.471582</td>\n",
       "      <td>0.462032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.994029</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.597856</td>\n",
       "      <td>0.544703</td>\n",
       "      <td>0.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>1.021638</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.653758</td>\n",
       "      <td>0.590917</td>\n",
       "      <td>0.609227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>1.038737</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.684728</td>\n",
       "      <td>0.615217</td>\n",
       "      <td>0.631405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>1.088054</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.760575</td>\n",
       "      <td>0.661148</td>\n",
       "      <td>0.690202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>1.146855</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.789139</td>\n",
       "      <td>0.677963</td>\n",
       "      <td>0.707759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>1.167429</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.797934</td>\n",
       "      <td>0.715129</td>\n",
       "      <td>0.734155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>1.207072</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.798291</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.725511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>1.269243</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.816475</td>\n",
       "      <td>0.710478</td>\n",
       "      <td>0.737007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>1.275737</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.779241</td>\n",
       "      <td>0.720849</td>\n",
       "      <td>0.731689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.300311</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.792594</td>\n",
       "      <td>0.743799</td>\n",
       "      <td>0.748468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>1.350155</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.789965</td>\n",
       "      <td>0.719239</td>\n",
       "      <td>0.733685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.346938</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.807019</td>\n",
       "      <td>0.739801</td>\n",
       "      <td>0.757689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>1.408023</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.799022</td>\n",
       "      <td>0.731057</td>\n",
       "      <td>0.746104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.415612</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.807708</td>\n",
       "      <td>0.721074</td>\n",
       "      <td>0.742364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.431822</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.815709</td>\n",
       "      <td>0.719273</td>\n",
       "      <td>0.742060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.460266</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>0.738869</td>\n",
       "      <td>0.744632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.419495</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.781190</td>\n",
       "      <td>0.738571</td>\n",
       "      <td>0.745552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.473781</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.724725</td>\n",
       "      <td>0.741451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.473870</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786427</td>\n",
       "      <td>0.747463</td>\n",
       "      <td>0.750759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.502624</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.798067</td>\n",
       "      <td>0.728081</td>\n",
       "      <td>0.744772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.516462</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.785723</td>\n",
       "      <td>0.734352</td>\n",
       "      <td>0.742672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.511044</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.802166</td>\n",
       "      <td>0.732078</td>\n",
       "      <td>0.748027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.537161</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.807395</td>\n",
       "      <td>0.729771</td>\n",
       "      <td>0.751026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.549951</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.743512</td>\n",
       "      <td>0.755498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.543385</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.792185</td>\n",
       "      <td>0.737093</td>\n",
       "      <td>0.748973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.547681</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.806521</td>\n",
       "      <td>0.742531</td>\n",
       "      <td>0.758718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.552182</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.793557</td>\n",
       "      <td>0.741310</td>\n",
       "      <td>0.752180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.554468</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.800789</td>\n",
       "      <td>0.743913</td>\n",
       "      <td>0.757732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 13:09:28,448] Trial 108 finished with value: 0.7577323391778955 and parameters: {'learning_rate': 0.00011423850364123761, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 46}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 109 with params: {'learning_rate': 0.00018609888492482365, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 16:03, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.751000</td>\n",
       "      <td>1.104338</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.425990</td>\n",
       "      <td>0.446392</td>\n",
       "      <td>0.424020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.990570</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.575621</td>\n",
       "      <td>0.535414</td>\n",
       "      <td>0.538391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>1.015573</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.731516</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>0.676905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>1.091683</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.761416</td>\n",
       "      <td>0.667155</td>\n",
       "      <td>0.698591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>1.152814</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.702835</td>\n",
       "      <td>0.728374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>1.215136</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.797281</td>\n",
       "      <td>0.670274</td>\n",
       "      <td>0.709720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.282920</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.772991</td>\n",
       "      <td>0.705772</td>\n",
       "      <td>0.720394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>1.280434</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.810101</td>\n",
       "      <td>0.720838</td>\n",
       "      <td>0.741978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>1.335141</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.793576</td>\n",
       "      <td>0.709409</td>\n",
       "      <td>0.733371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.414489</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.809540</td>\n",
       "      <td>0.718992</td>\n",
       "      <td>0.743521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.446065</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.786620</td>\n",
       "      <td>0.717685</td>\n",
       "      <td>0.731423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.520824</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.792287</td>\n",
       "      <td>0.738932</td>\n",
       "      <td>0.749910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.497484</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.776588</td>\n",
       "      <td>0.725259</td>\n",
       "      <td>0.732108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.490212</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.804506</td>\n",
       "      <td>0.718660</td>\n",
       "      <td>0.741999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.559290</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.793391</td>\n",
       "      <td>0.717378</td>\n",
       "      <td>0.736066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.563671</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.813962</td>\n",
       "      <td>0.709361</td>\n",
       "      <td>0.734447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.581756</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.785861</td>\n",
       "      <td>0.725443</td>\n",
       "      <td>0.737892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.646170</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.796049</td>\n",
       "      <td>0.704205</td>\n",
       "      <td>0.725626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.598403</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796425</td>\n",
       "      <td>0.708692</td>\n",
       "      <td>0.730909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.651395</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.814241</td>\n",
       "      <td>0.719628</td>\n",
       "      <td>0.744084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.656264</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.784374</td>\n",
       "      <td>0.722842</td>\n",
       "      <td>0.734110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.632124</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.815477</td>\n",
       "      <td>0.720954</td>\n",
       "      <td>0.746689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.676850</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>0.727825</td>\n",
       "      <td>0.746761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.661947</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.804085</td>\n",
       "      <td>0.710149</td>\n",
       "      <td>0.737059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.700275</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.816052</td>\n",
       "      <td>0.706084</td>\n",
       "      <td>0.737553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.702660</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.805124</td>\n",
       "      <td>0.718633</td>\n",
       "      <td>0.741758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.726195</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.802430</td>\n",
       "      <td>0.712717</td>\n",
       "      <td>0.737220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.713817</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.804860</td>\n",
       "      <td>0.717144</td>\n",
       "      <td>0.741423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.710058</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.799636</td>\n",
       "      <td>0.718401</td>\n",
       "      <td>0.739887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.717616</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.800331</td>\n",
       "      <td>0.718401</td>\n",
       "      <td>0.740086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 13:25:33,952] Trial 109 finished with value: 0.7400863228030755 and parameters: {'learning_rate': 0.00018609888492482365, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 47}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 110 with params: {'learning_rate': 5.7545494777844725e-05, 'weight_decay': 0.008, 'adam_beta1': 0.91, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:40 < 13:21, 16.38 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.742200</td>\n",
       "      <td>1.952346</td>\n",
       "      <td>0.615032</td>\n",
       "      <td>0.264737</td>\n",
       "      <td>0.247092</td>\n",
       "      <td>0.230439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.388100</td>\n",
       "      <td>1.321236</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.377772</td>\n",
       "      <td>0.389919</td>\n",
       "      <td>0.368616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.844700</td>\n",
       "      <td>1.126132</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.457473</td>\n",
       "      <td>0.453527</td>\n",
       "      <td>0.436628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>1.048735</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.481864</td>\n",
       "      <td>0.478101</td>\n",
       "      <td>0.471672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.014299</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.507041</td>\n",
       "      <td>0.499725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 13:28:15,274] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 111 with params: {'learning_rate': 0.0002181832842611734, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:44, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.615300</td>\n",
       "      <td>1.059808</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.439039</td>\n",
       "      <td>0.451457</td>\n",
       "      <td>0.429190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.983736</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.622593</td>\n",
       "      <td>0.578327</td>\n",
       "      <td>0.584337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>1.050896</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.746515</td>\n",
       "      <td>0.666515</td>\n",
       "      <td>0.686418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>1.138328</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.785986</td>\n",
       "      <td>0.674874</td>\n",
       "      <td>0.709667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>1.173374</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.801540</td>\n",
       "      <td>0.721127</td>\n",
       "      <td>0.743811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>1.245914</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782746</td>\n",
       "      <td>0.672913</td>\n",
       "      <td>0.706138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>1.306219</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.822557</td>\n",
       "      <td>0.710793</td>\n",
       "      <td>0.742431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>1.345420</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.697598</td>\n",
       "      <td>0.726068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.363331</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.722243</td>\n",
       "      <td>0.740238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.401254</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.815626</td>\n",
       "      <td>0.727664</td>\n",
       "      <td>0.749829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.490004</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.793244</td>\n",
       "      <td>0.725364</td>\n",
       "      <td>0.743230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.475453</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.791586</td>\n",
       "      <td>0.724942</td>\n",
       "      <td>0.738627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.536437</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>0.717444</td>\n",
       "      <td>0.733588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.512843</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.824997</td>\n",
       "      <td>0.732750</td>\n",
       "      <td>0.759722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.534915</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.801048</td>\n",
       "      <td>0.738444</td>\n",
       "      <td>0.752423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.570061</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.810365</td>\n",
       "      <td>0.718369</td>\n",
       "      <td>0.743510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.629804</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.802165</td>\n",
       "      <td>0.724178</td>\n",
       "      <td>0.745433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.639340</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.762999</td>\n",
       "      <td>0.701364</td>\n",
       "      <td>0.715246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.611340</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.805678</td>\n",
       "      <td>0.731053</td>\n",
       "      <td>0.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.684919</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.803800</td>\n",
       "      <td>0.728021</td>\n",
       "      <td>0.750448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.665348</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.795711</td>\n",
       "      <td>0.718876</td>\n",
       "      <td>0.741033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.681082</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.818209</td>\n",
       "      <td>0.716491</td>\n",
       "      <td>0.747881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.712859</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.814010</td>\n",
       "      <td>0.716099</td>\n",
       "      <td>0.743662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.733236</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.815420</td>\n",
       "      <td>0.722669</td>\n",
       "      <td>0.748115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.738556</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.806810</td>\n",
       "      <td>0.721268</td>\n",
       "      <td>0.745512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.774078</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.805441</td>\n",
       "      <td>0.717077</td>\n",
       "      <td>0.742737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.776438</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.806093</td>\n",
       "      <td>0.719773</td>\n",
       "      <td>0.742916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.750930</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.809406</td>\n",
       "      <td>0.719605</td>\n",
       "      <td>0.743218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.756482</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799876</td>\n",
       "      <td>0.719783</td>\n",
       "      <td>0.740843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.763596</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.802553</td>\n",
       "      <td>0.719962</td>\n",
       "      <td>0.742289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 13:44:01,908] Trial 111 finished with value: 0.7422891395603033 and parameters: {'learning_rate': 0.0002181832842611734, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 37}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 112 with params: {'learning_rate': 9.176214230786556e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.355500</td>\n",
       "      <td>1.508245</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.365123</td>\n",
       "      <td>0.353124</td>\n",
       "      <td>0.336734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.113992</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.442220</td>\n",
       "      <td>0.451310</td>\n",
       "      <td>0.432263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>1.021599</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.491249</td>\n",
       "      <td>0.499768</td>\n",
       "      <td>0.487756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>1.009208</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.591403</td>\n",
       "      <td>0.541464</td>\n",
       "      <td>0.548484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>1.016081</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.652416</td>\n",
       "      <td>0.598586</td>\n",
       "      <td>0.611810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>1.047768</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.700750</td>\n",
       "      <td>0.614019</td>\n",
       "      <td>0.640248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>1.081920</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.764120</td>\n",
       "      <td>0.676228</td>\n",
       "      <td>0.698580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>1.102918</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.816089</td>\n",
       "      <td>0.705387</td>\n",
       "      <td>0.735339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>1.159704</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.797593</td>\n",
       "      <td>0.676030</td>\n",
       "      <td>0.709348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>1.208305</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.796071</td>\n",
       "      <td>0.698244</td>\n",
       "      <td>0.726753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>1.185236</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.778961</td>\n",
       "      <td>0.722389</td>\n",
       "      <td>0.736171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>1.228429</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.800686</td>\n",
       "      <td>0.737699</td>\n",
       "      <td>0.749913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>1.247854</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.787326</td>\n",
       "      <td>0.710409</td>\n",
       "      <td>0.731909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>1.264744</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.804635</td>\n",
       "      <td>0.716917</td>\n",
       "      <td>0.742572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>1.326918</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783872</td>\n",
       "      <td>0.723448</td>\n",
       "      <td>0.738915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>1.329697</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.793412</td>\n",
       "      <td>0.720482</td>\n",
       "      <td>0.741577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.361727</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.797527</td>\n",
       "      <td>0.723152</td>\n",
       "      <td>0.741583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>1.378496</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.778119</td>\n",
       "      <td>0.720753</td>\n",
       "      <td>0.730565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.349989</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.788449</td>\n",
       "      <td>0.736221</td>\n",
       "      <td>0.749308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.391764</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.798282</td>\n",
       "      <td>0.734686</td>\n",
       "      <td>0.751226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.390133</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.788720</td>\n",
       "      <td>0.722899</td>\n",
       "      <td>0.739832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.413782</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.777452</td>\n",
       "      <td>0.727405</td>\n",
       "      <td>0.739214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.438302</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.766329</td>\n",
       "      <td>0.725416</td>\n",
       "      <td>0.734178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.453950</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785629</td>\n",
       "      <td>0.742079</td>\n",
       "      <td>0.750609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.451013</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.798032</td>\n",
       "      <td>0.730597</td>\n",
       "      <td>0.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.465034</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791632</td>\n",
       "      <td>0.736115</td>\n",
       "      <td>0.749909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.464599</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.788182</td>\n",
       "      <td>0.739636</td>\n",
       "      <td>0.749971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.467451</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.788104</td>\n",
       "      <td>0.739662</td>\n",
       "      <td>0.751062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.469278</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786921</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.751018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.472909</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.785492</td>\n",
       "      <td>0.738006</td>\n",
       "      <td>0.749227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 13:59:40,145] Trial 112 finished with value: 0.749226744183446 and parameters: {'learning_rate': 9.176214230786556e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 47}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 113 with params: {'learning_rate': 8.435718072766996e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:29, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.417400</td>\n",
       "      <td>1.576713</td>\n",
       "      <td>0.699358</td>\n",
       "      <td>0.330864</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.311077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>1.138395</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.447958</td>\n",
       "      <td>0.448887</td>\n",
       "      <td>0.433379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>1.034114</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.463862</td>\n",
       "      <td>0.484723</td>\n",
       "      <td>0.469665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>1.002701</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.528318</td>\n",
       "      <td>0.531467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.999847</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.651157</td>\n",
       "      <td>0.597032</td>\n",
       "      <td>0.610266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>1.027401</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.695937</td>\n",
       "      <td>0.613426</td>\n",
       "      <td>0.635812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>1.062441</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.745658</td>\n",
       "      <td>0.651140</td>\n",
       "      <td>0.675115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>1.078212</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.700016</td>\n",
       "      <td>0.732313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>1.132729</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.796279</td>\n",
       "      <td>0.680208</td>\n",
       "      <td>0.712440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>1.177269</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.789621</td>\n",
       "      <td>0.698097</td>\n",
       "      <td>0.724323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>1.167181</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.778202</td>\n",
       "      <td>0.705945</td>\n",
       "      <td>0.725323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>1.214358</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.801852</td>\n",
       "      <td>0.729375</td>\n",
       "      <td>0.745961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>1.224254</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.783177</td>\n",
       "      <td>0.707403</td>\n",
       "      <td>0.728464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>1.242198</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.808357</td>\n",
       "      <td>0.710629</td>\n",
       "      <td>0.739655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>1.291389</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.796338</td>\n",
       "      <td>0.723983</td>\n",
       "      <td>0.744159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1.319019</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.732695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>1.330889</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796689</td>\n",
       "      <td>0.711093</td>\n",
       "      <td>0.734419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.346568</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.784427</td>\n",
       "      <td>0.724053</td>\n",
       "      <td>0.735789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.323021</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.799233</td>\n",
       "      <td>0.742748</td>\n",
       "      <td>0.758443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.357123</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799398</td>\n",
       "      <td>0.731043</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.367954</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.791451</td>\n",
       "      <td>0.723034</td>\n",
       "      <td>0.739967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.392083</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.720160</td>\n",
       "      <td>0.734539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.406572</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.777591</td>\n",
       "      <td>0.718247</td>\n",
       "      <td>0.732450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.421152</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.783693</td>\n",
       "      <td>0.743458</td>\n",
       "      <td>0.750795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.421606</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.784659</td>\n",
       "      <td>0.729222</td>\n",
       "      <td>0.742646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.435536</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.788827</td>\n",
       "      <td>0.737296</td>\n",
       "      <td>0.749672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.435403</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.788666</td>\n",
       "      <td>0.738159</td>\n",
       "      <td>0.749349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.433065</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.789248</td>\n",
       "      <td>0.740031</td>\n",
       "      <td>0.751775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.437056</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785820</td>\n",
       "      <td>0.739388</td>\n",
       "      <td>0.750404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.438614</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.784200</td>\n",
       "      <td>0.740164</td>\n",
       "      <td>0.749809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 14:15:11,142] Trial 113 finished with value: 0.7498094005531141 and parameters: {'learning_rate': 8.435718072766996e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 42}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 114 with params: {'learning_rate': 2.5723446972771798e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:12 < 10:24, 16.81 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.240300</td>\n",
       "      <td>2.705844</td>\n",
       "      <td>0.477544</td>\n",
       "      <td>0.106512</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>0.111238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.276500</td>\n",
       "      <td>2.023566</td>\n",
       "      <td>0.609533</td>\n",
       "      <td>0.257317</td>\n",
       "      <td>0.238784</td>\n",
       "      <td>0.218594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.685700</td>\n",
       "      <td>1.628776</td>\n",
       "      <td>0.694775</td>\n",
       "      <td>0.318808</td>\n",
       "      <td>0.319023</td>\n",
       "      <td>0.299029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.306300</td>\n",
       "      <td>1.396870</td>\n",
       "      <td>0.718607</td>\n",
       "      <td>0.362548</td>\n",
       "      <td>0.356197</td>\n",
       "      <td>0.335192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.047200</td>\n",
       "      <td>1.261350</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.403964</td>\n",
       "      <td>0.420620</td>\n",
       "      <td>0.399116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>1.169467</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.427680</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.418246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>1.112701</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.471678</td>\n",
       "      <td>0.452871</td>\n",
       "      <td>0.440246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>1.075281</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.479604</td>\n",
       "      <td>0.469043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>1.054254</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.516142</td>\n",
       "      <td>0.484254</td>\n",
       "      <td>0.478268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>1.035312</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.519906</td>\n",
       "      <td>0.497543</td>\n",
       "      <td>0.495027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 14:20:24,623] Trial 114 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 115 with params: {'learning_rate': 7.709532607807873e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:11 < 10:22, 16.87 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.485500</td>\n",
       "      <td>1.656262</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.338564</td>\n",
       "      <td>0.320728</td>\n",
       "      <td>0.301907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.081000</td>\n",
       "      <td>1.167659</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.448737</td>\n",
       "      <td>0.442017</td>\n",
       "      <td>0.427482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>1.049636</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.463645</td>\n",
       "      <td>0.482160</td>\n",
       "      <td>0.467162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>1.003672</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.568874</td>\n",
       "      <td>0.517788</td>\n",
       "      <td>0.521510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.652324</td>\n",
       "      <td>0.594553</td>\n",
       "      <td>0.609109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>1.015664</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.670526</td>\n",
       "      <td>0.604224</td>\n",
       "      <td>0.620542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>1.051036</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.716316</td>\n",
       "      <td>0.628606</td>\n",
       "      <td>0.651066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>1.058037</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.746873</td>\n",
       "      <td>0.654928</td>\n",
       "      <td>0.681691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>1.102655</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.749705</td>\n",
       "      <td>0.659814</td>\n",
       "      <td>0.684199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>1.151028</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.777492</td>\n",
       "      <td>0.681746</td>\n",
       "      <td>0.708560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 14:25:36,944] Trial 115 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 116 with params: {'learning_rate': 4.7746956596454305e-05, 'weight_decay': 0.004, 'adam_beta1': 0.9, 'warmup_steps': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:16 < 10:33, 16.57 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.872800</td>\n",
       "      <td>2.129713</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>0.252862</td>\n",
       "      <td>0.228426</td>\n",
       "      <td>0.212863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.589200</td>\n",
       "      <td>1.443242</td>\n",
       "      <td>0.716774</td>\n",
       "      <td>0.358056</td>\n",
       "      <td>0.358462</td>\n",
       "      <td>0.339538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.012300</td>\n",
       "      <td>1.196964</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.420351</td>\n",
       "      <td>0.434433</td>\n",
       "      <td>0.414387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.710800</td>\n",
       "      <td>1.088445</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.446956</td>\n",
       "      <td>0.455146</td>\n",
       "      <td>0.440706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>1.043321</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.504102</td>\n",
       "      <td>0.493295</td>\n",
       "      <td>0.486256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>1.018230</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.515417</td>\n",
       "      <td>0.503378</td>\n",
       "      <td>0.496894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>1.011139</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.530545</td>\n",
       "      <td>0.515894</td>\n",
       "      <td>0.509724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.645245</td>\n",
       "      <td>0.571565</td>\n",
       "      <td>0.589473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>1.017303</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.646480</td>\n",
       "      <td>0.581341</td>\n",
       "      <td>0.598603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>1.036544</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.683779</td>\n",
       "      <td>0.607665</td>\n",
       "      <td>0.625609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 14:30:54,860] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 117 with params: {'learning_rate': 0.00010621624277688624, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:47, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.249100</td>\n",
       "      <td>1.406284</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.389860</td>\n",
       "      <td>0.376971</td>\n",
       "      <td>0.351668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>1.075534</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.448090</td>\n",
       "      <td>0.458540</td>\n",
       "      <td>0.441843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.553624</td>\n",
       "      <td>0.521835</td>\n",
       "      <td>0.516520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>1.020796</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.624909</td>\n",
       "      <td>0.564089</td>\n",
       "      <td>0.579561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>1.034201</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.695056</td>\n",
       "      <td>0.614922</td>\n",
       "      <td>0.636982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>1.087131</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.741036</td>\n",
       "      <td>0.638265</td>\n",
       "      <td>0.668740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>1.126766</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792863</td>\n",
       "      <td>0.689311</td>\n",
       "      <td>0.717248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>1.153630</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.805831</td>\n",
       "      <td>0.712739</td>\n",
       "      <td>0.734223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>1.192806</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.788588</td>\n",
       "      <td>0.687299</td>\n",
       "      <td>0.714459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>1.250016</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.810566</td>\n",
       "      <td>0.712281</td>\n",
       "      <td>0.739876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>1.237122</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.791349</td>\n",
       "      <td>0.707997</td>\n",
       "      <td>0.730355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>1.262918</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.809335</td>\n",
       "      <td>0.753231</td>\n",
       "      <td>0.762101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>1.325496</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.798655</td>\n",
       "      <td>0.721207</td>\n",
       "      <td>0.741353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.320904</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.807196</td>\n",
       "      <td>0.728118</td>\n",
       "      <td>0.752086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.374342</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.789107</td>\n",
       "      <td>0.725100</td>\n",
       "      <td>0.741060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.390490</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.814063</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.747226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.404380</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.819024</td>\n",
       "      <td>0.725379</td>\n",
       "      <td>0.750461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.440578</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.793608</td>\n",
       "      <td>0.733519</td>\n",
       "      <td>0.746086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.399676</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.791033</td>\n",
       "      <td>0.736575</td>\n",
       "      <td>0.749991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.445728</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.792537</td>\n",
       "      <td>0.726091</td>\n",
       "      <td>0.743105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.443072</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.786180</td>\n",
       "      <td>0.734326</td>\n",
       "      <td>0.747056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.466943</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.794979</td>\n",
       "      <td>0.728421</td>\n",
       "      <td>0.746788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.494619</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778740</td>\n",
       "      <td>0.732151</td>\n",
       "      <td>0.742739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.492176</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.799259</td>\n",
       "      <td>0.731709</td>\n",
       "      <td>0.749082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.503809</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.812759</td>\n",
       "      <td>0.735768</td>\n",
       "      <td>0.755467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.520424</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.791960</td>\n",
       "      <td>0.736690</td>\n",
       "      <td>0.750128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.520573</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.799026</td>\n",
       "      <td>0.735367</td>\n",
       "      <td>0.750720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.514980</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.796122</td>\n",
       "      <td>0.735750</td>\n",
       "      <td>0.749865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.521239</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.797924</td>\n",
       "      <td>0.735691</td>\n",
       "      <td>0.751007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.524624</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.798885</td>\n",
       "      <td>0.736743</td>\n",
       "      <td>0.751780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 14:46:44,610] Trial 117 finished with value: 0.7517800588353467 and parameters: {'learning_rate': 0.00010621624277688624, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 47}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 118 with params: {'learning_rate': 0.00016095510626902364, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.877500</td>\n",
       "      <td>1.163629</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.439526</td>\n",
       "      <td>0.441883</td>\n",
       "      <td>0.425341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.994391</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.524377</td>\n",
       "      <td>0.518597</td>\n",
       "      <td>0.514821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.986253</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.644898</td>\n",
       "      <td>0.608369</td>\n",
       "      <td>0.613656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>1.049332</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.741122</td>\n",
       "      <td>0.648481</td>\n",
       "      <td>0.677204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>1.100132</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.789615</td>\n",
       "      <td>0.708808</td>\n",
       "      <td>0.732204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>1.154135</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.781466</td>\n",
       "      <td>0.669554</td>\n",
       "      <td>0.699196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>1.238040</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.803121</td>\n",
       "      <td>0.710882</td>\n",
       "      <td>0.732790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.236204</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.790114</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.741519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>1.312925</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.794243</td>\n",
       "      <td>0.690165</td>\n",
       "      <td>0.719655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.365507</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.800334</td>\n",
       "      <td>0.723666</td>\n",
       "      <td>0.741995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.402412</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.690494</td>\n",
       "      <td>0.710284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.438838</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.786546</td>\n",
       "      <td>0.723667</td>\n",
       "      <td>0.734861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.470222</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.783198</td>\n",
       "      <td>0.722273</td>\n",
       "      <td>0.733916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.434472</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.803654</td>\n",
       "      <td>0.720204</td>\n",
       "      <td>0.740938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.495414</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.804564</td>\n",
       "      <td>0.715379</td>\n",
       "      <td>0.735759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.502073</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.799254</td>\n",
       "      <td>0.714174</td>\n",
       "      <td>0.733357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.512815</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>0.704293</td>\n",
       "      <td>0.726304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.575827</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>0.708145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.551906</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.774527</td>\n",
       "      <td>0.723419</td>\n",
       "      <td>0.733341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>1.586303</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.793039</td>\n",
       "      <td>0.728519</td>\n",
       "      <td>0.743181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.610011</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.783466</td>\n",
       "      <td>0.717066</td>\n",
       "      <td>0.732123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.598262</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.793508</td>\n",
       "      <td>0.701457</td>\n",
       "      <td>0.728312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.625045</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.794717</td>\n",
       "      <td>0.714900</td>\n",
       "      <td>0.735086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.628076</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798137</td>\n",
       "      <td>0.703942</td>\n",
       "      <td>0.730943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.673542</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.808119</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.676923</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.794237</td>\n",
       "      <td>0.731486</td>\n",
       "      <td>0.745277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.678491</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.788800</td>\n",
       "      <td>0.719349</td>\n",
       "      <td>0.737346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.667761</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.809432</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.751052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.671666</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789577</td>\n",
       "      <td>0.731685</td>\n",
       "      <td>0.744173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.677228</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.795358</td>\n",
       "      <td>0.731447</td>\n",
       "      <td>0.747509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:02:28,194] Trial 118 finished with value: 0.7475090018430808 and parameters: {'learning_rate': 0.00016095510626902364, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 50}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 119 with params: {'learning_rate': 0.0001255088838467423, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 16:02, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.057500</td>\n",
       "      <td>1.277342</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.395549</td>\n",
       "      <td>0.373923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>1.040623</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.473833</td>\n",
       "      <td>0.476713</td>\n",
       "      <td>0.468302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.985733</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.605897</td>\n",
       "      <td>0.566735</td>\n",
       "      <td>0.572386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>1.029379</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.674242</td>\n",
       "      <td>0.601429</td>\n",
       "      <td>0.622605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>1.049310</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.693764</td>\n",
       "      <td>0.625455</td>\n",
       "      <td>0.642038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>1.120528</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.755771</td>\n",
       "      <td>0.656653</td>\n",
       "      <td>0.682040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>1.184511</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.807816</td>\n",
       "      <td>0.694075</td>\n",
       "      <td>0.720323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>1.202217</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>0.713310</td>\n",
       "      <td>0.730299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>1.241799</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.784921</td>\n",
       "      <td>0.692634</td>\n",
       "      <td>0.716085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>1.305864</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.805745</td>\n",
       "      <td>0.703018</td>\n",
       "      <td>0.727892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>1.328700</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.770220</td>\n",
       "      <td>0.704297</td>\n",
       "      <td>0.720981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>1.347429</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.790105</td>\n",
       "      <td>0.732038</td>\n",
       "      <td>0.739561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>1.377961</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.785567</td>\n",
       "      <td>0.708929</td>\n",
       "      <td>0.727099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.356565</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.805650</td>\n",
       "      <td>0.729228</td>\n",
       "      <td>0.749375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.451316</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.718417</td>\n",
       "      <td>0.735961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.450881</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.807176</td>\n",
       "      <td>0.721278</td>\n",
       "      <td>0.742673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.459587</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.770601</td>\n",
       "      <td>0.697643</td>\n",
       "      <td>0.716186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.482835</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.784169</td>\n",
       "      <td>0.716028</td>\n",
       "      <td>0.731373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.439382</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.787757</td>\n",
       "      <td>0.729985</td>\n",
       "      <td>0.742329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.505776</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.801391</td>\n",
       "      <td>0.712878</td>\n",
       "      <td>0.734485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.498030</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797072</td>\n",
       "      <td>0.749106</td>\n",
       "      <td>0.754553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.549956</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.802511</td>\n",
       "      <td>0.727267</td>\n",
       "      <td>0.744263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.544352</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.731398</td>\n",
       "      <td>0.744065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.531283</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.805826</td>\n",
       "      <td>0.726142</td>\n",
       "      <td>0.747174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.557502</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.812211</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.742004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.573898</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.802376</td>\n",
       "      <td>0.730169</td>\n",
       "      <td>0.748719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.566122</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.793685</td>\n",
       "      <td>0.720624</td>\n",
       "      <td>0.737978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.570556</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.800487</td>\n",
       "      <td>0.720167</td>\n",
       "      <td>0.740577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.579346</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.795443</td>\n",
       "      <td>0.728893</td>\n",
       "      <td>0.744807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.582440</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.728438</td>\n",
       "      <td>0.744769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:18:33,113] Trial 119 finished with value: 0.7447685465145879 and parameters: {'learning_rate': 0.0001255088838467423, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 36}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 120 with params: {'learning_rate': 9.011141377096925e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:52, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.375400</td>\n",
       "      <td>1.525882</td>\n",
       "      <td>0.707608</td>\n",
       "      <td>0.334099</td>\n",
       "      <td>0.335346</td>\n",
       "      <td>0.317321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>1.119412</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.446084</td>\n",
       "      <td>0.452571</td>\n",
       "      <td>0.434856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>1.024117</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.467696</td>\n",
       "      <td>0.489205</td>\n",
       "      <td>0.473804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>1.007291</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.582128</td>\n",
       "      <td>0.535592</td>\n",
       "      <td>0.539961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>1.011284</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.652767</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.612248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>1.039945</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.693025</td>\n",
       "      <td>0.613943</td>\n",
       "      <td>0.636463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>1.076802</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.744555</td>\n",
       "      <td>0.654603</td>\n",
       "      <td>0.677109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>1.096201</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.808859</td>\n",
       "      <td>0.704213</td>\n",
       "      <td>0.729960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>1.150920</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.807661</td>\n",
       "      <td>0.688407</td>\n",
       "      <td>0.722279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>1.201321</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.802483</td>\n",
       "      <td>0.705905</td>\n",
       "      <td>0.733150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>1.180882</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.790806</td>\n",
       "      <td>0.732626</td>\n",
       "      <td>0.746984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>1.232924</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>0.740417</td>\n",
       "      <td>0.751725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.248004</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.779357</td>\n",
       "      <td>0.718103</td>\n",
       "      <td>0.732378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>1.260544</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.806777</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.747066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>1.317381</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791489</td>\n",
       "      <td>0.733828</td>\n",
       "      <td>0.747419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>1.330867</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.789971</td>\n",
       "      <td>0.717655</td>\n",
       "      <td>0.737693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>1.350281</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.798227</td>\n",
       "      <td>0.714089</td>\n",
       "      <td>0.735252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.370052</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780052</td>\n",
       "      <td>0.724017</td>\n",
       "      <td>0.734265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>1.346470</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.734071</td>\n",
       "      <td>0.742517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.382789</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.784707</td>\n",
       "      <td>0.738755</td>\n",
       "      <td>0.746984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.385355</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.780842</td>\n",
       "      <td>0.725193</td>\n",
       "      <td>0.738609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.407954</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.782216</td>\n",
       "      <td>0.731698</td>\n",
       "      <td>0.742303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.426897</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.773495</td>\n",
       "      <td>0.729445</td>\n",
       "      <td>0.738844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.448774</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.779973</td>\n",
       "      <td>0.740275</td>\n",
       "      <td>0.747415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.442819</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794371</td>\n",
       "      <td>0.739871</td>\n",
       "      <td>0.751989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.460238</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.780135</td>\n",
       "      <td>0.740423</td>\n",
       "      <td>0.748120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.460775</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.787762</td>\n",
       "      <td>0.739695</td>\n",
       "      <td>0.749967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.463443</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.787819</td>\n",
       "      <td>0.739119</td>\n",
       "      <td>0.749512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.464039</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.788215</td>\n",
       "      <td>0.739357</td>\n",
       "      <td>0.750352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.467585</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781892</td>\n",
       "      <td>0.737827</td>\n",
       "      <td>0.748125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:34:27,365] Trial 120 finished with value: 0.748125303860967 and parameters: {'learning_rate': 9.011141377096925e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 49}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 121 with params: {'learning_rate': 0.00026452018072984935, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:41 < 05:20, 16.37 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.513300</td>\n",
       "      <td>1.036795</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.445692</td>\n",
       "      <td>0.469064</td>\n",
       "      <td>0.446431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>1.010571</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.683568</td>\n",
       "      <td>0.615237</td>\n",
       "      <td>0.631119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>1.107712</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.765997</td>\n",
       "      <td>0.694361</td>\n",
       "      <td>0.712586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>1.182190</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.815061</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.726739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>1.215126</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.802916</td>\n",
       "      <td>0.713559</td>\n",
       "      <td>0.741940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>1.257128</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.813763</td>\n",
       "      <td>0.715617</td>\n",
       "      <td>0.742709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>1.307117</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.808843</td>\n",
       "      <td>0.694851</td>\n",
       "      <td>0.728923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>1.355468</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.820665</td>\n",
       "      <td>0.729501</td>\n",
       "      <td>0.754460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.464665</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.800307</td>\n",
       "      <td>0.726080</td>\n",
       "      <td>0.745076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.494175</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.799757</td>\n",
       "      <td>0.740914</td>\n",
       "      <td>0.755725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.544379</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.789388</td>\n",
       "      <td>0.723675</td>\n",
       "      <td>0.739951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.492696</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.825188</td>\n",
       "      <td>0.729019</td>\n",
       "      <td>0.760664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.557888</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.793505</td>\n",
       "      <td>0.704679</td>\n",
       "      <td>0.730774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.566757</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.788334</td>\n",
       "      <td>0.720463</td>\n",
       "      <td>0.739996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.631834</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.792305</td>\n",
       "      <td>0.713268</td>\n",
       "      <td>0.735529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.669869</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.786653</td>\n",
       "      <td>0.713703</td>\n",
       "      <td>0.731983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.686050</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.789672</td>\n",
       "      <td>0.735698</td>\n",
       "      <td>0.748030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.710366</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.739504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.693193</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.823638</td>\n",
       "      <td>0.722815</td>\n",
       "      <td>0.752048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.782764</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.703089</td>\n",
       "      <td>0.732592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:45:09,714] Trial 121 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 122 with params: {'learning_rate': 0.00014265093249551882, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:37 < 05:18, 16.46 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.981500</td>\n",
       "      <td>1.217675</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.399721</td>\n",
       "      <td>0.375746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>1.012147</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.490368</td>\n",
       "      <td>0.493911</td>\n",
       "      <td>0.485783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.980968</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.656434</td>\n",
       "      <td>0.597453</td>\n",
       "      <td>0.611131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>1.033406</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.715118</td>\n",
       "      <td>0.626068</td>\n",
       "      <td>0.649621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>1.077060</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.776211</td>\n",
       "      <td>0.692720</td>\n",
       "      <td>0.714752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>1.126601</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.771172</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.691536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>1.215627</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.805477</td>\n",
       "      <td>0.694432</td>\n",
       "      <td>0.722116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.215063</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786303</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.725485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>1.266257</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.703854</td>\n",
       "      <td>0.724802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>1.340050</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.812906</td>\n",
       "      <td>0.709240</td>\n",
       "      <td>0.734134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.365175</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.774211</td>\n",
       "      <td>0.708024</td>\n",
       "      <td>0.720541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.378018</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776115</td>\n",
       "      <td>0.723163</td>\n",
       "      <td>0.728204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.426907</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.718355</td>\n",
       "      <td>0.731863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.414965</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.801681</td>\n",
       "      <td>0.747232</td>\n",
       "      <td>0.755965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.508322</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.804403</td>\n",
       "      <td>0.734560</td>\n",
       "      <td>0.748163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>1.487078</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.805199</td>\n",
       "      <td>0.711909</td>\n",
       "      <td>0.733778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.517437</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.777895</td>\n",
       "      <td>0.702383</td>\n",
       "      <td>0.719491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.541774</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.783264</td>\n",
       "      <td>0.717608</td>\n",
       "      <td>0.729187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.500410</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.792423</td>\n",
       "      <td>0.714947</td>\n",
       "      <td>0.730952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.562344</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.804434</td>\n",
       "      <td>0.706813</td>\n",
       "      <td>0.730987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:55:48,842] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 123 with params: {'learning_rate': 3.600855231113945e-05, 'weight_decay': 0.005, 'adam_beta1': 0.93, 'warmup_steps': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:36 < 13:03, 16.75 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.076500</td>\n",
       "      <td>2.424616</td>\n",
       "      <td>0.528873</td>\n",
       "      <td>0.187502</td>\n",
       "      <td>0.161560</td>\n",
       "      <td>0.141584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.927000</td>\n",
       "      <td>1.698295</td>\n",
       "      <td>0.688359</td>\n",
       "      <td>0.313226</td>\n",
       "      <td>0.307569</td>\n",
       "      <td>0.290011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.312900</td>\n",
       "      <td>1.360065</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.368989</td>\n",
       "      <td>0.372857</td>\n",
       "      <td>0.351945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>1.199388</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.396006</td>\n",
       "      <td>0.417711</td>\n",
       "      <td>0.394623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>1.114617</td>\n",
       "      <td>0.762603</td>\n",
       "      <td>0.471516</td>\n",
       "      <td>0.453615</td>\n",
       "      <td>0.442213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 15:58:26,910] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 124 with params: {'learning_rate': 7.154718987890825e-05, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:22 < 10:44, 16.28 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.562300</td>\n",
       "      <td>1.732927</td>\n",
       "      <td>0.675527</td>\n",
       "      <td>0.323137</td>\n",
       "      <td>0.298897</td>\n",
       "      <td>0.287353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.156900</td>\n",
       "      <td>1.196745</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>0.415421</td>\n",
       "      <td>0.399274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>1.061149</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.457257</td>\n",
       "      <td>0.473226</td>\n",
       "      <td>0.453022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>1.010198</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.480875</td>\n",
       "      <td>0.491498</td>\n",
       "      <td>0.480421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.990250</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.605187</td>\n",
       "      <td>0.554335</td>\n",
       "      <td>0.562536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.010412</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.662850</td>\n",
       "      <td>0.585894</td>\n",
       "      <td>0.604278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>1.047366</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.695936</td>\n",
       "      <td>0.621804</td>\n",
       "      <td>0.639609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>1.048704</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.694481</td>\n",
       "      <td>0.627883</td>\n",
       "      <td>0.644324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>1.087433</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.701545</td>\n",
       "      <td>0.630522</td>\n",
       "      <td>0.647202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>1.146066</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.761929</td>\n",
       "      <td>0.674117</td>\n",
       "      <td>0.694985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:03:50,537] Trial 124 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 125 with params: {'learning_rate': 6.320517481758516e-05, 'weight_decay': 0.006, 'adam_beta1': 0.91, 'warmup_steps': 45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:37 < 13:10, 16.61 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.665600</td>\n",
       "      <td>1.858693</td>\n",
       "      <td>0.629698</td>\n",
       "      <td>0.284384</td>\n",
       "      <td>0.261013</td>\n",
       "      <td>0.244578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.286800</td>\n",
       "      <td>1.264213</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.402980</td>\n",
       "      <td>0.399468</td>\n",
       "      <td>0.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>1.096587</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.448729</td>\n",
       "      <td>0.465752</td>\n",
       "      <td>0.445737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>1.031736</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.474726</td>\n",
       "      <td>0.476057</td>\n",
       "      <td>0.468477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>1.001287</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.533529</td>\n",
       "      <td>0.512202</td>\n",
       "      <td>0.506809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:06:29,750] Trial 125 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 126 with params: {'learning_rate': 5.2653913461752696e-05, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:41 < 13:27, 16.25 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.805600</td>\n",
       "      <td>2.034397</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.270404</td>\n",
       "      <td>0.237027</td>\n",
       "      <td>0.219467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.480900</td>\n",
       "      <td>1.374630</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.376793</td>\n",
       "      <td>0.376024</td>\n",
       "      <td>0.357780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.920900</td>\n",
       "      <td>1.155891</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.437487</td>\n",
       "      <td>0.440278</td>\n",
       "      <td>0.421176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>1.065480</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.494136</td>\n",
       "      <td>0.473739</td>\n",
       "      <td>0.462120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>1.027599</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.501580</td>\n",
       "      <td>0.493605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:09:12,653] Trial 126 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 127 with params: {'learning_rate': 9.152827015282269e-05, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:21 < 10:42, 16.34 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.369500</td>\n",
       "      <td>1.520737</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.365790</td>\n",
       "      <td>0.350511</td>\n",
       "      <td>0.335361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.932800</td>\n",
       "      <td>1.114061</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.443782</td>\n",
       "      <td>0.452313</td>\n",
       "      <td>0.433464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>1.023968</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.465794</td>\n",
       "      <td>0.488113</td>\n",
       "      <td>0.472151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>1.004306</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.586594</td>\n",
       "      <td>0.542721</td>\n",
       "      <td>0.547169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>1.012781</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.645823</td>\n",
       "      <td>0.598545</td>\n",
       "      <td>0.607795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>1.045457</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.696102</td>\n",
       "      <td>0.609233</td>\n",
       "      <td>0.634240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>1.078581</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.745121</td>\n",
       "      <td>0.652639</td>\n",
       "      <td>0.675748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>1.101350</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.805744</td>\n",
       "      <td>0.695856</td>\n",
       "      <td>0.725240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>1.153122</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799983</td>\n",
       "      <td>0.683401</td>\n",
       "      <td>0.716419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>1.205795</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.790849</td>\n",
       "      <td>0.698141</td>\n",
       "      <td>0.725256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:14:34,955] Trial 127 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 128 with params: {'learning_rate': 9.670871525412212e-05, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 16:00, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.342800</td>\n",
       "      <td>1.482278</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.367097</td>\n",
       "      <td>0.358150</td>\n",
       "      <td>0.341020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.889800</td>\n",
       "      <td>1.099321</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.443481</td>\n",
       "      <td>0.453481</td>\n",
       "      <td>0.434975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>1.014487</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.465691</td>\n",
       "      <td>0.492292</td>\n",
       "      <td>0.474256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>1.009913</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.607852</td>\n",
       "      <td>0.547252</td>\n",
       "      <td>0.556693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>1.022350</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.645434</td>\n",
       "      <td>0.597018</td>\n",
       "      <td>0.606889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>1.060812</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.694191</td>\n",
       "      <td>0.606372</td>\n",
       "      <td>0.631929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>1.099570</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.767052</td>\n",
       "      <td>0.673101</td>\n",
       "      <td>0.696507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>1.125193</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.808456</td>\n",
       "      <td>0.715886</td>\n",
       "      <td>0.738767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>1.171994</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.811745</td>\n",
       "      <td>0.690098</td>\n",
       "      <td>0.724938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>1.229040</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.791413</td>\n",
       "      <td>0.701144</td>\n",
       "      <td>0.726174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>1.213186</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.769572</td>\n",
       "      <td>0.713849</td>\n",
       "      <td>0.726964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.251432</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.798802</td>\n",
       "      <td>0.738353</td>\n",
       "      <td>0.746758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>1.275541</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.775649</td>\n",
       "      <td>0.712616</td>\n",
       "      <td>0.725775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.284272</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794667</td>\n",
       "      <td>0.729311</td>\n",
       "      <td>0.747717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.353827</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.785499</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>0.745213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>1.350141</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.782449</td>\n",
       "      <td>0.731067</td>\n",
       "      <td>0.742972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.370816</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.796497</td>\n",
       "      <td>0.711291</td>\n",
       "      <td>0.730564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.401763</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.785797</td>\n",
       "      <td>0.731259</td>\n",
       "      <td>0.737986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.370920</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.779409</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.733450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.403247</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.797641</td>\n",
       "      <td>0.739673</td>\n",
       "      <td>0.751755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.417844</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783627</td>\n",
       "      <td>0.740682</td>\n",
       "      <td>0.748928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.432134</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.775375</td>\n",
       "      <td>0.730846</td>\n",
       "      <td>0.737510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.459427</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>0.731886</td>\n",
       "      <td>0.743416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.469028</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.785729</td>\n",
       "      <td>0.738672</td>\n",
       "      <td>0.748716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.468662</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.799349</td>\n",
       "      <td>0.738143</td>\n",
       "      <td>0.752557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.480442</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.779259</td>\n",
       "      <td>0.748056</td>\n",
       "      <td>0.751180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>1.484522</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.786643</td>\n",
       "      <td>0.737529</td>\n",
       "      <td>0.748825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.486584</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.786075</td>\n",
       "      <td>0.734233</td>\n",
       "      <td>0.746666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.488766</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.783284</td>\n",
       "      <td>0.737750</td>\n",
       "      <td>0.747899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.490241</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781625</td>\n",
       "      <td>0.733355</td>\n",
       "      <td>0.744850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:30:38,066] Trial 128 finished with value: 0.7448504013966059 and parameters: {'learning_rate': 9.670871525412212e-05, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 53}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 129 with params: {'learning_rate': 0.0003454647548761435, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:32 < 05:16, 16.60 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.310300</td>\n",
       "      <td>0.994044</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.498393</td>\n",
       "      <td>0.495042</td>\n",
       "      <td>0.483763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>1.088319</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.718656</td>\n",
       "      <td>0.645221</td>\n",
       "      <td>0.662029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>1.187219</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783006</td>\n",
       "      <td>0.717175</td>\n",
       "      <td>0.731384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>1.285250</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.781785</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>0.713629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>1.298487</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.772758</td>\n",
       "      <td>0.720422</td>\n",
       "      <td>0.728631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.338235</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.817578</td>\n",
       "      <td>0.701908</td>\n",
       "      <td>0.734873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.498653</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.796187</td>\n",
       "      <td>0.701057</td>\n",
       "      <td>0.721507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.458078</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.797811</td>\n",
       "      <td>0.724014</td>\n",
       "      <td>0.742660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.573691</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.797007</td>\n",
       "      <td>0.711586</td>\n",
       "      <td>0.731886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.581267</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.787280</td>\n",
       "      <td>0.716453</td>\n",
       "      <td>0.732702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.526812</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.768244</td>\n",
       "      <td>0.727994</td>\n",
       "      <td>0.729476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.650242</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.777290</td>\n",
       "      <td>0.721134</td>\n",
       "      <td>0.730524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.661146</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.762289</td>\n",
       "      <td>0.726857</td>\n",
       "      <td>0.729164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.599366</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.767767</td>\n",
       "      <td>0.727087</td>\n",
       "      <td>0.731346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.743765</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.790693</td>\n",
       "      <td>0.712561</td>\n",
       "      <td>0.731519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.751274</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.773540</td>\n",
       "      <td>0.711271</td>\n",
       "      <td>0.721921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.815869</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.761685</td>\n",
       "      <td>0.709362</td>\n",
       "      <td>0.712002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.754172</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.762319</td>\n",
       "      <td>0.704842</td>\n",
       "      <td>0.708920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.802267</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.704270</td>\n",
       "      <td>0.723467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.752749</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.768835</td>\n",
       "      <td>0.698436</td>\n",
       "      <td>0.711770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:41:11,565] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 130 with params: {'learning_rate': 6.173972306226184e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:34 < 12:53, 16.96 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.672400</td>\n",
       "      <td>1.873099</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.258032</td>\n",
       "      <td>0.242476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.305300</td>\n",
       "      <td>1.275022</td>\n",
       "      <td>0.741522</td>\n",
       "      <td>0.400504</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.375773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.100894</td>\n",
       "      <td>0.767186</td>\n",
       "      <td>0.450659</td>\n",
       "      <td>0.466115</td>\n",
       "      <td>0.446733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>1.035686</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.478864</td>\n",
       "      <td>0.477828</td>\n",
       "      <td>0.470239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>1.002410</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.538064</td>\n",
       "      <td>0.511714</td>\n",
       "      <td>0.509992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:43:47,522] Trial 130 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 131 with params: {'learning_rate': 0.0002316662864002156, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.605600</td>\n",
       "      <td>1.052360</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.421010</td>\n",
       "      <td>0.459520</td>\n",
       "      <td>0.432436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>1.004250</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.651741</td>\n",
       "      <td>0.580584</td>\n",
       "      <td>0.594481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>1.068530</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.745395</td>\n",
       "      <td>0.659596</td>\n",
       "      <td>0.684245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>1.119120</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.796360</td>\n",
       "      <td>0.679119</td>\n",
       "      <td>0.718719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.243230</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778486</td>\n",
       "      <td>0.711342</td>\n",
       "      <td>0.729071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>1.266095</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.797395</td>\n",
       "      <td>0.679649</td>\n",
       "      <td>0.716546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.343879</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.804589</td>\n",
       "      <td>0.691706</td>\n",
       "      <td>0.721394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>1.366550</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.811144</td>\n",
       "      <td>0.725993</td>\n",
       "      <td>0.744067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.381895</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.808216</td>\n",
       "      <td>0.722895</td>\n",
       "      <td>0.746008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>1.458593</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.817162</td>\n",
       "      <td>0.727618</td>\n",
       "      <td>0.751767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.544407</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.801132</td>\n",
       "      <td>0.710577</td>\n",
       "      <td>0.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1.490034</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.776012</td>\n",
       "      <td>0.708093</td>\n",
       "      <td>0.725496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.553401</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.780872</td>\n",
       "      <td>0.703331</td>\n",
       "      <td>0.722088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.524085</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.817008</td>\n",
       "      <td>0.733643</td>\n",
       "      <td>0.756552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.607285</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.790060</td>\n",
       "      <td>0.715445</td>\n",
       "      <td>0.734573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.645050</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.800320</td>\n",
       "      <td>0.708149</td>\n",
       "      <td>0.735517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.646817</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.794793</td>\n",
       "      <td>0.713094</td>\n",
       "      <td>0.735310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.748468</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.797238</td>\n",
       "      <td>0.718743</td>\n",
       "      <td>0.743782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.699070</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.781177</td>\n",
       "      <td>0.720223</td>\n",
       "      <td>0.737268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.772868</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.797909</td>\n",
       "      <td>0.716375</td>\n",
       "      <td>0.740752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.758344</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.778882</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.732699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.799225</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.798481</td>\n",
       "      <td>0.716119</td>\n",
       "      <td>0.741353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.816023</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.787381</td>\n",
       "      <td>0.721603</td>\n",
       "      <td>0.740314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.788169</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.803580</td>\n",
       "      <td>0.704275</td>\n",
       "      <td>0.736307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.844337</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.803693</td>\n",
       "      <td>0.702596</td>\n",
       "      <td>0.733639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.859482</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.807045</td>\n",
       "      <td>0.716840</td>\n",
       "      <td>0.744617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.848648</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.801825</td>\n",
       "      <td>0.712888</td>\n",
       "      <td>0.741349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.831973</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.809312</td>\n",
       "      <td>0.715094</td>\n",
       "      <td>0.744273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.839113</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.801976</td>\n",
       "      <td>0.714273</td>\n",
       "      <td>0.740841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.839840</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.803351</td>\n",
       "      <td>0.714058</td>\n",
       "      <td>0.741474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:59:35,901] Trial 131 finished with value: 0.7414736742501329 and parameters: {'learning_rate': 0.0002316662864002156, 'weight_decay': 0.005, 'adam_beta1': 0.91, 'warmup_steps': 53}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132 with params: {'learning_rate': 0.00010262366613347604, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.278900</td>\n",
       "      <td>1.431489</td>\n",
       "      <td>0.726856</td>\n",
       "      <td>0.368490</td>\n",
       "      <td>0.368288</td>\n",
       "      <td>0.343143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>1.083724</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.456303</td>\n",
       "      <td>0.456385</td>\n",
       "      <td>0.439839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>1.008860</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.531415</td>\n",
       "      <td>0.510180</td>\n",
       "      <td>0.501933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245200</td>\n",
       "      <td>1.015146</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.625958</td>\n",
       "      <td>0.567159</td>\n",
       "      <td>0.581625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>1.029784</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.614335</td>\n",
       "      <td>0.636064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.097200</td>\n",
       "      <td>1.078897</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.709194</td>\n",
       "      <td>0.626496</td>\n",
       "      <td>0.649916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>1.116246</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.772972</td>\n",
       "      <td>0.680582</td>\n",
       "      <td>0.704662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>1.143488</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.808454</td>\n",
       "      <td>0.713645</td>\n",
       "      <td>0.736830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>1.184423</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.794242</td>\n",
       "      <td>0.680810</td>\n",
       "      <td>0.713151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>1.234521</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.793247</td>\n",
       "      <td>0.703987</td>\n",
       "      <td>0.729528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>1.223642</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785082</td>\n",
       "      <td>0.717702</td>\n",
       "      <td>0.734286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>1.255011</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.809824</td>\n",
       "      <td>0.733722</td>\n",
       "      <td>0.747714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>1.304464</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.793335</td>\n",
       "      <td>0.715910</td>\n",
       "      <td>0.734310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.296731</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.803212</td>\n",
       "      <td>0.723914</td>\n",
       "      <td>0.747428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.354891</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.778178</td>\n",
       "      <td>0.719164</td>\n",
       "      <td>0.733140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.370604</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.808778</td>\n",
       "      <td>0.720566</td>\n",
       "      <td>0.746710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.388554</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.811191</td>\n",
       "      <td>0.724023</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>1.430362</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.778114</td>\n",
       "      <td>0.723986</td>\n",
       "      <td>0.731034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.390908</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.793694</td>\n",
       "      <td>0.736983</td>\n",
       "      <td>0.751276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.429485</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.793016</td>\n",
       "      <td>0.740207</td>\n",
       "      <td>0.752552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.418050</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.786118</td>\n",
       "      <td>0.734928</td>\n",
       "      <td>0.747264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.447967</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.785221</td>\n",
       "      <td>0.729525</td>\n",
       "      <td>0.742385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.481452</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.720359</td>\n",
       "      <td>0.733709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.472885</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.799094</td>\n",
       "      <td>0.730730</td>\n",
       "      <td>0.748694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.481955</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.818735</td>\n",
       "      <td>0.735517</td>\n",
       "      <td>0.757486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.505132</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.792226</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>0.749891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.503581</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.801009</td>\n",
       "      <td>0.741424</td>\n",
       "      <td>0.754782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.499527</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.797336</td>\n",
       "      <td>0.740223</td>\n",
       "      <td>0.753181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.506156</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.796004</td>\n",
       "      <td>0.734357</td>\n",
       "      <td>0.749947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.510302</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.799083</td>\n",
       "      <td>0.739708</td>\n",
       "      <td>0.753743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:15:27,394] Trial 132 finished with value: 0.7537432577475205 and parameters: {'learning_rate': 0.00010262366613347604, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 47}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 133 with params: {'learning_rate': 1.5940010731374617e-05, 'weight_decay': 0.007, 'adam_beta1': 0.93, 'warmup_steps': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:35 < 05:17, 16.51 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.431000</td>\n",
       "      <td>3.051452</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>0.099765</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.073996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.733500</td>\n",
       "      <td>2.498484</td>\n",
       "      <td>0.512374</td>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.149399</td>\n",
       "      <td>0.125148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.232400</td>\n",
       "      <td>2.109993</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.263890</td>\n",
       "      <td>0.223031</td>\n",
       "      <td>0.206171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.864100</td>\n",
       "      <td>1.829331</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.316023</td>\n",
       "      <td>0.272596</td>\n",
       "      <td>0.257564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.580200</td>\n",
       "      <td>1.622367</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.313256</td>\n",
       "      <td>0.296722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.367100</td>\n",
       "      <td>1.472049</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.363364</td>\n",
       "      <td>0.347726</td>\n",
       "      <td>0.332322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.194700</td>\n",
       "      <td>1.361274</td>\n",
       "      <td>0.725940</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>0.379086</td>\n",
       "      <td>0.367151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.064600</td>\n",
       "      <td>1.285432</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.435756</td>\n",
       "      <td>0.424818</td>\n",
       "      <td>0.405490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>1.225831</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.421530</td>\n",
       "      <td>0.431591</td>\n",
       "      <td>0.413321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>1.183149</td>\n",
       "      <td>0.749771</td>\n",
       "      <td>0.438816</td>\n",
       "      <td>0.438646</td>\n",
       "      <td>0.422643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>1.143575</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.462934</td>\n",
       "      <td>0.444895</td>\n",
       "      <td>0.434485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>1.113379</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.459490</td>\n",
       "      <td>0.451396</td>\n",
       "      <td>0.436664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>1.090626</td>\n",
       "      <td>0.758937</td>\n",
       "      <td>0.469004</td>\n",
       "      <td>0.459442</td>\n",
       "      <td>0.447021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>1.074699</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.513585</td>\n",
       "      <td>0.472087</td>\n",
       "      <td>0.467132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.581500</td>\n",
       "      <td>1.063151</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.512975</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.471714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>1.054533</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.536889</td>\n",
       "      <td>0.489446</td>\n",
       "      <td>0.482312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>1.046916</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.513386</td>\n",
       "      <td>0.487438</td>\n",
       "      <td>0.477607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>1.037757</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.537779</td>\n",
       "      <td>0.500245</td>\n",
       "      <td>0.496196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>1.030633</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.516910</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.481205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>1.030584</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.504507</td>\n",
       "      <td>0.490021</td>\n",
       "      <td>0.482347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:26:04,402] Trial 133 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 134 with params: {'learning_rate': 7.394453948315603e-05, 'weight_decay': 0.006, 'adam_beta1': 0.92, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:32 < 12:41, 17.23 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>1.683671</td>\n",
       "      <td>0.686526</td>\n",
       "      <td>0.315513</td>\n",
       "      <td>0.310032</td>\n",
       "      <td>0.293872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>1.175540</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.445153</td>\n",
       "      <td>0.433603</td>\n",
       "      <td>0.418560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>1.054620</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.468154</td>\n",
       "      <td>0.481046</td>\n",
       "      <td>0.466538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>1.001333</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.500637</td>\n",
       "      <td>0.501272</td>\n",
       "      <td>0.491873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.992170</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.598269</td>\n",
       "      <td>0.560500</td>\n",
       "      <td>0.567757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:28:37,884] Trial 134 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 135 with params: {'learning_rate': 1.120156779854372e-06, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:36 < 13:03, 16.74 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.839200</td>\n",
       "      <td>3.788987</td>\n",
       "      <td>0.126489</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.749000</td>\n",
       "      <td>3.710993</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.016772</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.016557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.679800</td>\n",
       "      <td>3.648659</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.018210</td>\n",
       "      <td>0.030227</td>\n",
       "      <td>0.015816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.621200</td>\n",
       "      <td>3.592611</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.016617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.566500</td>\n",
       "      <td>3.540942</td>\n",
       "      <td>0.214482</td>\n",
       "      <td>0.057960</td>\n",
       "      <td>0.031215</td>\n",
       "      <td>0.020220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:31:15,806] Trial 135 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 136 with params: {'learning_rate': 1.689424523580871e-06, 'weight_decay': 0.003, 'adam_beta1': 0.96, 'warmup_steps': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2625/15750 02:40 < 13:22, 16.36 it/s, Epoch 5/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.815600</td>\n",
       "      <td>3.746221</td>\n",
       "      <td>0.190651</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>0.014340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.694800</td>\n",
       "      <td>3.645936</td>\n",
       "      <td>0.209899</td>\n",
       "      <td>0.018845</td>\n",
       "      <td>0.029953</td>\n",
       "      <td>0.016056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.602100</td>\n",
       "      <td>3.558832</td>\n",
       "      <td>0.210816</td>\n",
       "      <td>0.037059</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>0.018091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>3.478700</td>\n",
       "      <td>0.239230</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.038738</td>\n",
       "      <td>0.030147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.442100</td>\n",
       "      <td>3.403980</td>\n",
       "      <td>0.296059</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.055867</td>\n",
       "      <td>0.043845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:33:57,418] Trial 136 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 137 with params: {'learning_rate': 0.00012659250612851074, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:59, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.008400</td>\n",
       "      <td>1.267725</td>\n",
       "      <td>0.743355</td>\n",
       "      <td>0.383972</td>\n",
       "      <td>0.394545</td>\n",
       "      <td>0.372894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649900</td>\n",
       "      <td>1.026593</td>\n",
       "      <td>0.766269</td>\n",
       "      <td>0.480071</td>\n",
       "      <td>0.486115</td>\n",
       "      <td>0.474939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.989857</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.627870</td>\n",
       "      <td>0.579920</td>\n",
       "      <td>0.587063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>1.032018</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.666485</td>\n",
       "      <td>0.605481</td>\n",
       "      <td>0.622505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>1.047942</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.688351</td>\n",
       "      <td>0.622149</td>\n",
       "      <td>0.638168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>1.120836</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.749489</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.676783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>1.197357</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.794068</td>\n",
       "      <td>0.688508</td>\n",
       "      <td>0.712171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>1.207549</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.795098</td>\n",
       "      <td>0.714387</td>\n",
       "      <td>0.733379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>1.251225</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.781765</td>\n",
       "      <td>0.713202</td>\n",
       "      <td>0.729608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>1.301092</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.809925</td>\n",
       "      <td>0.722951</td>\n",
       "      <td>0.741578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>1.306988</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.725635</td>\n",
       "      <td>0.732414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.317395</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.776535</td>\n",
       "      <td>0.733582</td>\n",
       "      <td>0.738090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>1.363239</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.770524</td>\n",
       "      <td>0.736225</td>\n",
       "      <td>0.736862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.366729</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.806327</td>\n",
       "      <td>0.740778</td>\n",
       "      <td>0.755711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>1.454978</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.769282</td>\n",
       "      <td>0.724765</td>\n",
       "      <td>0.732539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.457071</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.789648</td>\n",
       "      <td>0.720551</td>\n",
       "      <td>0.738418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>1.426930</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.803567</td>\n",
       "      <td>0.742012</td>\n",
       "      <td>0.754588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.488504</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.770034</td>\n",
       "      <td>0.724020</td>\n",
       "      <td>0.728869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.445356</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783859</td>\n",
       "      <td>0.738648</td>\n",
       "      <td>0.745606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.465739</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.798149</td>\n",
       "      <td>0.735179</td>\n",
       "      <td>0.748393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.496241</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.736535</td>\n",
       "      <td>0.746649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.484943</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.782054</td>\n",
       "      <td>0.733381</td>\n",
       "      <td>0.742742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.540191</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.811483</td>\n",
       "      <td>0.739480</td>\n",
       "      <td>0.757467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.517722</td>\n",
       "      <td>0.801100</td>\n",
       "      <td>0.810104</td>\n",
       "      <td>0.747001</td>\n",
       "      <td>0.762295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.545449</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.822950</td>\n",
       "      <td>0.743286</td>\n",
       "      <td>0.763449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.562357</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.794503</td>\n",
       "      <td>0.739410</td>\n",
       "      <td>0.748896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.555702</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.798356</td>\n",
       "      <td>0.740395</td>\n",
       "      <td>0.751637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.557404</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.738499</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.564206</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.795458</td>\n",
       "      <td>0.738603</td>\n",
       "      <td>0.749135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.566070</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.797529</td>\n",
       "      <td>0.739857</td>\n",
       "      <td>0.750610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 17:49:58,574] Trial 137 finished with value: 0.7506098650354259 and parameters: {'learning_rate': 0.00012659250612851074, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 15}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 138 with params: {'learning_rate': 0.00032147533081042234, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:34 < 05:17, 16.54 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.378700</td>\n",
       "      <td>1.011717</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.496821</td>\n",
       "      <td>0.503515</td>\n",
       "      <td>0.491901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>1.067106</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.726728</td>\n",
       "      <td>0.642232</td>\n",
       "      <td>0.661287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>1.159223</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>0.722743</td>\n",
       "      <td>0.722379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>1.233416</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.810150</td>\n",
       "      <td>0.707814</td>\n",
       "      <td>0.729408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.240345</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.800517</td>\n",
       "      <td>0.702265</td>\n",
       "      <td>0.731110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>1.333337</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.804693</td>\n",
       "      <td>0.701285</td>\n",
       "      <td>0.732077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>1.378937</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.813969</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.738207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.414233</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.788478</td>\n",
       "      <td>0.718147</td>\n",
       "      <td>0.730689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.527082</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.793872</td>\n",
       "      <td>0.715781</td>\n",
       "      <td>0.728740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.535246</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.791523</td>\n",
       "      <td>0.731971</td>\n",
       "      <td>0.740264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.599987</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.803665</td>\n",
       "      <td>0.715567</td>\n",
       "      <td>0.738837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.582488</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.788390</td>\n",
       "      <td>0.720365</td>\n",
       "      <td>0.737142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>1.661174</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.711408</td>\n",
       "      <td>0.731584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.626588</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.790770</td>\n",
       "      <td>0.730621</td>\n",
       "      <td>0.747081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.723099</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.780813</td>\n",
       "      <td>0.726009</td>\n",
       "      <td>0.737082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.740177</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.764676</td>\n",
       "      <td>0.693549</td>\n",
       "      <td>0.709596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.795129</td>\n",
       "      <td>0.778185</td>\n",
       "      <td>0.763460</td>\n",
       "      <td>0.697667</td>\n",
       "      <td>0.712732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.833894</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.759199</td>\n",
       "      <td>0.698815</td>\n",
       "      <td>0.710043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.775796</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>0.689097</td>\n",
       "      <td>0.714532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.750395</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.771265</td>\n",
       "      <td>0.692840</td>\n",
       "      <td>0.710319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:00:34,567] Trial 138 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 139 with params: {'learning_rate': 0.0001470306243406786, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 16:00, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.877800</td>\n",
       "      <td>1.190850</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.398756</td>\n",
       "      <td>0.418218</td>\n",
       "      <td>0.395815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.998541</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.511246</td>\n",
       "      <td>0.508103</td>\n",
       "      <td>0.498670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.991286</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.644451</td>\n",
       "      <td>0.597946</td>\n",
       "      <td>0.605462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>1.059864</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.705372</td>\n",
       "      <td>0.622089</td>\n",
       "      <td>0.647908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>1.092770</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.746890</td>\n",
       "      <td>0.681847</td>\n",
       "      <td>0.699036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>1.149117</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>0.671298</td>\n",
       "      <td>0.699472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>1.241620</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.807699</td>\n",
       "      <td>0.708920</td>\n",
       "      <td>0.732779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.220495</td>\n",
       "      <td>0.802016</td>\n",
       "      <td>0.792915</td>\n",
       "      <td>0.738014</td>\n",
       "      <td>0.746296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>1.281229</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.783143</td>\n",
       "      <td>0.729937</td>\n",
       "      <td>0.742069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.331828</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.801712</td>\n",
       "      <td>0.731598</td>\n",
       "      <td>0.744047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.369664</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.764898</td>\n",
       "      <td>0.721469</td>\n",
       "      <td>0.726510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>1.400548</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.776204</td>\n",
       "      <td>0.718094</td>\n",
       "      <td>0.725756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.407965</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.766957</td>\n",
       "      <td>0.712684</td>\n",
       "      <td>0.720634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.433189</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.809659</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>0.744303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>1.483850</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.782519</td>\n",
       "      <td>0.734527</td>\n",
       "      <td>0.741468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.491722</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.807097</td>\n",
       "      <td>0.721586</td>\n",
       "      <td>0.744837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.500418</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.794592</td>\n",
       "      <td>0.743749</td>\n",
       "      <td>0.752027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.551119</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.787267</td>\n",
       "      <td>0.737565</td>\n",
       "      <td>0.744851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.519523</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.772705</td>\n",
       "      <td>0.739860</td>\n",
       "      <td>0.742717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.534805</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.790852</td>\n",
       "      <td>0.730516</td>\n",
       "      <td>0.740971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.548149</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.800540</td>\n",
       "      <td>0.736314</td>\n",
       "      <td>0.749340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.525993</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.801152</td>\n",
       "      <td>0.741297</td>\n",
       "      <td>0.750868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.584579</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.803277</td>\n",
       "      <td>0.734050</td>\n",
       "      <td>0.749163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.583475</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.808609</td>\n",
       "      <td>0.744484</td>\n",
       "      <td>0.759069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.610683</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.815035</td>\n",
       "      <td>0.735963</td>\n",
       "      <td>0.755001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.621071</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.780468</td>\n",
       "      <td>0.730145</td>\n",
       "      <td>0.742132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.611424</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.804417</td>\n",
       "      <td>0.734413</td>\n",
       "      <td>0.749716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.607868</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.802843</td>\n",
       "      <td>0.741042</td>\n",
       "      <td>0.752534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.617220</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.789816</td>\n",
       "      <td>0.723399</td>\n",
       "      <td>0.738981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.622096</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.799918</td>\n",
       "      <td>0.732945</td>\n",
       "      <td>0.746363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:16:37,524] Trial 139 finished with value: 0.7463632574954153 and parameters: {'learning_rate': 0.0001470306243406786, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 15}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 140 with params: {'learning_rate': 0.00013224486121218954, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:49, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.057800</td>\n",
       "      <td>1.262759</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.382843</td>\n",
       "      <td>0.396547</td>\n",
       "      <td>0.373449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>1.032404</td>\n",
       "      <td>0.768103</td>\n",
       "      <td>0.480880</td>\n",
       "      <td>0.483810</td>\n",
       "      <td>0.476760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.988354</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.624227</td>\n",
       "      <td>0.583652</td>\n",
       "      <td>0.589868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>1.027273</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.673821</td>\n",
       "      <td>0.606101</td>\n",
       "      <td>0.625579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>1.056237</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.721740</td>\n",
       "      <td>0.644104</td>\n",
       "      <td>0.662484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>1.123869</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.780738</td>\n",
       "      <td>0.672679</td>\n",
       "      <td>0.701422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>1.192315</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.827008</td>\n",
       "      <td>0.696715</td>\n",
       "      <td>0.728467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>1.195493</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.802216</td>\n",
       "      <td>0.716893</td>\n",
       "      <td>0.736478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.254764</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.795570</td>\n",
       "      <td>0.699742</td>\n",
       "      <td>0.724015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>1.325528</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.820197</td>\n",
       "      <td>0.715569</td>\n",
       "      <td>0.741557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>1.346527</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.774572</td>\n",
       "      <td>0.690652</td>\n",
       "      <td>0.712216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>1.335240</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.779225</td>\n",
       "      <td>0.716104</td>\n",
       "      <td>0.725169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.395700</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.783320</td>\n",
       "      <td>0.710992</td>\n",
       "      <td>0.726305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.372816</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.809475</td>\n",
       "      <td>0.725285</td>\n",
       "      <td>0.744580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.471351</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.806142</td>\n",
       "      <td>0.733984</td>\n",
       "      <td>0.748493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.456722</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.806533</td>\n",
       "      <td>0.728108</td>\n",
       "      <td>0.744180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.467831</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.812068</td>\n",
       "      <td>0.736027</td>\n",
       "      <td>0.753160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.531683</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.794315</td>\n",
       "      <td>0.727240</td>\n",
       "      <td>0.738688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.462944</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.783202</td>\n",
       "      <td>0.727877</td>\n",
       "      <td>0.738004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.526772</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.801583</td>\n",
       "      <td>0.721077</td>\n",
       "      <td>0.740093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.540528</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.780580</td>\n",
       "      <td>0.728871</td>\n",
       "      <td>0.736268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.526067</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.780404</td>\n",
       "      <td>0.729408</td>\n",
       "      <td>0.736817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.545295</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.785258</td>\n",
       "      <td>0.723579</td>\n",
       "      <td>0.734779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.541214</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.796175</td>\n",
       "      <td>0.721089</td>\n",
       "      <td>0.737639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.560766</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.806881</td>\n",
       "      <td>0.727634</td>\n",
       "      <td>0.745337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.576228</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.775249</td>\n",
       "      <td>0.728021</td>\n",
       "      <td>0.734383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.582146</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.801590</td>\n",
       "      <td>0.730530</td>\n",
       "      <td>0.744651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.573901</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.801851</td>\n",
       "      <td>0.726876</td>\n",
       "      <td>0.743316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.586674</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.776296</td>\n",
       "      <td>0.725068</td>\n",
       "      <td>0.735325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.589575</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.780865</td>\n",
       "      <td>0.733520</td>\n",
       "      <td>0.740690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:32:29,238] Trial 140 finished with value: 0.740690457093409 and parameters: {'learning_rate': 0.00013224486121218954, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 47}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 141 with params: {'learning_rate': 7.141808307151675e-05, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:10 < 10:20, 16.92 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.502300</td>\n",
       "      <td>1.703081</td>\n",
       "      <td>0.681943</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.305179</td>\n",
       "      <td>0.292087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.137500</td>\n",
       "      <td>1.188271</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.426055</td>\n",
       "      <td>0.422278</td>\n",
       "      <td>0.405032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>1.060088</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.469650</td>\n",
       "      <td>0.477135</td>\n",
       "      <td>0.460502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>1.007941</td>\n",
       "      <td>0.769936</td>\n",
       "      <td>0.499218</td>\n",
       "      <td>0.492561</td>\n",
       "      <td>0.486089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.546363</td>\n",
       "      <td>0.554695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>1.009257</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.660708</td>\n",
       "      <td>0.594666</td>\n",
       "      <td>0.609327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>1.041990</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.690981</td>\n",
       "      <td>0.616534</td>\n",
       "      <td>0.635775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>1.045234</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.719421</td>\n",
       "      <td>0.633779</td>\n",
       "      <td>0.655082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>1.085659</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.703091</td>\n",
       "      <td>0.628635</td>\n",
       "      <td>0.647119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>1.139007</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.726103</td>\n",
       "      <td>0.651093</td>\n",
       "      <td>0.669483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:37:40,644] Trial 141 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 142 with params: {'learning_rate': 0.0003766607477623454, 'weight_decay': 0.006, 'adam_beta1': 0.9, 'warmup_steps': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5250/15750 05:17 < 10:35, 16.53 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.145900</td>\n",
       "      <td>0.986591</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.571671</td>\n",
       "      <td>0.541234</td>\n",
       "      <td>0.538351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>1.117572</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.696294</td>\n",
       "      <td>0.665974</td>\n",
       "      <td>0.665445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>1.245067</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.759368</td>\n",
       "      <td>0.726144</td>\n",
       "      <td>0.729238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>1.300824</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.811120</td>\n",
       "      <td>0.703880</td>\n",
       "      <td>0.734273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.328176</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.811087</td>\n",
       "      <td>0.710390</td>\n",
       "      <td>0.737505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>1.390029</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.806548</td>\n",
       "      <td>0.677027</td>\n",
       "      <td>0.711601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.418555</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.767424</td>\n",
       "      <td>0.696943</td>\n",
       "      <td>0.711711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.527336</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.768002</td>\n",
       "      <td>0.697535</td>\n",
       "      <td>0.711939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>1.535500</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.755542</td>\n",
       "      <td>0.705814</td>\n",
       "      <td>0.712459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.517018</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.772800</td>\n",
       "      <td>0.696144</td>\n",
       "      <td>0.714931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:42:59,335] Trial 142 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 143 with params: {'learning_rate': 0.00040117968531803847, 'weight_decay': 0.008, 'adam_beta1': 0.9, 'warmup_steps': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/15750 10:33 < 05:16, 16.56 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.137000</td>\n",
       "      <td>0.993255</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.550957</td>\n",
       "      <td>0.547171</td>\n",
       "      <td>0.540467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>1.098529</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.707756</td>\n",
       "      <td>0.652581</td>\n",
       "      <td>0.663938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>1.169235</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.724840</td>\n",
       "      <td>0.725470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.318295</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.727398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>1.271994</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.806373</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.733775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.342072</td>\n",
       "      <td>0.800183</td>\n",
       "      <td>0.786425</td>\n",
       "      <td>0.704418</td>\n",
       "      <td>0.727428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>1.514403</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.803135</td>\n",
       "      <td>0.714263</td>\n",
       "      <td>0.735980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>1.546135</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.802435</td>\n",
       "      <td>0.707865</td>\n",
       "      <td>0.732380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>1.590392</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.795580</td>\n",
       "      <td>0.714910</td>\n",
       "      <td>0.735132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.622345</td>\n",
       "      <td>0.777269</td>\n",
       "      <td>0.791531</td>\n",
       "      <td>0.690920</td>\n",
       "      <td>0.721466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.638690</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.793248</td>\n",
       "      <td>0.711832</td>\n",
       "      <td>0.728009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.714346</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.791958</td>\n",
       "      <td>0.718087</td>\n",
       "      <td>0.737364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.705888</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.777981</td>\n",
       "      <td>0.710633</td>\n",
       "      <td>0.723607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>1.732696</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.776532</td>\n",
       "      <td>0.723461</td>\n",
       "      <td>0.731703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.845844</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.746302</td>\n",
       "      <td>0.700767</td>\n",
       "      <td>0.702416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.791869</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.769871</td>\n",
       "      <td>0.698730</td>\n",
       "      <td>0.709461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.819019</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.762032</td>\n",
       "      <td>0.715815</td>\n",
       "      <td>0.717441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>1.859184</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.773123</td>\n",
       "      <td>0.685848</td>\n",
       "      <td>0.709448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.884094</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.749755</td>\n",
       "      <td>0.684262</td>\n",
       "      <td>0.700549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.933208</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.767818</td>\n",
       "      <td>0.690262</td>\n",
       "      <td>0.713744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 18:53:34,519] Trial 143 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144 with params: {'learning_rate': 0.00018022936964075576, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 15:57, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.714600</td>\n",
       "      <td>1.111930</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.429791</td>\n",
       "      <td>0.446445</td>\n",
       "      <td>0.423249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.983629</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.597813</td>\n",
       "      <td>0.545795</td>\n",
       "      <td>0.548821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>1.028205</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.704342</td>\n",
       "      <td>0.631772</td>\n",
       "      <td>0.649346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>1.097979</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.754397</td>\n",
       "      <td>0.661686</td>\n",
       "      <td>0.689648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>1.149755</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.789857</td>\n",
       "      <td>0.712070</td>\n",
       "      <td>0.733505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>1.230175</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.775133</td>\n",
       "      <td>0.666504</td>\n",
       "      <td>0.697567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>1.283372</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.800852</td>\n",
       "      <td>0.690620</td>\n",
       "      <td>0.721478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>1.272397</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.782101</td>\n",
       "      <td>0.718634</td>\n",
       "      <td>0.728795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.327727</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.794861</td>\n",
       "      <td>0.715673</td>\n",
       "      <td>0.736224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.373839</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.713585</td>\n",
       "      <td>0.734986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.451464</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.790260</td>\n",
       "      <td>0.713913</td>\n",
       "      <td>0.735751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.455084</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.779906</td>\n",
       "      <td>0.717382</td>\n",
       "      <td>0.727754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.489181</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.765655</td>\n",
       "      <td>0.716840</td>\n",
       "      <td>0.724327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>1.487859</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.814356</td>\n",
       "      <td>0.728544</td>\n",
       "      <td>0.751592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>1.517696</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.806233</td>\n",
       "      <td>0.712742</td>\n",
       "      <td>0.736123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.519825</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.817264</td>\n",
       "      <td>0.706288</td>\n",
       "      <td>0.737637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.603970</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.805937</td>\n",
       "      <td>0.717850</td>\n",
       "      <td>0.739909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>1.649785</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.801662</td>\n",
       "      <td>0.705471</td>\n",
       "      <td>0.728910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.585050</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>0.720725</td>\n",
       "      <td>0.741033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.608555</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.797492</td>\n",
       "      <td>0.723002</td>\n",
       "      <td>0.743178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.621144</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.783806</td>\n",
       "      <td>0.716821</td>\n",
       "      <td>0.735211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.614099</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.805223</td>\n",
       "      <td>0.709514</td>\n",
       "      <td>0.735333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.638578</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.798012</td>\n",
       "      <td>0.717548</td>\n",
       "      <td>0.739970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.627762</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.814198</td>\n",
       "      <td>0.726463</td>\n",
       "      <td>0.751320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.655444</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.809023</td>\n",
       "      <td>0.727260</td>\n",
       "      <td>0.749895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.687868</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.800745</td>\n",
       "      <td>0.712172</td>\n",
       "      <td>0.737767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.705631</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.808762</td>\n",
       "      <td>0.712665</td>\n",
       "      <td>0.739652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.696453</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.800821</td>\n",
       "      <td>0.718522</td>\n",
       "      <td>0.742291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.694837</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.803185</td>\n",
       "      <td>0.719307</td>\n",
       "      <td>0.743479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.699356</td>\n",
       "      <td>0.794684</td>\n",
       "      <td>0.801794</td>\n",
       "      <td>0.719341</td>\n",
       "      <td>0.742858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 19:09:34,149] Trial 144 finished with value: 0.7428583967330559 and parameters: {'learning_rate': 0.00018022936964075576, 'weight_decay': 0.007, 'adam_beta1': 0.9, 'warmup_steps': 18}. Best is trial 101 with value: 0.7580381076828203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 145 with params: {'learning_rate': 0.00014074166190486731, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8687' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8687/15750 08:33 < 06:57, 16.91 it/s, Epoch 16.54/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.972100</td>\n",
       "      <td>1.222858</td>\n",
       "      <td>0.748854</td>\n",
       "      <td>0.384880</td>\n",
       "      <td>0.405142</td>\n",
       "      <td>0.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.591200</td>\n",
       "      <td>1.017145</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.489351</td>\n",
       "      <td>0.493473</td>\n",
       "      <td>0.484553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.991532</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.651248</td>\n",
       "      <td>0.595188</td>\n",
       "      <td>0.606448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>1.044246</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.717364</td>\n",
       "      <td>0.626150</td>\n",
       "      <td>0.650823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>1.075532</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.767169</td>\n",
       "      <td>0.691195</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>1.146067</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.774485</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>1.216859</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.687725</td>\n",
       "      <td>0.716664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>1.217579</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.783812</td>\n",
       "      <td>0.719046</td>\n",
       "      <td>0.731645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>1.286508</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.787690</td>\n",
       "      <td>0.688259</td>\n",
       "      <td>0.716363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>1.349379</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.801483</td>\n",
       "      <td>0.707703</td>\n",
       "      <td>0.729129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>1.363677</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.776110</td>\n",
       "      <td>0.713896</td>\n",
       "      <td>0.725909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.369400</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.764874</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.726019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.423137</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.786233</td>\n",
       "      <td>0.711173</td>\n",
       "      <td>0.728139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>1.392298</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.793977</td>\n",
       "      <td>0.712311</td>\n",
       "      <td>0.733716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.518544</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.802538</td>\n",
       "      <td>0.726314</td>\n",
       "      <td>0.743131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.511653</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.803479</td>\n",
       "      <td>0.709126</td>\n",
       "      <td>0.733438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-24023 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "[W 2025-03-16 19:18:09,168] Trial 145 failed with parameters: {'learning_rate': 0.00014074166190486731, 'weight_decay': 0.007, 'adam_beta1': 0.91, 'warmup_steps': 38} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 250, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2241, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3698, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3759, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1673, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 261, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "[W 2025-03-16 19:18:09,175] Trial 145 failed with value None.\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_trial3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptuna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhp_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_objective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpruner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest-base-aug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3588\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   3585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m hp_name\n\u001b[1;32m   3586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_objective \u001b[38;5;241m=\u001b[39m default_compute_objective \u001b[38;5;28;01mif\u001b[39;00m compute_objective \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m compute_objective\n\u001b[0;32m-> 3588\u001b[0m best_run \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_run\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/hyperparameter_search.py:72\u001b[0m, in \u001b[0;36mOptunaBackend.run\u001b[0;34m(self, trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, n_trials: \u001b[38;5;28mint\u001b[39m, direction: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_hp_search_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py:268\u001b[0m, in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m directions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m direction\n\u001b[1;32m    267\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39mdirection, directions\u001b[38;5;241m=\u001b[39mdirections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 268\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    270\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py:250\u001b[0m, in \u001b[0;36mrun_hp_search_optuna.<locals>._objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    248\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:1673\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_trial3 = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name=\"Test-base-aug\",\n",
    "    n_trials=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a68e47b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_trial3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbest_trial3\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_trial3' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_trial3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60102d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ac624",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill_fine_aug_hp-search\", logging_dir=f\"~/logs/{DATASET}/bert-distill_fine_aug_hp-search\", remove_unused_columns=False, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    params =  {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 1e-2, step=1e-3),\n",
    "        \"adam_beta1\" : trial.suggest_float(\"adam_beta1\", 0.9, 0.99, step=0.01),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, warm_up),\n",
    "        \"lambda_param\": trial.suggest_float(\"lambda_param\",0,1,step=.1),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 2,7, step=.5)\n",
    "    }\n",
    "    print(f\"Trial {trial.number} with params: {params}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c11b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.HyperbandPruner(min_resource=min_r, max_resource=max_r, reduction_factor=2, bootstrap_count=2)\n",
    "sampler = optuna.samplers.TPESampler(seed=42, multivariate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b353692",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    model_init = lambda: get_Bert(),\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial4 = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1\"],\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name=\"Test-Distill-aug\",\n",
    "    n_trials=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='136', objective=0.7874461791210883, hyperparameters={'learning_rate': 0.0021806066601338593, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 50, 'lambda_param': 0.8, 'temperature': 4.0}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_trial4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349df401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best normal training score:  BestRun(run_id='41', objective=0.7157156862853267, hyperparameters={'learning_rate': 0.004873101422020569, 'weight_decay': 0.001, 'adam_beta1': 0.93, 'warmup_steps': 4}, run_summary=None)\n",
      "Best distilation trianing score:  BestRun(run_id='115', objective=0.7423955520558099, hyperparameters={'learning_rate': 0.004092058596290564, 'weight_decay': 0.008, 'adam_beta1': 0.92, 'warmup_steps': 2, 'lambda_param': 1.0, 'temperature': 4.0}, run_summary=None)\n",
      "Best normal training score with augmentations:  BestRun(run_id='69', objective=0.7829410034428768, hyperparameters={'learning_rate': 0.0014622771684147115, 'weight_decay': 0.006, 'adam_beta1': 0.93, 'warmup_steps': 46}, run_summary=None)\n",
      "Best distilation trianing score with augmentations:  BestRun(run_id='136', objective=0.7874461791210883, hyperparameters={'learning_rate': 0.0021806066601338593, 'weight_decay': 0.001, 'adam_beta1': 0.9, 'warmup_steps': 50, 'lambda_param': 0.8, 'temperature': 4.0}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best normal training score: \", best_trial)\n",
    "print(\"Best distilation trianing score: \", best_trial2)\n",
    "print(\"Best normal training score with augmentations: \", best_trial3)\n",
    "print(\"Best distilation trianing score with augmentations: \",best_trial4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
