{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Předzpracování datasetu DBpedia\n",
    "\n",
    "Tento notebook slouží k předzpracování datasetu DBpedia. Pro dataset jsou vytvořeny augmentované záznamy a předpočítány logity.\n",
    "Nejprve jsou načteny všechny potřebné knihovny včetně vlastní sbírky objektů a funkcí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset, concatenate_datasets, ClassLabel, Features, Value, Sequence\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BasicTokenizer\n",
    "from torch.utils.data import  DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import base\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ověření, že GPU je k dispozici a balíček torch je správně nakonfigurován."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konfigurace augmentačních parametrů.\n",
    "Dataset se bude procházet pouze jednou (vzhledem ke své velikosti), každý token v záznamu bude na deset procent zamaskován a na třicet procent nahrazen jiným tokenem se stejným POS tagem. Po průchodu všech tokenů v záznamu může dojít s pravděpodobností dvacet procent ke zkrácení záznamu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_params = {\"n_iter\": 1, \"p_mask\":0.1, \"p_pos\": 0.3, \"p_ng\":0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Získání datasetu, základního tokenizeru a jeho použití nad datasetem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer(do_lower_case=True)\n",
    "DATASET = \"dbpedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_from_disk(f\"~/data/{DATASET}/train\")\n",
    "sentences = list(map(lambda e: e[\"sentence\"], train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pohled na průměrné délky záznamů dle počtu tokenů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths = [len(tokenizer.tokenize(sentence)) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_token_lengths = sorted(token_lengths, reverse=True)\n",
    "avg_tokens = np.mean(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1498, 1283, 1211, 989, 734, 655, 551, 516, 501, 433, 410, 394, 384, 364, 354, 338, 336, 331, 313, 311, 308, 307, 306, 301, 294]\n",
      "53.53230357142857\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_token_lengths[0:25])\n",
    "print(avg_tokens)\n",
    "print(sorted_token_lengths[-25:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Získání POS tagů jednotlivých tokenů v záznamech pro potřeby augmentace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_word_map_list = base.get_pos_tag_word_map(sentences, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 8, 'sentence': ' Zarrin Darreh (Persian: زرين دره\\u200e also Romanized as Zarrīn Darreh) is a village in Chahriq Rural District Kuhsar District Salmas County West Azerbaijan Province Iran. At the 2006 census its population was 37 in 8 families.'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spuštění procesu augmentace s popsanými parametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_datasets = base.get_augmented_dataset(augmentation_params, train_data, pos_tag_word_map_list, tokenizer=tokenizer, include_idx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'sentence'])\n"
     ]
    }
   ],
   "source": [
    "print(augmented_datasets[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Převedení nových záznamů do dataset objektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4301190d36894d669c5b005bb855a6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/448000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aug_datasets_formated = []\n",
    "ds_schema = Features({\n",
    "    \"sentence\": Value(\"string\"),\n",
    "    \"label\": ClassLabel(names=[\"Company\", \"EducationalInstitution\", \"Artist\", \"Athlete\", \"OfficeHolder\", \"MeanOfTransportation\", \"Building\", \"NaturalPlace\", \"Village\", \"Animal\", \"Plant\", \"Album\", \"Film\", \"WrittenWork\"])\n",
    "})\n",
    "\n",
    "for iter in augmented_datasets:\n",
    "    dataset = Dataset.from_dict(iter)\n",
    "    dataset = dataset.cast(ds_schema)\n",
    "    aug_datasets_formated.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ověření efektu augmentace, zde např. maskování tokenů, změna tokenů i zkrácení celého záznamu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'zarrin [MASK] persian : [MASK] - romaine zarrin in southeast france . the zarrin is listed as a monument historique as 1939', 'label': 6}\n",
      "{'label': 6, 'sentence': \" Château d'Alba-la-Romaine is a château in Alba-la-Romaine Ardèche in southeast France.The castle is listed as a Monument historique since 1939 by the French Ministry of Culture.\"}\n"
     ]
    }
   ],
   "source": [
    "print(aug_datasets_formated[0][70])\n",
    "print(train_data[70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načtení učitelského modelu a jeho tokenizeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"fabriceyhc/bert-base-uncased-dbpedia_14\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-dbpedia_14\", num_labels=14)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/teacher.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Předzpracování trénovací části datasetu a výpočet logitů učitelským modelem. Vypočtené logity se přidávají jako nový sloupec datasetu, naopak se odstraňují záznamy ponechané po tokenizeru učitele, které dále nejsou třeba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151bfbb9c5284b97a7af4f2dde3de724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = base.prepare_dataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "train_logits = base.generate_logits(train_dataloader, model)\n",
    "train_dataset = train_dataset.add_column(\"logits\", train_logits)\n",
    "train_dataset = train_dataset.remove_columns([\"token_type_ids\", \"attention_mask\", \"input_ids\"])\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výpočet správnosti učitelských predikcí nad trénovací částí datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca13f2a2ba64d5bb7d19bbd5640f64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/448000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base dataset:  0.9887633928571429\n"
     ]
    }
   ],
   "source": [
    "print(base.check_acc(train_dataset, \"Accuracy for base dataset: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stejné kroky pro výpočet logitů jsou provedeny i pro augmentovanou variantu datasetu. Připraveno pro potřeby většího množství průchodů augmentace. Dataset je nejprve tokenizován a následně je předán učitely k inferenci pro získání logitů, které jsou k datasetu uloženy. Odstraněny jsou poté nepotřebné pozůstatky tokenizace. V rámci průběhu zpracování rovnou dochází k filtraci augmentovaných záznamů dle popsaného mechanismu (notebook precompute_logits_10.ipynb).\n",
    "\n",
    "Takto zpracované datasety se postupně spojují do jednoho celku. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573520eb1f30413689a36f6b5ed38d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing augmented datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07b11c38f7d44839c9c3044ff68aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the provided dataset:   0%|          | 0/448000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6bf96fc9954c1a8a8d9d5b4e1d514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9369050f164d4d8db341e1bbfc69f047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/448000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for augmented dataset:  0.9568727678571428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73a7cb270e649ba8d74e0461f1a54cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing entries from augmented dataset that are different from the base one - based on saved logits:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741a9d607d424101a1d3945630320ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/431401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for filtered dataset:  0.9914766076110162\n"
     ]
    }
   ],
   "source": [
    "aug_clean_datasets = []\n",
    "for dataset in tqdm(aug_datasets_formated, total=(len(aug_datasets_formated)), desc=\"Processing augmented datasets: \"):\n",
    "    aug_train_dataset = base.prepare_dataset(dataset, tokenizer)\n",
    "    aug_train_dataloader = DataLoader(aug_train_dataset, batch_size=128, shuffle=False)\n",
    "    aug_train_logits = base.generate_logits(aug_train_dataloader, model)\n",
    "    aug_train_dataset = aug_train_dataset.add_column(\"logits\", aug_train_logits)\n",
    "    aug_train_dataset = aug_train_dataset.remove_columns([\"token_type_ids\", \"attention_mask\", \"input_ids\"])\n",
    "    aug_train_dataset.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")\n",
    "\n",
    "    print(base.check_acc(aug_train_dataset, \"Accuracy for augmented dataset: \"))\n",
    "\n",
    "    aug_train_dataset = base.remove_diff_pred_class(train_dataset, aug_train_dataset, pytorch_dataset=False)\n",
    "    \n",
    "    print(base.check_acc(aug_train_dataset, \"Accuracy for filtered dataset: \"))\n",
    "\n",
    "    aug_train_dataset.reset_format()\n",
    "    aug_clean_datasets.extend(aug_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ClassLabel(names=['Company', 'EducationalInstitution', 'Artist', 'Athlete', 'OfficeHolder', 'MeanOfTransportation', 'Building', 'NaturalPlace', 'Village', 'Animal', 'Plant', 'Album', 'Film', 'WrittenWork'], id=None), 'sentence': Value(dtype='string', id=None), 'logits': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Převedení spojeného a zpracovaného augmentovaného datasetu do dataset objektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84571049d3954a4f991895340ebb5c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/431401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_schema = Features({\n",
    "    \"sentence\": Value(\"string\"),\n",
    "    \"labels\": ClassLabel(names=[\"Company\", \"EducationalInstitution\", \"Artist\", \"Athlete\", \"OfficeHolder\", \"MeanOfTransportation\", \"Building\", \"NaturalPlace\", \"Village\", \"Animal\", \"Plant\", \"Album\", \"Film\", \"WrittenWork\"]),\n",
    "    \"logits\": Sequence(feature=Value(dtype=\"float32\")),\n",
    "})\n",
    "\n",
    "aug_dataset = Dataset.from_list(aug_clean_datasets)\n",
    "aug_dataset = aug_dataset.cast(ds_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výpočet správnosti nad zpracovanými datasety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c050daac9853441ca974d4c6b5f96f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/448000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base dataset:  0.9887633928571429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f853583d89ad4c5c880d374c9aa1c40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/431401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for augmented dataset:  0.9914766076110162\n"
     ]
    }
   ],
   "source": [
    "print(base.check_acc(train_dataset, \"Accuracy for base dataset: \"))\n",
    "print(base.check_acc(aug_dataset, \"Accuracy for augmented dataset: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spojení původního a augmentovaného datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_data = concatenate_datasets([train_dataset, aug_dataset])\n",
    "train_all_data.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Získání správnosti nad touto kombinací."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10173243fcdd41a4bba43501dc7980c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/879401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for combined dataset:  0.9900943937975963\n"
     ]
    }
   ],
   "source": [
    "print(base.check_acc(train_all_data, \"Accuracy for combined dataset: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'sentence', 'logits']\n"
     ]
    }
   ],
   "source": [
    "print(train_all_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_data.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uložení zpracovaných datasetů na disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73a3d0036ac45e9bed4e7f342ff29b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/879401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_all_data.save_to_disk(f\"~/data/{DATASET}/train-logits-augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2a2708f748439e83b2fa1234f1f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/448000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.reset_format()\n",
    "train_dataset.save_to_disk(f\"~/data/{DATASET}/train-logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načtení zbylých částí datasetu (validační a testovací). Výpočet logitů je proveden pro každou z těchto částí stejným způsobem jako v příapdě trénovací části.\n",
    "\n",
    "Nejprve je část tokenizována učitelem a vložena do dataloaderu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_from_disk(f\"~/data/{DATASET}/eval\")\n",
    "\n",
    "eval_dataset = base.prepare_dataset(eval_data, tokenizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_from_disk(f\"~/data/{DATASET}/test\")\n",
    "\n",
    "test_dataset = base.prepare_dataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Následně jsou pro každou část spočteny logity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9ae6730a9464ba924968efea9e130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765f5a53df1487391023f1aba77dec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_logits = base.generate_logits(eval_dataloader, model)\n",
    "test_logits = base.generate_logits(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logity jsou přidány jako nový sloupec, odstraněny jsou již nepotřebné pozůstatky tokenizace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.reset_format()\n",
    "eval_dataset = eval_dataset.add_column(\"logits\", eval_logits)\n",
    "eval_dataset = eval_dataset.remove_columns([\"token_type_ids\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.reset_format()\n",
    "test_dataset = test_dataset.add_column(\"logits\", test_logits)\n",
    "test_dataset = test_dataset.remove_columns([\"token_type_ids\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecf5acab108470495a159ace3b04928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/112000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddaf5dc0df04598bbbbaf1e9e10bf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset.save_to_disk(f\"~/data/{DATASET}/eval-logits\")\n",
    "test_dataset.save_to_disk(f\"~/data/{DATASET}/test-logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uložení dat na disk a vypočtení správnosti predikcí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cf10755c6b4b928da7a5328b5af64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/112000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base eval dataset:  0.9881785714285715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14910869fee47b3a2f7dba02c551127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating accuracy based on the saved logits:   0%|          | 0/70000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for base test dataset:  0.9883285714285714\n"
     ]
    }
   ],
   "source": [
    "eval_data = load_from_disk(f\"~/data/{DATASET}/eval-logits\")\n",
    "test_data = load_from_disk(f\"~/data/{DATASET}/test-logits\")\n",
    "\n",
    "eval_data.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")\n",
    "test_data.set_format(type=\"torch\", columns=[\"logits\", \"labels\"], device=\"cpu\")\n",
    "\n",
    "print(base.check_acc(eval_data, \"Accuracy for base eval dataset: \"))\n",
    "print(base.check_acc(test_data, \"Accuracy for base test dataset: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výpočet výkonnostních metrik a velikosti u učitelského modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = base.prepare_dataset(train_data, tokenizer)\n",
    "\n",
    "train_data_gpu = copy.deepcopy(train_dataset)\n",
    "train_data_gpu.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cuda\")\n",
    "gpu_data_loader = DataLoader(train_data_gpu, batch_size=1, shuffle=False)\n",
    "\n",
    "train_data_cpu = copy.deepcopy(train_dataset)\n",
    "train_data_cpu.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cpu\")\n",
    "cpu_data_loader = DataLoader(train_data_cpu, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 417.690MB.\n",
      "Total Trainable Params: 109493006.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Modules</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert.embeddings.word_embeddings.weight</td>\n",
       "      <td>23440896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert.embeddings.position_embeddings.weight</td>\n",
       "      <td>393216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert.embeddings.token_type_embeddings.weight</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert.embeddings.LayerNorm.weight</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert.embeddings.LayerNorm.bias</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>bert.encoder.layer.11.output.LayerNorm.bias</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>bert.pooler.dense.weight</td>\n",
       "      <td>589824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>bert.pooler.dense.bias</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>classifier.weight</td>\n",
       "      <td>10752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>classifier.bias</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Modules Parameters\n",
       "0          bert.embeddings.word_embeddings.weight   23440896\n",
       "1      bert.embeddings.position_embeddings.weight     393216\n",
       "2    bert.embeddings.token_type_embeddings.weight       1536\n",
       "3                bert.embeddings.LayerNorm.weight        768\n",
       "4                  bert.embeddings.LayerNorm.bias        768\n",
       "..                                            ...        ...\n",
       "196   bert.encoder.layer.11.output.LayerNorm.bias        768\n",
       "197                      bert.pooler.dense.weight     589824\n",
       "198                        bert.pooler.dense.bias        768\n",
       "199                             classifier.weight      10752\n",
       "200                               classifier.bias         14\n",
       "\n",
       "[201 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7bc898ae19c0>\n",
      "self.infer_speed_comp()\n",
      "  49.43 ms\n",
      "  1 measurement, 1000 runs , 4 threads\n"
     ]
    }
   ],
   "source": [
    "cpu_benchmark = base.BenchMarkRunner(model, cpu_data_loader, \"cpu\", 1000)\n",
    "print(cpu_benchmark.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7bc7fa81f7c0>\n",
      "self.infer_speed_comp()\n",
      "  5.80 ms\n",
      "  1 measurement, 1000 runs , 4 threads\n"
     ]
    }
   ],
   "source": [
    "gpu_benchmark = base.BenchMarkRunner(model, gpu_data_loader, \"cuda\", 1000)\n",
    "print(gpu_benchmark.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9883300032042014\n",
      "Accuracy: 0.9883285714285714\n",
      "Precision: 0.9883487370157686\n",
      "Recall: 0.9883285714285714\n"
     ]
    }
   ],
   "source": [
    "base.get_scores(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
