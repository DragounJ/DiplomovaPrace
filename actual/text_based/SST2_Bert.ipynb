{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torchtext není k dispozici pro poslední verzi pytorch, budeme tedy využuívat něco jiného ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, BertForSequenceClassification, BertTokenizer, EarlyStoppingCallback\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import base\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_from_disk(f\"~/data/{DATASET}/train-logits\")\n",
    "eval = load_from_disk(f\"~/data/{DATASET}/eval-logits\")\n",
    "test = load_from_disk(f\"~/data/{DATASET}/test-logits\")\n",
    "\n",
    "train_aug = load_from_disk(f\"~/data/{DATASET}/train-logits-augmented\")\n",
    "test_blank= load_from_disk(f\"~/data/{DATASET}/test-blank-logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"gchhablani/bert-base-cased-finetuned-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a186c633d442c88255e8e6d1c01a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the train dataset:   0%|          | 0/53879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71012e773152432e8bda80ae6633c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the eval dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b6de5c1be54572a0c0d9952db25356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the test dataset:   0%|          | 0/13470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de28dba24b5461aaee479c0f9a99c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the augmented dataset:   0%|          | 0/293636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdfe561203d4c7f801aaf2dee3a0373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the blank test dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the train dataset\")\n",
    "eval = eval.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the eval dataset\")\n",
    "test = test.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the test dataset\")\n",
    "\n",
    "train_aug = train_aug.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the augmented dataset\")\n",
    "test_blank = test_blank.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the blank test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gpu = copy.deepcopy(train)\n",
    "train_data_gpu.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cuda\")\n",
    "gpu_data_loader = DataLoader(train_data_gpu, batch_size=1, shuffle=False)\n",
    "\n",
    "train_data_cpu = copy.deepcopy(train)\n",
    "train_data_cpu.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cpu\")\n",
    "cpu_data_loader = DataLoader(train_data_cpu, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base\", logging_dir=f\"~/logs/{DATASET}/bert-base\", lr=.00003, epochs=20, weight_decay=0.008, warmup_steps=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7157' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7157/8420 04:21 < 00:46, 27.36 it/s, Epoch 17/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.618141</td>\n",
       "      <td>0.685780</td>\n",
       "      <td>0.685718</td>\n",
       "      <td>0.685464</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.525342</td>\n",
       "      <td>0.744266</td>\n",
       "      <td>0.746798</td>\n",
       "      <td>0.743233</td>\n",
       "      <td>0.743008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.488910</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.771295</td>\n",
       "      <td>0.770102</td>\n",
       "      <td>0.770206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.489184</td>\n",
       "      <td>0.783257</td>\n",
       "      <td>0.784527</td>\n",
       "      <td>0.782573</td>\n",
       "      <td>0.782678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.476693</td>\n",
       "      <td>0.799312</td>\n",
       "      <td>0.799304</td>\n",
       "      <td>0.799139</td>\n",
       "      <td>0.799195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.462904</td>\n",
       "      <td>0.805046</td>\n",
       "      <td>0.805066</td>\n",
       "      <td>0.804854</td>\n",
       "      <td>0.804922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.490490</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>0.798748</td>\n",
       "      <td>0.794919</td>\n",
       "      <td>0.794964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.474048</td>\n",
       "      <td>0.797018</td>\n",
       "      <td>0.797041</td>\n",
       "      <td>0.797139</td>\n",
       "      <td>0.797005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.476259</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.808091</td>\n",
       "      <td>0.807780</td>\n",
       "      <td>0.807323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.484224</td>\n",
       "      <td>0.806193</td>\n",
       "      <td>0.806144</td>\n",
       "      <td>0.806233</td>\n",
       "      <td>0.806162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.256300</td>\n",
       "      <td>0.504617</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>0.796962</td>\n",
       "      <td>0.795256</td>\n",
       "      <td>0.795397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.498673</td>\n",
       "      <td>0.809633</td>\n",
       "      <td>0.809658</td>\n",
       "      <td>0.809443</td>\n",
       "      <td>0.809512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.502412</td>\n",
       "      <td>0.809633</td>\n",
       "      <td>0.809633</td>\n",
       "      <td>0.809737</td>\n",
       "      <td>0.809617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.500506</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>0.813096</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.813061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>0.514002</td>\n",
       "      <td>0.806193</td>\n",
       "      <td>0.806239</td>\n",
       "      <td>0.805980</td>\n",
       "      <td>0.806058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.514268</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.807320</td>\n",
       "      <td>0.807190</td>\n",
       "      <td>0.807238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.523821</td>\n",
       "      <td>0.808486</td>\n",
       "      <td>0.808451</td>\n",
       "      <td>0.808359</td>\n",
       "      <td>0.808395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7157, training_loss=0.32444957303021466, metrics={'train_runtime': 262.4925, 'train_samples_per_second': 4105.184, 'train_steps_per_second': 32.077, 'total_flos': 681851783718000.0, 'train_loss': 0.32444957303021466, 'epoch': 17.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/106 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2646693289279938,\n",
       " 'eval_accuracy': 0.8999257609502599,\n",
       " 'eval_precision': 0.8981981941832378,\n",
       " 'eval_recall': 0.8992998607658361,\n",
       " 'eval_f1': 0.898712891806191,\n",
       " 'eval_runtime': 5.7292,\n",
       " 'eval_samples_per_second': 2351.127,\n",
       " 'eval_steps_per_second': 18.502,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-base.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703a55183f6c40f396c8c754a6d9bb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_blank.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=\"cuda\")\n",
    "test_blank_dataloader = DataLoader(test_blank, batch_size=128, shuffle=False)\n",
    "test_blank_logits = base.generate_logits(test_blank_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output file named: /home/jovyan/data/sst2/tiny-bert-base-test.tsv upload it to GLUE benchmark to obtain results!\n"
     ]
    }
   ],
   "source": [
    "base.generate_real_test_file_sst2(test_blank_logits, f\"{os.path.expanduser('~')}/data/{DATASET}/tiny-bert-base-test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill\", logging_dir=f\"~/logs/{DATASET}/bert-distill\", remove_unused_columns=False, lr=0.00005, weight_decay=0.08, epochs=20, temp=6.5, lambda_param=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7999' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7999/8420 07:28 < 00:23, 17.85 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.565100</td>\n",
       "      <td>3.393508</td>\n",
       "      <td>0.712156</td>\n",
       "      <td>0.714411</td>\n",
       "      <td>0.711070</td>\n",
       "      <td>0.710646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.254500</td>\n",
       "      <td>2.743101</td>\n",
       "      <td>0.766055</td>\n",
       "      <td>0.766889</td>\n",
       "      <td>0.766524</td>\n",
       "      <td>0.766024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.404400</td>\n",
       "      <td>2.444158</td>\n",
       "      <td>0.786697</td>\n",
       "      <td>0.786638</td>\n",
       "      <td>0.786583</td>\n",
       "      <td>0.786606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.922400</td>\n",
       "      <td>2.426809</td>\n",
       "      <td>0.778670</td>\n",
       "      <td>0.783258</td>\n",
       "      <td>0.777437</td>\n",
       "      <td>0.777193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.636400</td>\n",
       "      <td>2.362908</td>\n",
       "      <td>0.783257</td>\n",
       "      <td>0.783663</td>\n",
       "      <td>0.782826</td>\n",
       "      <td>0.782946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.441200</td>\n",
       "      <td>2.352273</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.789473</td>\n",
       "      <td>0.788541</td>\n",
       "      <td>0.788670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.318700</td>\n",
       "      <td>2.393249</td>\n",
       "      <td>0.797018</td>\n",
       "      <td>0.797183</td>\n",
       "      <td>0.796718</td>\n",
       "      <td>0.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.203600</td>\n",
       "      <td>2.335956</td>\n",
       "      <td>0.799312</td>\n",
       "      <td>0.799409</td>\n",
       "      <td>0.799055</td>\n",
       "      <td>0.799147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.127900</td>\n",
       "      <td>2.415294</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.801064</td>\n",
       "      <td>0.800855</td>\n",
       "      <td>0.800449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.071300</td>\n",
       "      <td>2.436018</td>\n",
       "      <td>0.805046</td>\n",
       "      <td>0.805156</td>\n",
       "      <td>0.805233</td>\n",
       "      <td>0.805042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.016300</td>\n",
       "      <td>2.440127</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.801758</td>\n",
       "      <td>0.799802</td>\n",
       "      <td>0.799949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>2.377260</td>\n",
       "      <td>0.803899</td>\n",
       "      <td>0.804004</td>\n",
       "      <td>0.803644</td>\n",
       "      <td>0.803738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.948100</td>\n",
       "      <td>2.462492</td>\n",
       "      <td>0.808486</td>\n",
       "      <td>0.808673</td>\n",
       "      <td>0.808190</td>\n",
       "      <td>0.808302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>2.438192</td>\n",
       "      <td>0.801606</td>\n",
       "      <td>0.801557</td>\n",
       "      <td>0.801644</td>\n",
       "      <td>0.801574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.889300</td>\n",
       "      <td>2.509079</td>\n",
       "      <td>0.799312</td>\n",
       "      <td>0.800177</td>\n",
       "      <td>0.798760</td>\n",
       "      <td>0.798910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.881900</td>\n",
       "      <td>2.432499</td>\n",
       "      <td>0.808486</td>\n",
       "      <td>0.808418</td>\n",
       "      <td>0.808443</td>\n",
       "      <td>0.808430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>2.484718</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.800644</td>\n",
       "      <td>0.800455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.837600</td>\n",
       "      <td>2.463926</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.807417</td>\n",
       "      <td>0.807106</td>\n",
       "      <td>0.807193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.819300</td>\n",
       "      <td>2.478650</td>\n",
       "      <td>0.806193</td>\n",
       "      <td>0.806562</td>\n",
       "      <td>0.805812</td>\n",
       "      <td>0.805947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7999, training_loss=1.4788935866739201, metrics={'train_runtime': 448.42, 'train_samples_per_second': 2403.06, 'train_steps_per_second': 18.777, 'total_flos': 762069640626000.0, 'train_loss': 1.4788935866739201, 'epoch': 19.0})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/106 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1506083011627197,\n",
       " 'eval_accuracy': 0.9138084632516704,\n",
       " 'eval_precision': 0.9120214133331037,\n",
       " 'eval_recall': 0.913884299788369,\n",
       " 'eval_f1': 0.9128467149013444,\n",
       " 'eval_runtime': 6.3788,\n",
       " 'eval_samples_per_second': 2111.697,\n",
       " 'eval_steps_per_second': 16.618,\n",
       " 'epoch': 19.0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-distil.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b10b2edcf745e08b8ed7da2683f15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output file named: /home/jovyan/data/sst2/tiny-bert-distill-test.tsv upload it to GLUE benchmark to obtain results!\n"
     ]
    }
   ],
   "source": [
    "test_blank_logits = base.generate_logits(test_blank_dataloader, student_model)\n",
    "base.generate_real_test_file_sst2(test_blank_logits, f\"{os.path.expanduser('~')}/data/{DATASET}/tiny-bert-distill-test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-base-aug\", logging_dir=f\"~/logs/{DATASET}/bert-base-aug\", epochs=20, lr=0.00001, weight_decay=0.007, warmup_steps=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22950' max='45900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22950/45900 18:11 < 18:11, 21.02 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603600</td>\n",
       "      <td>0.545399</td>\n",
       "      <td>0.721330</td>\n",
       "      <td>0.726085</td>\n",
       "      <td>0.722563</td>\n",
       "      <td>0.720518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.481661</td>\n",
       "      <td>0.774083</td>\n",
       "      <td>0.774142</td>\n",
       "      <td>0.773817</td>\n",
       "      <td>0.773897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.457512</td>\n",
       "      <td>0.790138</td>\n",
       "      <td>0.790554</td>\n",
       "      <td>0.790467</td>\n",
       "      <td>0.790135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.455039</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.802862</td>\n",
       "      <td>0.802938</td>\n",
       "      <td>0.802748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>0.463331</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.802701</td>\n",
       "      <td>0.802644</td>\n",
       "      <td>0.802668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.470651</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.807304</td>\n",
       "      <td>0.807401</td>\n",
       "      <td>0.807314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.479160</td>\n",
       "      <td>0.808486</td>\n",
       "      <td>0.808438</td>\n",
       "      <td>0.808527</td>\n",
       "      <td>0.808456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.483468</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.800527</td>\n",
       "      <td>0.800223</td>\n",
       "      <td>0.800307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.493203</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.802686</td>\n",
       "      <td>0.802686</td>\n",
       "      <td>0.802686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.499995</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.802683</td>\n",
       "      <td>0.802728</td>\n",
       "      <td>0.802701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22950, training_loss=0.32016659273301334, metrics={'train_runtime': 1092.1365, 'train_samples_per_second': 5377.277, 'train_steps_per_second': 42.028, 'total_flos': 2185902729360000.0, 'train_loss': 0.32016659273301334, 'epoch': 10.0})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/106 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26851242780685425,\n",
       " 'eval_accuracy': 0.9002227171492205,\n",
       " 'eval_precision': 0.8984696130765251,\n",
       " 'eval_recall': 0.8996704696118623,\n",
       " 'eval_f1': 0.8990266947261107,\n",
       " 'eval_runtime': 5.4443,\n",
       " 'eval_samples_per_second': 2474.148,\n",
       " 'eval_steps_per_second': 19.47,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-base-aug.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23b6abebe36407d9b81ebf7251600e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output file named: /home/jovyan/data/sst2/tiny-bert-base-aug-test.tsv upload it to GLUE benchmark to obtain results!\n"
     ]
    }
   ],
   "source": [
    "test_blank_logits = base.generate_logits(test_blank_dataloader, model)\n",
    "base.generate_real_test_file_sst2(test_blank_logits, f\"{os.path.expanduser('~')}/data/{DATASET}/tiny-bert-base-aug-test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/bert-distill-aug\", logging_dir=f\"~/logs/{DATASET}/bert-distill-aug\", remove_unused_columns=False, lr=0.00001, weight_decay=0.05, warmup_steps=20, epochs=20, temp=5.5, lambda_param=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    student_model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_aug,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41310' max='45900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41310/45900 32:32 < 03:36, 21.16 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.906700</td>\n",
       "      <td>3.283970</td>\n",
       "      <td>0.722477</td>\n",
       "      <td>0.725702</td>\n",
       "      <td>0.723478</td>\n",
       "      <td>0.722003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.686800</td>\n",
       "      <td>2.603310</td>\n",
       "      <td>0.772936</td>\n",
       "      <td>0.773361</td>\n",
       "      <td>0.772480</td>\n",
       "      <td>0.772590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.000300</td>\n",
       "      <td>2.288749</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.788931</td>\n",
       "      <td>0.789004</td>\n",
       "      <td>0.788951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.635700</td>\n",
       "      <td>2.140533</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.800728</td>\n",
       "      <td>0.800728</td>\n",
       "      <td>0.800459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.409800</td>\n",
       "      <td>2.091068</td>\n",
       "      <td>0.800459</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.800644</td>\n",
       "      <td>0.800455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.271800</td>\n",
       "      <td>2.050542</td>\n",
       "      <td>0.801606</td>\n",
       "      <td>0.801557</td>\n",
       "      <td>0.801644</td>\n",
       "      <td>0.801574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.172500</td>\n",
       "      <td>2.059860</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>0.813220</td>\n",
       "      <td>0.813284</td>\n",
       "      <td>0.813071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.103100</td>\n",
       "      <td>2.036237</td>\n",
       "      <td>0.810780</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>0.810821</td>\n",
       "      <td>0.810750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.043800</td>\n",
       "      <td>2.041963</td>\n",
       "      <td>0.811927</td>\n",
       "      <td>0.812037</td>\n",
       "      <td>0.812116</td>\n",
       "      <td>0.811923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.000500</td>\n",
       "      <td>2.047816</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>0.815452</td>\n",
       "      <td>0.815347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.966300</td>\n",
       "      <td>2.031480</td>\n",
       "      <td>0.814220</td>\n",
       "      <td>0.814185</td>\n",
       "      <td>0.814284</td>\n",
       "      <td>0.814196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>2.041174</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.815303</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.815326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>2.047597</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.816456</td>\n",
       "      <td>0.816536</td>\n",
       "      <td>0.816479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>2.052406</td>\n",
       "      <td>0.814220</td>\n",
       "      <td>0.814307</td>\n",
       "      <td>0.813989</td>\n",
       "      <td>0.814079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>2.062036</td>\n",
       "      <td>0.818807</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.818831</td>\n",
       "      <td>0.818773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>2.058419</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>0.813096</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.813061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>2.062854</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.816452</td>\n",
       "      <td>0.816452</td>\n",
       "      <td>0.816452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.843500</td>\n",
       "      <td>2.063986</td>\n",
       "      <td>0.811927</td>\n",
       "      <td>0.811975</td>\n",
       "      <td>0.812074</td>\n",
       "      <td>0.811918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=41310, training_loss=1.3557164572884517, metrics={'train_runtime': 1952.8108, 'train_samples_per_second': 3007.316, 'train_steps_per_second': 23.505, 'total_flos': 3934624912848000.0, 'train_loss': 1.3557164572884517, 'epoch': 18.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/106 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1050318479537964,\n",
       " 'eval_accuracy': 0.9155902004454343,\n",
       " 'eval_precision': 0.9139867735650128,\n",
       " 'eval_recall': 0.9152724406270596,\n",
       " 'eval_f1': 0.9145825730990167,\n",
       " 'eval_runtime': 5.1495,\n",
       " 'eval_samples_per_second': 2615.808,\n",
       " 'eval_steps_per_second': 20.585,\n",
       " 'epoch': 18.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(), f\"{os.path.expanduser('~')}/models/{DATASET}/bert-distil-aug.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b3869bb53a45ad94b0c37d905126c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating logits for given dataset:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output file named: /home/jovyan/data/sst2/tiny-bert-distill-aug-test.tsv upload it to GLUE benchmark to obtain results!\n"
     ]
    }
   ],
   "source": [
    "test_blank_logits = base.generate_logits(test_blank_dataloader, model)\n",
    "base.generate_real_test_file_sst2(test_blank_logits, f\"{os.path.expanduser('~')}/data/{DATASET}/tiny-bert-distill-aug-test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 16.740MB.\n",
      "Total Trainable Params: 4386178.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Modules</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert.embeddings.word_embeddings.weight</td>\n",
       "      <td>3906816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert.embeddings.position_embeddings.weight</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert.embeddings.token_type_embeddings.weight</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert.embeddings.LayerNorm.weight</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert.embeddings.LayerNorm.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.query.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.query.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.key.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.key.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.value.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert.encoder.layer.0.attention.self.value.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert.encoder.layer.0.attention.output.dense.we...</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert.encoder.layer.0.attention.output.dense.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert.encoder.layer.0.attention.output.LayerNor...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert.encoder.layer.0.attention.output.LayerNor...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert.encoder.layer.0.intermediate.dense.weight</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert.encoder.layer.0.intermediate.dense.bias</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert.encoder.layer.0.output.dense.weight</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert.encoder.layer.0.output.dense.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert.encoder.layer.0.output.LayerNorm.weight</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert.encoder.layer.0.output.LayerNorm.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.query.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.query.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.key.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.key.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.value.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bert.encoder.layer.1.attention.self.value.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bert.encoder.layer.1.attention.output.dense.we...</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert.encoder.layer.1.attention.output.dense.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bert.encoder.layer.1.attention.output.LayerNor...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bert.encoder.layer.1.attention.output.LayerNor...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert.encoder.layer.1.intermediate.dense.weight</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bert.encoder.layer.1.intermediate.dense.bias</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bert.encoder.layer.1.output.dense.weight</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bert.encoder.layer.1.output.dense.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bert.encoder.layer.1.output.LayerNorm.weight</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bert.encoder.layer.1.output.LayerNorm.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bert.pooler.dense.weight</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bert.pooler.dense.bias</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>classifier.weight</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>classifier.bias</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Modules Parameters\n",
       "0              bert.embeddings.word_embeddings.weight    3906816\n",
       "1          bert.embeddings.position_embeddings.weight      65536\n",
       "2        bert.embeddings.token_type_embeddings.weight        256\n",
       "3                    bert.embeddings.LayerNorm.weight        128\n",
       "4                      bert.embeddings.LayerNorm.bias        128\n",
       "5    bert.encoder.layer.0.attention.self.query.weight      16384\n",
       "6      bert.encoder.layer.0.attention.self.query.bias        128\n",
       "7      bert.encoder.layer.0.attention.self.key.weight      16384\n",
       "8        bert.encoder.layer.0.attention.self.key.bias        128\n",
       "9    bert.encoder.layer.0.attention.self.value.weight      16384\n",
       "10     bert.encoder.layer.0.attention.self.value.bias        128\n",
       "11  bert.encoder.layer.0.attention.output.dense.we...      16384\n",
       "12   bert.encoder.layer.0.attention.output.dense.bias        128\n",
       "13  bert.encoder.layer.0.attention.output.LayerNor...        128\n",
       "14  bert.encoder.layer.0.attention.output.LayerNor...        128\n",
       "15     bert.encoder.layer.0.intermediate.dense.weight      65536\n",
       "16       bert.encoder.layer.0.intermediate.dense.bias        512\n",
       "17           bert.encoder.layer.0.output.dense.weight      65536\n",
       "18             bert.encoder.layer.0.output.dense.bias        128\n",
       "19       bert.encoder.layer.0.output.LayerNorm.weight        128\n",
       "20         bert.encoder.layer.0.output.LayerNorm.bias        128\n",
       "21   bert.encoder.layer.1.attention.self.query.weight      16384\n",
       "22     bert.encoder.layer.1.attention.self.query.bias        128\n",
       "23     bert.encoder.layer.1.attention.self.key.weight      16384\n",
       "24       bert.encoder.layer.1.attention.self.key.bias        128\n",
       "25   bert.encoder.layer.1.attention.self.value.weight      16384\n",
       "26     bert.encoder.layer.1.attention.self.value.bias        128\n",
       "27  bert.encoder.layer.1.attention.output.dense.we...      16384\n",
       "28   bert.encoder.layer.1.attention.output.dense.bias        128\n",
       "29  bert.encoder.layer.1.attention.output.LayerNor...        128\n",
       "30  bert.encoder.layer.1.attention.output.LayerNor...        128\n",
       "31     bert.encoder.layer.1.intermediate.dense.weight      65536\n",
       "32       bert.encoder.layer.1.intermediate.dense.bias        512\n",
       "33           bert.encoder.layer.1.output.dense.weight      65536\n",
       "34             bert.encoder.layer.1.output.dense.bias        128\n",
       "35       bert.encoder.layer.1.output.LayerNorm.weight        128\n",
       "36         bert.encoder.layer.1.output.LayerNorm.bias        128\n",
       "37                           bert.pooler.dense.weight      16384\n",
       "38                             bert.pooler.dense.bias        128\n",
       "39                                  classifier.weight        256\n",
       "40                                    classifier.bias          2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.count_parameters(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fad741a9540>\n",
      "self.infer_speed_comp()\n",
      "  3.67 ms\n",
      "  1 measurement, 1000 runs , 4 threads\n"
     ]
    }
   ],
   "source": [
    "cpu_benchmark = base.BenchMarkRunner(student_model, cpu_data_loader, \"cpu\", 1000)\n",
    "print(cpu_benchmark.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fac9a608160>\n",
      "self.infer_speed_comp()\n",
      "  1.94 ms\n",
      "  1 measurement, 1000 runs , 4 threads\n"
     ]
    }
   ],
   "source": [
    "gpu_benchmark = base.BenchMarkRunner(student_model, gpu_data_loader, \"cuda\", 1000)\n",
    "print(gpu_benchmark.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
