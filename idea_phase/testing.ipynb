{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, BertForSequenceClassification, BertTokenizer, EarlyStoppingCallback, AutoConfig, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import base\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"trec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_from_disk(f\"~/data/{DATASET}/train-logits_fine\")\n",
    "eval = load_from_disk(f\"~/data/{DATASET}/eval-logits_fine\")\n",
    "test = load_from_disk(f\"~/data/{DATASET}/test-logits_fine\")\n",
    "\n",
    "train_aug = load_from_disk(f\"~/data/{DATASET}/train-logits-augmented_fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the train dataset\")\n",
    "eval = eval.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the eval dataset\")\n",
    "test = test.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the test dataset\")\n",
    "\n",
    "train_aug = train_aug.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the augmented dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ndavid/autotrain-trec-fine-bert-739422530 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([47, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([47]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1965090/106609535.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{os.path.expanduser('~')}/models/{DATASET}/teacher_fine.pth\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")\n",
    "config.max_length = 20 \n",
    "config.num_labels = 50\n",
    "config.output_hidden_states = True\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\", config=config, ignore_mismatched_sizes=True)\n",
    "state_dict = torch.load(model_path, map_location=torch.device('cpu')) \n",
    "\n",
    "teacher_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilTrainerInner(Trainer):\n",
    "    \"\"\"Distilation trainer, computes loss with logits from teacher in mind. Logits are precomputed.\"\"\"\n",
    "    def __init__(self, student_model=None, teacher_model = None, *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.student = student_model\n",
    "        self.teacher = teacher_model\n",
    "        self.layer_loss_function = nn.MSELoss()\n",
    "        self.logit_loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.temperature = self.args.temperature\n",
    "        self.lambda_param = self.args.lambda_param\n",
    "        self.alpha_param = self.args.alpha_param\n",
    "\n",
    "\n",
    "        self.student_to_teacher = nn.Linear(128, 768).to(device)\n",
    "        self.model_parameters = list(self.model.parameters()) + list(self.student_to_teacher.parameters())\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        \n",
    "        student_output = student(**inputs, output_hidden_states=True)\n",
    "        student_target_loss = student_output[\"loss\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs, output_hidden_states=True)\n",
    "\n",
    "        teacher_hidden_states = teacher_output.hidden_states\n",
    "        student_hidden_states = student_output.hidden_states\n",
    "\n",
    "        teacher_l6 = teacher_hidden_states[6] / self.temperature\n",
    "        teacher_l12 = teacher_hidden_states[12] / self.temperature\n",
    "        student_l1 = student_hidden_states[1]\n",
    "        student_l2 = student_hidden_states[2] \n",
    "\n",
    "        student_l1_projection = self.student_to_teacher(student_l1) / self.temperature\n",
    "        student_l2_projection = self.student_to_teacher(student_l2) / self.temperature\n",
    "\n",
    "        layer_distillation_loss = (\n",
    "            self.layer_loss_function(student_l1_projection, teacher_l6) +\n",
    "            self.layer_loss_function(student_l2_projection, teacher_l12)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        soft_teacher = F.softmax(logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output['logits'] / self.temperature, dim=-1)\n",
    "\n",
    "        logit_distillation_loss = self.logit_loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        logit_label_loss = ((1. - self.lambda_param) * student_target_loss + self.lambda_param * logit_distillation_loss)\n",
    "\n",
    "        \n",
    "        loss = (1 - self.alpha_param) * logit_label_loss + self.alpha_param * layer_distillation_loss\n",
    "\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            # For evaluation, we disable extra outputs.\n",
    "            outputs = model(**inputs, output_hidden_states=False)\n",
    "            loss = outputs.loss if \"loss\" in outputs else None\n",
    "            logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_training_args(TrainingArguments):\n",
    "    \"\"\"Custom wrapper of training args for distillation.\"\"\"\n",
    "    def __init__(self, lambda_param, alpha_param, temperature, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "        self.lambda_param = lambda_param\n",
    "        self.alpha_param = alpha_param\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(output_dir, logging_dir, remove_unused_columns=True, lr=5e-5, epochs=5, weight_decay=0, adam_beta1 = .9, lambda_param=.5, alpha_param = .5, temp=5, batch_size=128, num_workers=4, warmup_steps=0):\n",
    "    \"\"\"Returns training args that can be adjusted.\"\"\"\n",
    "    return (\n",
    "        Custom_training_args(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        adam_beta1 = adam_beta1,\n",
    "        warmup_steps = warmup_steps,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=lr, #Defaultní hodnota \n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        seed = 42,  #Defaultní hodnota \n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end = True,\n",
    "        fp16=True, \n",
    "        logging_dir=logging_dir,\n",
    "        remove_unused_columns=remove_unused_columns,\n",
    "        lambda_param = lambda_param,\n",
    "        alpha_param = alpha_param, \n",
    "        temperature = temp,\n",
    "        dataloader_num_workers=num_workers,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = get_training_args(output_dir=f\"~/results/{DATASET}/hokus_pokus\", logging_dir=f\"~/logs/{DATASET}/hokus_pokus\", remove_unused_columns=False, warmup_steps=4, lr=5e-4, weight_decay=.003, batch_size=128, epochs=20, temp=2.5, lambda_param=.4, alpha_param=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistilTrainerInner(\n",
    "    student_model = student_model,\n",
    "    teacher_model = teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 13:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.248500</td>\n",
       "      <td>2.304686</td>\n",
       "      <td>0.505041</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.153706</td>\n",
       "      <td>0.138322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>1.770681</td>\n",
       "      <td>0.634280</td>\n",
       "      <td>0.218068</td>\n",
       "      <td>0.241304</td>\n",
       "      <td>0.217888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>1.415867</td>\n",
       "      <td>0.703941</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.296876</td>\n",
       "      <td>0.276550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>1.279000</td>\n",
       "      <td>0.715857</td>\n",
       "      <td>0.303754</td>\n",
       "      <td>0.325029</td>\n",
       "      <td>0.300446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>1.175202</td>\n",
       "      <td>0.734189</td>\n",
       "      <td>0.339924</td>\n",
       "      <td>0.354964</td>\n",
       "      <td>0.333470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>1.109791</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.385101</td>\n",
       "      <td>0.385757</td>\n",
       "      <td>0.369391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>1.088450</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.454085</td>\n",
       "      <td>0.418329</td>\n",
       "      <td>0.414791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>1.051247</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.444176</td>\n",
       "      <td>0.442952</td>\n",
       "      <td>0.428646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>1.001823</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.483770</td>\n",
       "      <td>0.467252</td>\n",
       "      <td>0.466109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.995045</td>\n",
       "      <td>0.781852</td>\n",
       "      <td>0.531094</td>\n",
       "      <td>0.491267</td>\n",
       "      <td>0.492554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.976425</td>\n",
       "      <td>0.792851</td>\n",
       "      <td>0.576287</td>\n",
       "      <td>0.520431</td>\n",
       "      <td>0.529828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.961736</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.566903</td>\n",
       "      <td>0.530425</td>\n",
       "      <td>0.533885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.959982</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.592644</td>\n",
       "      <td>0.560240</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.945735</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.610001</td>\n",
       "      <td>0.563321</td>\n",
       "      <td>0.571847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.953118</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.614602</td>\n",
       "      <td>0.564703</td>\n",
       "      <td>0.575865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.942979</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.607338</td>\n",
       "      <td>0.567590</td>\n",
       "      <td>0.576669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.939795</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.603877</td>\n",
       "      <td>0.570538</td>\n",
       "      <td>0.576759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.943043</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.611494</td>\n",
       "      <td>0.569171</td>\n",
       "      <td>0.579132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.945063</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.602210</td>\n",
       "      <td>0.572566</td>\n",
       "      <td>0.577421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.944389</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.602301</td>\n",
       "      <td>0.573691</td>\n",
       "      <td>0.577941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.39037716933659145, metrics={'train_runtime': 800.8023, 'train_samples_per_second': 108.916, 'train_steps_per_second': 0.874, 'total_flos': 65900954952000.0, 'train_loss': 0.39037716933659145, 'epoch': 20.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0279921293258667,\n",
       " 'eval_accuracy': 0.76,\n",
       " 'eval_precision': 0.5204448048364814,\n",
       " 'eval_recall': 0.5517825303517305,\n",
       " 'eval_f1': 0.5117426131149784,\n",
       " 'eval_runtime': 3.1235,\n",
       " 'eval_samples_per_second': 160.077,\n",
       " 'eval_steps_per_second': 1.281,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = get_training_args(output_dir=f\"~/results/{DATASET}/hokus_pokus\", logging_dir=f\"~/logs/{DATASET}/hokus_pokus\", remove_unused_columns=False, warmup_steps=4, lr=5e-4, weight_decay=.003, batch_size=128, epochs=20, temp=2.5, lambda_param=.4, alpha_param=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilTrainerInnerAVG(Trainer):\n",
    "    \"\"\"Distilation trainer, computes loss with logits from teacher in mind. Logits are precomputed.\"\"\"\n",
    "    def __init__(self, student_model=None, teacher_model = None, *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.student = student_model\n",
    "        self.teacher = teacher_model\n",
    "        self.layer_loss_function = nn.MSELoss()\n",
    "        self.logit_loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.temperature = self.args.temperature\n",
    "        self.lambda_param = self.args.lambda_param\n",
    "        self.alpha_param = self.args.alpha_param\n",
    "\n",
    "\n",
    "        self.student_to_teacher = nn.Linear(128, 768).to(device)\n",
    "        self.model_parameters = list(self.model.parameters()) + list(self.student_to_teacher.parameters())\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        \n",
    "        student_output = student(**inputs, output_hidden_states=True)\n",
    "        student_target_loss = student_output[\"loss\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs, output_hidden_states=True)\n",
    "\n",
    "        teacher_hidden_states = teacher_output.hidden_states\n",
    "        student_hidden_states = student_output.hidden_states\n",
    "\n",
    "        \n",
    "        \n",
    "        teacher_l6 = torch.stack(teacher_hidden_states[2:7], dim=0).mean(dim=0) / self.temperature\n",
    "        teacher_l12 = torch.stack(teacher_hidden_states[7:12], dim=0).mean(dim=0) / self.temperature\n",
    "        student_l1 = student_hidden_states[1]\n",
    "        student_l2 = student_hidden_states[2] \n",
    "\n",
    "        student_l1_projection = self.student_to_teacher(student_l1) / self.temperature\n",
    "        student_l2_projection = self.student_to_teacher(student_l2) / self.temperature\n",
    "\n",
    "        layer_distillation_loss = (\n",
    "            self.layer_loss_function(student_l1_projection, teacher_l6) +\n",
    "            self.layer_loss_function(student_l2_projection, teacher_l12)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        soft_teacher = F.softmax(logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output['logits'] / self.temperature, dim=-1)\n",
    "\n",
    "        logit_distillation_loss = self.logit_loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        logit_label_loss = ((1. - self.lambda_param) * student_target_loss + self.lambda_param * logit_distillation_loss)\n",
    "\n",
    "        \n",
    "        loss = (1 - self.alpha_param) * logit_label_loss + self.alpha_param * layer_distillation_loss\n",
    "\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            # For evaluation, we disable extra outputs.\n",
    "            outputs = model(**inputs, output_hidden_states=False)\n",
    "            loss = outputs.loss if \"loss\" in outputs else None\n",
    "            logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistilTrainerInnerAVG(\n",
    "    student_model = student_model,\n",
    "    teacher_model = teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 13:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.480100</td>\n",
       "      <td>2.863770</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.087182</td>\n",
       "      <td>0.101284</td>\n",
       "      <td>0.076397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.103200</td>\n",
       "      <td>2.138775</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.208789</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.174814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>1.635681</td>\n",
       "      <td>0.655362</td>\n",
       "      <td>0.267159</td>\n",
       "      <td>0.258225</td>\n",
       "      <td>0.237654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>1.374435</td>\n",
       "      <td>0.713107</td>\n",
       "      <td>0.302741</td>\n",
       "      <td>0.315566</td>\n",
       "      <td>0.295314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>1.259040</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.369574</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.335380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.430800</td>\n",
       "      <td>1.177486</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.364386</td>\n",
       "      <td>0.368775</td>\n",
       "      <td>0.350246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>1.139008</td>\n",
       "      <td>0.757104</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>0.424130</td>\n",
       "      <td>0.416781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>1.116509</td>\n",
       "      <td>0.758020</td>\n",
       "      <td>0.405699</td>\n",
       "      <td>0.433149</td>\n",
       "      <td>0.410593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>1.078361</td>\n",
       "      <td>0.754354</td>\n",
       "      <td>0.425039</td>\n",
       "      <td>0.422598</td>\n",
       "      <td>0.410365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>1.060170</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.461547</td>\n",
       "      <td>0.435401</td>\n",
       "      <td>0.431252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>1.046575</td>\n",
       "      <td>0.769019</td>\n",
       "      <td>0.493738</td>\n",
       "      <td>0.469003</td>\n",
       "      <td>0.462260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>1.027865</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.529806</td>\n",
       "      <td>0.501148</td>\n",
       "      <td>0.505541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>1.019385</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.544886</td>\n",
       "      <td>0.514066</td>\n",
       "      <td>0.518386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>1.003658</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.549532</td>\n",
       "      <td>0.522028</td>\n",
       "      <td>0.527182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>1.015267</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.552166</td>\n",
       "      <td>0.526327</td>\n",
       "      <td>0.529604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>1.009536</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.566816</td>\n",
       "      <td>0.535022</td>\n",
       "      <td>0.538411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.997811</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.568324</td>\n",
       "      <td>0.535221</td>\n",
       "      <td>0.537778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.991753</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.572824</td>\n",
       "      <td>0.535516</td>\n",
       "      <td>0.540975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.994743</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.569768</td>\n",
       "      <td>0.534438</td>\n",
       "      <td>0.538886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.997421</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.569768</td>\n",
       "      <td>0.534438</td>\n",
       "      <td>0.538886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.4159424332209996, metrics={'train_runtime': 801.2602, 'train_samples_per_second': 108.854, 'train_steps_per_second': 0.874, 'total_flos': 65900954952000.0, 'train_loss': 0.4159424332209996, 'epoch': 20.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
