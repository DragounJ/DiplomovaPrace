{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, BertForSequenceClassification, BertTokenizer, EarlyStoppingCallback, AutoConfig, TrainingArguments, AutoModelForImageClassification\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import base\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"trec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used: NVIDIA A100 80GB PCIe MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and will be used:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_from_disk(f\"~/data/{DATASET}/train-logits_fine\")\n",
    "eval = load_from_disk(f\"~/data/{DATASET}/eval-logits_fine\")\n",
    "test = load_from_disk(f\"~/data/{DATASET}/test-logits_fine\")\n",
    "\n",
    "train_aug = load_from_disk(f\"~/data/{DATASET}/train-logits-augmented_fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the train dataset\")\n",
    "eval = eval.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the eval dataset\")\n",
    "test = test.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the test dataset\")\n",
    "\n",
    "train_aug = train_aug.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=300), batched=True, desc=\"Tokenizing the augmented dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ndavid/autotrain-trec-fine-bert-739422530 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([47, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([47]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5453/3393057940.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\")\n",
    "config.max_length = 20 #revert to default ot skip warning \n",
    "config.num_labels = 50\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\"ndavid/autotrain-trec-fine-bert-739422530\", config=config, ignore_mismatched_sizes=True)\n",
    "model_path = f\"{os.path.expanduser('~')}/models/{DATASET}/teacher_fine.pth\"\n",
    "state_dict = torch.load(model_path, map_location=torch.device('cpu')) \n",
    "teacher_model.load_state_dict(state_dict)\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilTrainerInner(Trainer):\n",
    "    \"\"\"Distilation trainer, computes loss with logits from teacher in mind. Logits are precomputed.\"\"\"\n",
    "    def __init__(self, student_model=None, teacher_model = None, *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.student = student_model\n",
    "        self.teacher = teacher_model\n",
    "        self.layer_loss_function = nn.MSELoss()\n",
    "        self.logit_loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.temperature = self.args.temperature\n",
    "        self.lambda_param = self.args.lambda_param\n",
    "        self.alpha_param = self.args.alpha_param\n",
    "\n",
    "        self.student_to_teacher = nn.Linear(128, 768).to(device)\n",
    "\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        \n",
    "        student_output = student(**inputs, output_hidden_states=True)\n",
    "        student_target_loss = student_output[\"loss\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs, output_hidden_states=True)\n",
    "\n",
    "        teacher_hidden_states = teacher_output.hidden_states\n",
    "        student_hidden_states = student_output.hidden_states\n",
    "\n",
    "        \n",
    "        teacher_l6 = teacher_hidden_states[6] / self.temperature\n",
    "        teacher_l12 = teacher_hidden_states[12] / self.temperature\n",
    "        student_l1 = student_hidden_states[1]\n",
    "        student_l2 = student_hidden_states[2] \n",
    "\n",
    "        student_l1_projection = self.student_to_teacher(student_l1) / self.temperature\n",
    "        student_l2_projection = self.student_to_teacher(student_l2) / self.temperature\n",
    "\n",
    "        layer_distillation_loss = (\n",
    "            self.layer_loss_function(student_l1_projection, teacher_l6) +\n",
    "            self.layer_loss_function(student_l2_projection, teacher_l12)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        soft_teacher = F.softmax(logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output['logits'] / self.temperature, dim=-1)\n",
    "\n",
    "        logit_distillation_loss = self.logit_loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        logit_label_loss = ((1. - self.lambda_param) * student_target_loss + self.lambda_param * logit_distillation_loss)\n",
    "\n",
    "        \n",
    "        loss = (1 - self.alpha_param) * logit_label_loss + self.alpha_param * layer_distillation_loss\n",
    "\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=False)\n",
    "            loss = outputs.loss if \"loss\" in outputs else None\n",
    "            logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_training_args(TrainingArguments):\n",
    "    \"\"\"Custom wrapper of training args for distillation.\"\"\"\n",
    "    def __init__(self, lambda_param, alpha_param, temperature, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "        self.lambda_param = lambda_param\n",
    "        self.alpha_param = alpha_param\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(output_dir, logging_dir, remove_unused_columns=True, lr=5e-5, epochs=5, weight_decay=0, adam_beta1 = .9, lambda_param=.5, alpha_param = .5, temp=5, batch_size=128, num_workers=4, warmup_steps=0):\n",
    "    \"\"\"Returns training args that can be adjusted.\"\"\"\n",
    "    return (\n",
    "        Custom_training_args(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        adam_beta1 = adam_beta1,\n",
    "        warmup_steps = warmup_steps,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=lr, #Defaultní hodnota \n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        seed = 42,  #Defaultní hodnota \n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end = True,\n",
    "        fp16=True, \n",
    "        logging_dir=logging_dir,\n",
    "        remove_unused_columns=remove_unused_columns,\n",
    "        lambda_param = lambda_param,\n",
    "        alpha_param = alpha_param, \n",
    "        temperature = temp,\n",
    "        dataloader_num_workers=num_workers,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = get_training_args(output_dir=f\"~/results/{DATASET}/hokus_pokus\", logging_dir=f\"~/logs/{DATASET}/hokus_pokus\", remove_unused_columns=False, lr=8e-4, batch_size=128, epochs=20, temp=2, lambda_param=0, alpha_param=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistilTrainerInner(\n",
    "    student_model = student_model,\n",
    "    teacher_model = teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 05:54, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.351700</td>\n",
       "      <td>2.622619</td>\n",
       "      <td>0.450046</td>\n",
       "      <td>0.124208</td>\n",
       "      <td>0.126271</td>\n",
       "      <td>0.104896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.634100</td>\n",
       "      <td>1.841120</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.215776</td>\n",
       "      <td>0.250789</td>\n",
       "      <td>0.224072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.145700</td>\n",
       "      <td>1.420704</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.376744</td>\n",
       "      <td>0.338916</td>\n",
       "      <td>0.323342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>1.239802</td>\n",
       "      <td>0.730522</td>\n",
       "      <td>0.413277</td>\n",
       "      <td>0.380970</td>\n",
       "      <td>0.368196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>1.134269</td>\n",
       "      <td>0.752521</td>\n",
       "      <td>0.448728</td>\n",
       "      <td>0.440872</td>\n",
       "      <td>0.423798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>1.074933</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>0.483631</td>\n",
       "      <td>0.462137</td>\n",
       "      <td>0.455504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>1.051079</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.512179</td>\n",
       "      <td>0.477353</td>\n",
       "      <td>0.478819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>1.044749</td>\n",
       "      <td>0.774519</td>\n",
       "      <td>0.586416</td>\n",
       "      <td>0.539534</td>\n",
       "      <td>0.547690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>1.036319</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.616361</td>\n",
       "      <td>0.559525</td>\n",
       "      <td>0.566792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>1.042960</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.628541</td>\n",
       "      <td>0.565279</td>\n",
       "      <td>0.572681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>1.031686</td>\n",
       "      <td>0.775435</td>\n",
       "      <td>0.638354</td>\n",
       "      <td>0.590199</td>\n",
       "      <td>0.594426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>1.028670</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.687358</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.607723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>1.036350</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.696793</td>\n",
       "      <td>0.632734</td>\n",
       "      <td>0.643801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.044532</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.698721</td>\n",
       "      <td>0.628721</td>\n",
       "      <td>0.644196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>1.045374</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.701296</td>\n",
       "      <td>0.657838</td>\n",
       "      <td>0.664046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>1.052338</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.714150</td>\n",
       "      <td>0.670083</td>\n",
       "      <td>0.676555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>1.051156</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.711460</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.670271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>1.043586</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.742100</td>\n",
       "      <td>0.679403</td>\n",
       "      <td>0.695502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>1.049967</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.716639</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.679802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>1.049668</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.742682</td>\n",
       "      <td>0.679947</td>\n",
       "      <td>0.697998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.4959136758531843, metrics={'train_runtime': 355.5705, 'train_samples_per_second': 245.296, 'train_steps_per_second': 1.969, 'total_flos': 65900954952000.0, 'train_loss': 0.4959136758531843, 'epoch': 20.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0209091901779175,\n",
       " 'eval_accuracy': 0.786,\n",
       " 'eval_precision': 0.6741836658624986,\n",
       " 'eval_recall': 0.6495346381733221,\n",
       " 'eval_f1': 0.6295482650549977,\n",
       " 'eval_runtime': 3.4098,\n",
       " 'eval_samples_per_second': 146.638,\n",
       " 'eval_steps_per_second': 1.173,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = base.get_training_args(output_dir=f\"~/results/{DATASET}/hokus_pokus\", logging_dir=f\"~/logs/{DATASET}/hokus_pokus\", remove_unused_columns=False, lr=8e-4, batch_size=128, epochs=20,  temp=2, lambda_param=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = base.DistilTrainer(\n",
    "    student_model = student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 01:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.307700</td>\n",
       "      <td>1.931338</td>\n",
       "      <td>0.404216</td>\n",
       "      <td>0.066452</td>\n",
       "      <td>0.093272</td>\n",
       "      <td>0.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.624300</td>\n",
       "      <td>1.370640</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>0.248521</td>\n",
       "      <td>0.250510</td>\n",
       "      <td>0.230790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.151600</td>\n",
       "      <td>1.084126</td>\n",
       "      <td>0.695692</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.303464</td>\n",
       "      <td>0.280613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.952221</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.349467</td>\n",
       "      <td>0.357325</td>\n",
       "      <td>0.338163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.878760</td>\n",
       "      <td>0.747938</td>\n",
       "      <td>0.399649</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>0.382936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.848470</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.429065</td>\n",
       "      <td>0.408227</td>\n",
       "      <td>0.401909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.447800</td>\n",
       "      <td>0.817848</td>\n",
       "      <td>0.760770</td>\n",
       "      <td>0.491511</td>\n",
       "      <td>0.433454</td>\n",
       "      <td>0.436979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.810743</td>\n",
       "      <td>0.773602</td>\n",
       "      <td>0.509770</td>\n",
       "      <td>0.479757</td>\n",
       "      <td>0.480758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.790830</td>\n",
       "      <td>0.779102</td>\n",
       "      <td>0.547607</td>\n",
       "      <td>0.494367</td>\n",
       "      <td>0.502936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>0.543249</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.501678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.775826</td>\n",
       "      <td>0.784601</td>\n",
       "      <td>0.591484</td>\n",
       "      <td>0.532890</td>\n",
       "      <td>0.543822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.769606</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.591011</td>\n",
       "      <td>0.541189</td>\n",
       "      <td>0.549534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.757634</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.643207</td>\n",
       "      <td>0.575255</td>\n",
       "      <td>0.593849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.742101</td>\n",
       "      <td>0.791934</td>\n",
       "      <td>0.674281</td>\n",
       "      <td>0.597281</td>\n",
       "      <td>0.620531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.747194</td>\n",
       "      <td>0.796517</td>\n",
       "      <td>0.678139</td>\n",
       "      <td>0.602126</td>\n",
       "      <td>0.625069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.752723</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.677261</td>\n",
       "      <td>0.601341</td>\n",
       "      <td>0.621310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.761403</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.658885</td>\n",
       "      <td>0.601085</td>\n",
       "      <td>0.616052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.757215</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.678447</td>\n",
       "      <td>0.604166</td>\n",
       "      <td>0.627070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.756670</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.701236</td>\n",
       "      <td>0.631664</td>\n",
       "      <td>0.650241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.753634</td>\n",
       "      <td>0.797434</td>\n",
       "      <td>0.731893</td>\n",
       "      <td>0.646015</td>\n",
       "      <td>0.670628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.5186817496163505, metrics={'train_runtime': 107.5116, 'train_samples_per_second': 811.262, 'train_steps_per_second': 6.511, 'total_flos': 65900954952000.0, 'train_loss': 0.5186817496163505, 'epoch': 20.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 03:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.766819179058075,\n",
       " 'eval_accuracy': 0.762,\n",
       " 'eval_precision': 0.6137723868676089,\n",
       " 'eval_recall': 0.6109359742797066,\n",
       " 'eval_f1': 0.5813720599288332,\n",
       " 'eval_runtime': 3.5397,\n",
       " 'eval_samples_per_second': 141.254,\n",
       " 'eval_steps_per_second': 1.13,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = get_training_args(output_dir=f\"~/results/{DATASET}/hokus_pokus\", logging_dir=f\"~/logs/{DATASET}/hokus_pokus\", remove_unused_columns=False, lr=8e-4, batch_size=128, epochs=20, temp=2, lambda_param=0, alpha_param=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilTrainerInnerAVG(Trainer):\n",
    "    \"\"\"Distilation trainer, computes loss with logits from teacher in mind. Logits are precomputed.\"\"\"\n",
    "    def __init__(self, student_model=None, teacher_model = None, *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.student = student_model\n",
    "        self.teacher = teacher_model\n",
    "        self.layer_loss_function = nn.MSELoss()\n",
    "        self.logit_loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.temperature = self.args.temperature\n",
    "        self.lambda_param = self.args.lambda_param\n",
    "        self.alpha_param = self.args.alpha_param\n",
    "\n",
    "\n",
    "        self.student_to_teacher = nn.Linear(128, 768).to(device)\n",
    "        self.model_parameters = list(self.model.parameters()) + list(self.student_to_teacher.parameters())\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        \n",
    "        student_output = student(**inputs, output_hidden_states=True)\n",
    "        student_target_loss = student_output[\"loss\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs, output_hidden_states=True)\n",
    "\n",
    "        teacher_hidden_states = teacher_output.hidden_states\n",
    "        student_hidden_states = student_output.hidden_states\n",
    "\n",
    "        \n",
    "        \n",
    "        teacher_l6 = torch.stack(teacher_hidden_states[1:7], dim=0).mean(dim=0) / self.temperature\n",
    "        teacher_l12 = torch.stack(teacher_hidden_states[7:13], dim=0).mean(dim=0) / self.temperature\n",
    "        student_l1 = student_hidden_states[1]\n",
    "        student_l2 = student_hidden_states[2] \n",
    "\n",
    "        student_l1_projection = self.student_to_teacher(student_l1) / self.temperature\n",
    "        student_l2_projection = self.student_to_teacher(student_l2) / self.temperature\n",
    "\n",
    "        layer_distillation_loss = (\n",
    "            self.layer_loss_function(student_l1_projection, teacher_l6) +\n",
    "            self.layer_loss_function(student_l2_projection, teacher_l12)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        soft_teacher = F.softmax(logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output['logits'] / self.temperature, dim=-1)\n",
    "\n",
    "        logit_distillation_loss = self.logit_loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        logit_label_loss = ((1. - self.lambda_param) * student_target_loss + self.lambda_param * logit_distillation_loss)\n",
    "\n",
    "        \n",
    "        loss = (1 - self.alpha_param) * logit_label_loss + self.alpha_param * layer_distillation_loss\n",
    "\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        logits = inputs.pop(\"logits\")\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            # For evaluation, we disable extra outputs.\n",
    "            outputs = model(**inputs, output_hidden_states=False)\n",
    "            loss = outputs.loss if \"loss\" in outputs else None\n",
    "            logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistilTrainerInnerAVG(\n",
    "    student_model = student_model,\n",
    "    teacher_model = teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=eval,\n",
    "    compute_metrics=base.compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 06:05, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.308600</td>\n",
       "      <td>2.604124</td>\n",
       "      <td>0.442713</td>\n",
       "      <td>0.067964</td>\n",
       "      <td>0.109693</td>\n",
       "      <td>0.077831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.616400</td>\n",
       "      <td>1.825710</td>\n",
       "      <td>0.586618</td>\n",
       "      <td>0.250198</td>\n",
       "      <td>0.224954</td>\n",
       "      <td>0.202270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>1.388694</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.331629</td>\n",
       "      <td>0.316616</td>\n",
       "      <td>0.291173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>1.204824</td>\n",
       "      <td>0.737855</td>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.386627</td>\n",
       "      <td>0.370825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>1.085495</td>\n",
       "      <td>0.744271</td>\n",
       "      <td>0.412612</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>0.395178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>1.032137</td>\n",
       "      <td>0.761687</td>\n",
       "      <td>0.479304</td>\n",
       "      <td>0.454250</td>\n",
       "      <td>0.448534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>1.047209</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.549809</td>\n",
       "      <td>0.488388</td>\n",
       "      <td>0.487681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>1.030510</td>\n",
       "      <td>0.771769</td>\n",
       "      <td>0.546652</td>\n",
       "      <td>0.541012</td>\n",
       "      <td>0.529507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>1.027601</td>\n",
       "      <td>0.770852</td>\n",
       "      <td>0.569284</td>\n",
       "      <td>0.553769</td>\n",
       "      <td>0.546604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.982512</td>\n",
       "      <td>0.776352</td>\n",
       "      <td>0.580372</td>\n",
       "      <td>0.556895</td>\n",
       "      <td>0.551229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>1.001712</td>\n",
       "      <td>0.780018</td>\n",
       "      <td>0.642788</td>\n",
       "      <td>0.581772</td>\n",
       "      <td>0.591877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.962176</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.678293</td>\n",
       "      <td>0.607209</td>\n",
       "      <td>0.620602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.993389</td>\n",
       "      <td>0.789184</td>\n",
       "      <td>0.683015</td>\n",
       "      <td>0.631704</td>\n",
       "      <td>0.640130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>1.019183</td>\n",
       "      <td>0.791017</td>\n",
       "      <td>0.730540</td>\n",
       "      <td>0.677091</td>\n",
       "      <td>0.684154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>1.012063</td>\n",
       "      <td>0.786434</td>\n",
       "      <td>0.722583</td>\n",
       "      <td>0.674053</td>\n",
       "      <td>0.681834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>1.011991</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.722516</td>\n",
       "      <td>0.669094</td>\n",
       "      <td>0.682011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>1.009062</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.714764</td>\n",
       "      <td>0.679369</td>\n",
       "      <td>0.684463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>1.019149</td>\n",
       "      <td>0.788268</td>\n",
       "      <td>0.716566</td>\n",
       "      <td>0.678397</td>\n",
       "      <td>0.683429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>1.020338</td>\n",
       "      <td>0.783685</td>\n",
       "      <td>0.717280</td>\n",
       "      <td>0.683013</td>\n",
       "      <td>0.686514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.025189</td>\n",
       "      <td>0.785518</td>\n",
       "      <td>0.716538</td>\n",
       "      <td>0.677835</td>\n",
       "      <td>0.683790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.468877078805651, metrics={'train_runtime': 366.2642, 'train_samples_per_second': 238.134, 'train_steps_per_second': 1.911, 'total_flos': 65900954952000.0, 'train_loss': 0.468877078805651, 'epoch': 20.0})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9729981422424316,\n",
       " 'eval_accuracy': 0.778,\n",
       " 'eval_precision': 0.6108235367341464,\n",
       " 'eval_recall': 0.6287281800256842,\n",
       " 'eval_f1': 0.585669900362865,\n",
       " 'eval_runtime': 3.7778,\n",
       " 'eval_samples_per_second': 132.353,\n",
       " 'eval_steps_per_second': 1.059,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()\n",
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
